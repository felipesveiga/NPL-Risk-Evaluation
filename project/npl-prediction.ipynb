{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2958f028",
   "metadata": {
    "papermill": {
     "duration": 0.02393,
     "end_time": "2024-04-15T22:18:13.755206",
     "exception": false,
     "start_time": "2024-04-15T22:18:13.731276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n",
    "<div style='font-size:20px'> \n",
    "    <ul> \n",
    "        <li> \n",
    "            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n",
    "        </li>\n",
    "        <li> \n",
    "            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n",
    "            credit strategy must be adopted for new contracts.\n",
    "        </li>\n",
    "        <li> \n",
    "            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n",
    "            <ul style='list-style-type:decimal'> \n",
    "                <li> \n",
    "                    It must be focused on predicting whether a given client might produce an NPL in the future.\n",
    "                </li>\n",
    "                <li> \n",
    "                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n",
    "                    an incisive \"yes or no\" answer.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <p style='margin-left:30px'> <strong> Note:</strong> The bank's NPL definition is any loan which payment is at least 90 days late.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e22de",
   "metadata": {
    "papermill": {
     "duration": 0.022774,
     "end_time": "2024-04-15T22:18:13.801314",
     "exception": false,
     "start_time": "2024-04-15T22:18:13.778540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Importing</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n",
    "            and the second lists the loans they've contracted over some period of time.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c69be32-05ca-40fa-be5a-d0add93e875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.1\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.2\n",
      "    Uninstalling py4j-0.10.9.2:\n",
      "      Successfully uninstalled py4j-0.10.9.2\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.2.0\n",
      "    Uninstalling pyspark-3.2.0:\n",
      "      Successfully uninstalled pyspark-3.2.0\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67cb8efb-f668-4e6a-9155-102af7aa7302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pandas 2.2.0\n",
      "Uninstalling pandas-2.2.0:\n",
      "  Successfully uninstalled pandas-2.2.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas==2.2.0\n",
      "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/veiga/.local/lib/python3.10/site-packages (from pandas==2.2.0) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/veiga/.local/lib/python3.10/site-packages (from pandas==2.2.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/veiga/.local/lib/python3.10/site-packages (from pandas==2.2.0) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/veiga/.local/lib/python3.10/site-packages (from pandas==2.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.0) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y  pandas\n",
    "! pip install pandas==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a263f48b",
   "metadata": {
    "papermill": {
     "duration": 5.947684,
     "end_time": "2024-04-15T22:19:13.810386",
     "exception": false,
     "start_time": "2024-04-15T22:19:07.862702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e46b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.033966,
     "end_time": "2024-04-15T22:19:13.878977",
     "exception": false,
     "start_time": "2024-04-15T22:19:13.845011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This dataset is comprised of general information about the loans' clients.\n",
    "        </li>    \n",
    "        <li> \n",
    "            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n",
    "            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641eefe6",
   "metadata": {
    "papermill": {
     "duration": 8.166193,
     "end_time": "2024-04-15T22:19:22.142294",
     "exception": false,
     "start_time": "2024-04-15T22:19:13.976101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
      "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_clients = 'data/application_record.csv'\n",
    "\n",
    "# Defining the data types from the clients dataset.\n",
    "schema_clients = '''\n",
    "`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n",
    "`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n",
    "`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n",
    "`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n",
    "'''\n",
    "\n",
    "# Reading the database with the created schema.\n",
    "df_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10363c9f",
   "metadata": {
    "papermill": {
     "duration": 0.034961,
     "end_time": "2024-04-15T22:19:22.217367",
     "exception": false,
     "start_time": "2024-04-15T22:19:22.182406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "             Clients may not have unique rows in the dataset because the ID column identifies a contracted loan instead of a person.\n",
    "        </li>\n",
    "        <li> \n",
    "            Thus, I've found convenient for the project to create an ID column that assigns a code for each of the clients.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91cbcc55",
   "metadata": {
    "papermill": {
     "duration": 4.235329,
     "end_time": "2024-04-15T22:19:26.487791",
     "exception": false,
     "start_time": "2024-04-15T22:19:22.252462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     ID|count|\n",
      "+-------+-----+\n",
      "|7742298|    2|\n",
      "|7174719|    2|\n",
      "|7091721|    2|\n",
      "|7089090|    2|\n",
      "|7022197|    2|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another issue unnoticed by the Data Engineers is that the database contains repeated Loan ID's.\n",
    "from pyspark.sql.functions import max as ps_max\n",
    "\n",
    "# Observe that there are Loans mentioned two times. It would be proper to disconsider them. \n",
    "data_duplicate_id = (df_clients\n",
    "                     .groupBy('ID')\n",
    "                     .count()\n",
    "                     .filter('`count`>1')\n",
    "                            )\n",
    "data_duplicate_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1190f03b",
   "metadata": {
    "papermill": {
     "duration": 2.579485,
     "end_time": "2024-04-15T22:19:29.116183",
     "exception": false,
     "start_time": "2024-04-15T22:19:26.536698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|7742298|          F|           N|              Y|           0|        144000.0|         Working|Secondary / secon...|             Widow|House / apartment|    -20626|        -1455|         1|              0|         0|         0|  Cooking staff|            1.0|\n",
      "|7742298|          M|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -18239|        -5428|         1|              1|         0|         0|           NULL|            2.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we take a peek on the first mentioned ID, we can notice that the presented loan actually is assigned to two different people!\n",
    "\n",
    "# Thus, the presence of such deals is potentially harmful to our model. It would be sensible to discard such ID's from the database.\n",
    "df_clients.filter('`ID`==7742298').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f070967a",
   "metadata": {
    "papermill": {
     "duration": 0.153045,
     "end_time": "2024-04-15T22:19:29.310904",
     "exception": false,
     "start_time": "2024-04-15T22:19:29.157859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping out the problematic loans.\n",
    "df_clients = data_duplicate_id.join(df_clients, how='right', on='ID').where('`count` IS NULL').drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbf8a8ae",
   "metadata": {
    "papermill": {
     "duration": 0.099904,
     "end_time": "2024-04-15T22:19:29.462149",
     "exception": false,
     "start_time": "2024-04-15T22:19:29.362245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'CNT_CHILDREN',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'FLAG_MOBIL',\n",
       " 'FLAG_WORK_PHONE',\n",
       " 'FLAG_PHONE',\n",
       " 'FLAG_EMAIL',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'CNT_FAM_MEMBERS']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the `df_clients` features with the exception of ID.\n",
    "features_clients = df_clients.columns\n",
    "features_clients.remove('ID')\n",
    "features_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09b95e48",
   "metadata": {
    "papermill": {
     "duration": 8.883368,
     "end_time": "2024-04-15T22:19:38.397833",
     "exception": false,
     "start_time": "2024-04-15T22:19:29.514465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`df_clients` length: 438463\n",
      "Number of clients: 90084\n"
     ]
    }
   ],
   "source": [
    "# Now, getting back to the Client's ID issue, I'd like to present a brief analysis on it.\n",
    "# Note that the database's actual amount of clients is lower than its number of rows. \n",
    "data_clients = df_clients.dropDuplicates(features_clients) \n",
    "print(f'`df_clients` length: {df_clients.count()}')\n",
    "print(f'Number of clients: {data_clients.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57b32054-0967-407d-a030-27077f9948ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----+\n",
      "|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|count|\n",
      "+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----+\n",
      "|          F|           N|              N|           1|         81000.0|       State servant|    Higher education|             Married|     With parents|    -11707|        -2317|         1|              0|         0|         0|     Core staff|            3.0|  115|\n",
      "|          F|           N|              N|           0|         90000.0|           Pensioner|    Higher education|               Widow|House / apartment|    -22791|       365243|         1|              0|         1|         0|           NULL|            1.0|   61|\n",
      "|          F|           N|              Y|           0|        157500.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -14055|        -1986|         1|              0|         0|         0|  Cooking staff|            1.0|   60|\n",
      "|          M|           N|              Y|           0|        135000.0|Commercial associate|Secondary / secon...|      Civil marriage|House / apartment|     -9975|          -85|         1|              0|         0|         0|       Laborers|            2.0|   59|\n",
      "|          F|           N|              Y|           0|         90000.0|Commercial associate|Secondary / secon...|      Civil marriage|House / apartment|    -11916|        -1361|         1|              0|         0|         0|       Laborers|            2.0|   59|\n",
      "+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we group our data by all the features, you can notice that rows with the exact same information repeteatly show up.\n",
    "# Taking that in mind and the fact that such repeated rows still present different ID's, we conclude that what we actually received\n",
    "# is a dataset relating client information to a loan ID.\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "df_clients.groupBy(features_clients).count().orderBy(desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c05bb32",
   "metadata": {
    "papermill": {
     "duration": 0.24844,
     "end_time": "2024-04-15T22:19:38.698554",
     "exception": false,
     "start_time": "2024-04-15T22:19:38.450114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll assign an ID for every client mentioned in `df_clients`. \n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(features_clients)\n",
    "row_window = row_number().over(window)\n",
    "\n",
    "# A DataFrame with the clients' data and actual ID.\n",
    "df_id_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2096f8c3",
   "metadata": {
    "papermill": {
     "duration": 12.031835,
     "end_time": "2024-04-15T22:19:50.769323",
     "exception": false,
     "start_time": "2024-04-15T22:19:38.737488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|ID_CLIENT|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|5996382|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996383|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996384|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|6100451|          F|           N|              N|           0|         31500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22763|       365243|         1|              0|         1|         0|           NULL|            2.0|       15|\n",
      "|6100452|          F|           N|              N|           0|         31500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22763|       365243|         1|              0|         1|         0|           NULL|            2.0|       15|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll need to perform Null Safe JOIN's, since columns such as 'OCCUPATION_TYPE' contain null values.\n",
    "from functools import reduce\n",
    "\n",
    "# Creating the multiple null safe JOIN's condition.\n",
    "condition_id_client = reduce(lambda x,y: x&y, [df_clients[col].eqNullSafe(df_id_clients[col]) for col in features_clients])\n",
    "columns_join_id = ['df_clients.*', 'df_id_clients.ID_CLIENT'] # Listing the output columns.\n",
    "\n",
    "# Consolidating the final clients database.\n",
    "df_clients = (df_clients.alias('df_clients') # Resorting to aliases for both DataFrames present columns with same names.\n",
    "                 .join(df_id_clients.alias('df_id_clients'), condition_id_client)\n",
    "                 .select(columns_join_id))\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e6fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.038831,
     "end_time": "2024-04-15T22:19:50.864770",
     "exception": false,
     "start_time": "2024-04-15T22:19:50.825939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This table contains the payments records for every loan since its contraction. \n",
    "        </li>\n",
    "        <li> \n",
    "            But in order to the dataset be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n",
    "            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebe74010",
   "metadata": {
    "papermill": {
     "duration": 0.225283,
     "end_time": "2024-04-15T22:19:51.127568",
     "exception": false,
     "start_time": "2024-04-15T22:19:50.902285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------+\n",
      "|     ID|MONTHS_BALANCE|STATUS|\n",
      "+-------+--------------+------+\n",
      "|5001711|             0|     X|\n",
      "|5001711|            -1|     0|\n",
      "|5001711|            -2|     0|\n",
      "|5001711|            -3|     0|\n",
      "|5001712|             0|     C|\n",
      "+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bringing the dataset into our notebook.\n",
    "path_loans = 'data/credit_record.csv'\n",
    "schema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\n",
    "df_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\n",
    "df_loans.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa93824d",
   "metadata": {
    "papermill": {
     "duration": 14.045724,
     "end_time": "2024-04-15T22:20:05.211429",
     "exception": false,
     "start_time": "2024-04-15T22:19:51.165705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------+------+\n",
      "|ID_CLIENT|     ID|MONTHS_BALANCE|STATUS|\n",
      "+---------+-------+--------------+------+\n",
      "|    34269|5008810|             0|     C|\n",
      "|    34269|5008810|            -1|     C|\n",
      "|    34269|5008810|            -2|     C|\n",
      "|    34269|5008810|            -3|     C|\n",
      "|    34269|5008810|            -4|     C|\n",
      "+---------+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, providing the loans' client ID.\n",
    "df_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\n",
    "df_loans.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a622c8",
   "metadata": {
    "papermill": {
     "duration": 0.051067,
     "end_time": "2024-04-15T22:20:05.320753",
     "exception": false,
     "start_time": "2024-04-15T22:20:05.269686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n",
    "            <table style='font-size:15px;margin-top:20px'> \n",
    "                <tr>\n",
    "                    <th> Code</th>\n",
    "                    <th> Definition</th>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> C</td>\n",
    "                    <td> Paid off that month</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 0</td>\n",
    "                    <td> 1-29 days past due</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 1</td>\n",
    "                    <td> 30-59 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 2</td>\n",
    "                    <td> 60-89 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 3</td>\n",
    "                    <td> 90-119 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 4</td>\n",
    "                    <td> 120-149 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 5</td>\n",
    "                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> X</td>\n",
    "                    <td> No loan for the month</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Observe that in our case only the 3, 4 and 5 codes are of our interest. Thus it would be convenient to create a binary flag that denounces whether \n",
    "            the individual has ever caused an NPL.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3c4f883",
   "metadata": {
    "papermill": {
     "duration": 1.231101,
     "end_time": "2024-04-15T22:20:06.591419",
     "exception": false,
     "start_time": "2024-04-15T22:20:05.360318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Finally, generating our target-variable.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mdf_loans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID_CLIENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplyInPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_npl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_flag_npl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:201\u001b[0m, in \u001b[0;36mPandasGroupedOpsMixin.applyInPandas\u001b[0;34m(self, func, schema)\u001b[0m\n\u001b[1;32m    198\u001b[0m udf \u001b[38;5;241m=\u001b[39m pandas_udf(\n\u001b[1;32m    199\u001b[0m     func, returnType\u001b[38;5;241m=\u001b[39mschema, functionType\u001b[38;5;241m=\u001b[39mPandasUDFType\u001b[38;5;241m.\u001b[39mGROUPED_MAP)\n\u001b[1;32m    200\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n\u001b[0;32m--> 201\u001b[0m udf_column \u001b[38;5;241m=\u001b[39m \u001b[43mudf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39mflatMapGroupsInPandas(udf_column\u001b[38;5;241m.\u001b[39m_jc\u001b[38;5;241m.\u001b[39mexpr())\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/udf.py:199\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/udf.py:177\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[0;32m--> 177\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_judf\u001b[49m\n\u001b[1;32m    178\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/udf.py:161\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/udf.py:170\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    168\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[0;32m--> 170\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    172\u001b[0m judf \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexecution\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mUserDefinedPythonFunction(\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, wrapped_func, jdt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/udf.py:35\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[1;32m     34\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickled_command\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpythonExec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpythonVer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_javaAccumulator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1573\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1567\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1569\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1570\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1572\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1573\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1577\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"
     ]
    }
   ],
   "source": [
    "# The dependent variable's conception needs a custom GroupBy that PySpark is unable to perform by itself. Hence, we \n",
    "# are going to resort to pandas in this section.\n",
    "import pandas as pd\n",
    "\n",
    "# Defining the GroupBy's schema.\n",
    "schema_flag_npl = '`ID_CLIENT` STRING, `NPL` STRING'\n",
    "\n",
    "# This lambda expression signs whether a client has ever produced an NPL in the past.\n",
    "lambda_npl = lambda x: '1' if x.STATUS.isin(['3', '4', '5']).any() else '0'\n",
    "\n",
    "def has_npl(df:pd.DataFrame)->pd.DataFrame:\n",
    "    '''\n",
    "        Verifies if a client's  records contain any sort of Non-Performing Loan.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `df`: pd.DataFrame \n",
    "            The loan records of a certain client.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A `pd.DataFrame` with the client's ID and a flag indicating NPL existence in their loan history. \n",
    "    '''\n",
    "    output = df.groupby(['ID_CLIENT']).apply(lambda_npl) # `lambda_npl` takes care of the flags creation.\n",
    "    output.name = 'NPL' # Setting the flag column's name.\n",
    "    return output.reset_index()\n",
    "\n",
    "# Finally, generating our target-variable.\n",
    "target = df_loans.groupBy('ID_CLIENT').applyInPandas(has_npl, schema_flag_npl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.042101,
     "end_time": "2024-04-15T22:20:06.685305",
     "exception": false,
     "start_time": "2024-04-15T22:20:06.643204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Consolidating the Data</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With both datasets properly treated, we are able to JOIN them in a single table.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996b9de",
   "metadata": {
    "papermill": {
     "duration": 0.113114,
     "end_time": "2024-04-15T22:20:06.837776",
     "exception": false,
     "start_time": "2024-04-15T22:20:06.724662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, enriching the clients information with the NPL flag.\n",
    "df = df_id_clients.join(target, how='inner', on='ID_CLIENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccc466",
   "metadata": {
    "papermill": {
     "duration": 0.066073,
     "end_time": "2024-04-15T22:20:06.959640",
     "exception": false,
     "start_time": "2024-04-15T22:20:06.893567",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.parquet('data/dataset-npl.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2472ea2",
   "metadata": {
    "papermill": {
     "duration": 0.038386,
     "end_time": "2024-04-15T22:20:07.037753",
     "exception": false,
     "start_time": "2024-04-15T22:20:06.999367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Dataset Splitting</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the dataset properly treated, we are able to begin our EDA and model creation. But firstly we have to separate the data in \n",
    "            the training and test tables.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f1053",
   "metadata": {
    "papermill": {
     "duration": 65.542095,
     "end_time": "2024-04-15T22:21:12.618615",
     "exception": false,
     "start_time": "2024-04-15T22:20:07.076520",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we can see, we are dealing with an unbalanced dataset case. Thus it is interesting to maintain the target value proportions\n",
    "# in both training and test sets.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.groupby('NPL').count().withColumn('proportion', col('count')/df.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a633f6",
   "metadata": {
    "papermill": {
     "duration": 0.562933,
     "end_time": "2024-04-15T22:21:13.238281",
     "exception": false,
     "start_time": "2024-04-15T22:21:12.675348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the training and test sets.\n",
    "train = df.sampleBy('NPL', fractions={'0':.75, '1':.75}, seed=42)\n",
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a6f59",
   "metadata": {
    "papermill": {
     "duration": 112.518003,
     "end_time": "2024-04-15T22:23:05.805789",
     "exception": false,
     "start_time": "2024-04-15T22:21:13.287786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since the class proportions are relatively proximate, we are able to use them in our project.\n",
    "print('*** TRAIN ***')\n",
    "train.groupBy('NPL').count().withColumn('proportion', col('count')/train.count()).show()\n",
    "print('*** TEST ***')\n",
    "test.groupBy('NPL').count().withColumn('proportion', col('count')/test.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec96239-331e-4134-8802-095cdca6ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.pandas import __file__\n",
    "\n",
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dd709",
   "metadata": {
    "papermill": {
     "duration": 0.059313,
     "end_time": "2024-04-15T22:23:05.924331",
     "exception": false,
     "start_time": "2024-04-15T22:23:05.865018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the datasets in distinct .parquet files.\n",
    "# train.write.parquet('/kaggle/working/train.parquet', mode='overwrite')\n",
    "# test.write.parquet('/kaggle/working/test.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd8a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.04957,
     "end_time": "2024-04-15T22:23:06.023280",
     "exception": false,
     "start_time": "2024-04-15T22:23:05.973710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Defining the Models' Metric</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Providing False Negatives would clearly be more harmful for the bank's equity than False Positives.         \n",
    "        </li>\n",
    "        <li> \n",
    "            Following conversations with the credit analysts, we've ended up defining the case's official metric\n",
    "            as an f-score with $\\beta=4$. So, we are giving to Recall an importance that is 4x higher than the Precision's.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf7082",
   "metadata": {
    "papermill": {
     "duration": 0.049071,
     "end_time": "2024-04-15T22:23:06.121527",
     "exception": false,
     "start_time": "2024-04-15T22:23:06.072456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Exploratory Data Analysis</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the data properly segregated, let's briefly analyze its content and see whether we can spot differences \n",
    "            among the classes. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef856f",
   "metadata": {
    "papermill": {
     "duration": 14.071269,
     "end_time": "2024-04-15T22:23:20.241888",
     "exception": false,
     "start_time": "2024-04-15T22:23:06.170619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588844a7",
   "metadata": {
    "papermill": {
     "duration": 0.072391,
     "end_time": "2024-04-15T22:23:20.364645",
     "exception": false,
     "start_time": "2024-04-15T22:23:20.292254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248f80e",
   "metadata": {
    "papermill": {
     "duration": 1.090572,
     "end_time": "2024-04-15T22:23:21.505604",
     "exception": false,
     "start_time": "2024-04-15T22:23:20.415032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = spark.read.parquet('/kaggle/input/npl-train/train.parquet/')\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38f0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.049349,
     "end_time": "2024-04-15T22:23:21.604811",
     "exception": false,
     "start_time": "2024-04-15T22:23:21.555462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Assessing the Classes' Incomes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Let's analyze whether there is any revenue difference between individuals who produced an NPL and those who didn't. \n",
    "        </li>\n",
    "        <li> \n",
    "            Since we are dealing with an continuous outcome from two independent samples, I'll use the z-score formula below to examine the \n",
    "            income differences:\n",
    "            <center style='margin-top:20px'> \n",
    "                    $z=\\frac{\\overline{X}_{1}-\\overline{X}_{2}}{S_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$ | $S_{p}=\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$\n",
    "           </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11650b38",
   "metadata": {
    "papermill": {
     "duration": 0.423883,
     "end_time": "2024-04-15T22:23:22.080979",
     "exception": false,
     "start_time": "2024-04-15T22:23:21.657096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The formula above is only valid for samples with similar std's (0.5<s1/s2<2). So we ought to firstly guarantee that the incomes' std's\n",
    "# are in accordance to that rule.\n",
    "df_income_std = train.groupBy('NPL').agg({'AMT_INCOME_TOTAL':'std'})\n",
    "df_income_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b812c87",
   "metadata": {
    "papermill": {
     "duration": 0.308362,
     "end_time": "2024-04-15T22:23:22.440836",
     "exception": false,
     "start_time": "2024-04-15T22:23:22.132474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Because the std ratio is in the desired interval, we can proceed in using the formula.\n",
    "list_std = [row['stddev(AMT_INCOME_TOTAL)'] for row in df_income_std.collect()]\n",
    "list_std[0] / list_std[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeff80c",
   "metadata": {
    "papermill": {
     "duration": 1.951582,
     "end_time": "2024-04-15T22:23:24.455293",
     "exception": false,
     "start_time": "2024-04-15T22:23:22.503711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By setting our alpha=.05, we can see that there is no statistical evidence that people with no NPL receive higher incomes\n",
    "# than the other group.\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "zeros = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==0').collect()]\n",
    "ones = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==1').collect()]\n",
    "\n",
    "ztest(zeros, ones, alternative='larger')[1] # Computing our p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033b414",
   "metadata": {
    "papermill": {
     "duration": 0.071245,
     "end_time": "2024-04-15T22:23:24.600616",
     "exception": false,
     "start_time": "2024-04-15T22:23:24.529371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Valuable Assets Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The dataset contains columns representing whether the client possesses real state or a vehicle.\n",
    "        </li>\n",
    "        <li> \n",
    "            We can verify if people that own such properties have lower chance of producing an NPL, because they could sell them if they don't have enough \n",
    "            cash to pay the loans. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5353c",
   "metadata": {
    "papermill": {
     "duration": 0.521428,
     "end_time": "2024-04-15T22:23:25.196953",
     "exception": false,
     "start_time": "2024-04-15T22:23:24.675525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def own_valuable(col_car:str, col_realty:str)->str:\n",
    "    '''\n",
    "        Signs whether a client possesses a valuable asset that can be sold for paying their loans.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_car` str\n",
    "            The value of the flag that indicates the ownership of a car.\n",
    "        `col_realty`: str\n",
    "            The value of the flag that indicates the ownership of a real state.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A flag indicating the possession of a car or real state (valuable assets). \n",
    "    '''\n",
    "    if (col_car=='Y') or (col_realty=='Y'):\n",
    "        return '1'\n",
    "    return '0'\n",
    "\n",
    "# Creating the DataFrame that will be used for conducting the Hypothesis Test.\n",
    "df_valuable = train.select(own_valuable('FLAG_OWN_CAR', 'FLAG_OWN_REALTY').alias('OWN_VALUABLE'), 'NPL')\n",
    "df_valuable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe9cb5",
   "metadata": {
    "papermill": {
     "duration": 0.896944,
     "end_time": "2024-04-15T22:23:26.158642",
     "exception": false,
     "start_time": "2024-04-15T22:23:25.261698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting the amount of who own or not a valuable asset per target.\n",
    "gb_valuable = (df_valuable\n",
    "     .groupBy(['NPL', 'OWN_VALUABLE'])\n",
    "     .count())\n",
    "\n",
    "gb_valuable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b1405",
   "metadata": {
    "papermill": {
     "duration": 1.136542,
     "end_time": "2024-04-15T22:23:27.348919",
     "exception": false,
     "start_time": "2024-04-15T22:23:26.212377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, using `gb_valuable` in order to estimate the proportion of clients with a valuable asset for each target.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as ps_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('NPL')\n",
    "\n",
    "# It's noticeable that there is a rough 7% difference between the proportions. But can we regard that as statistically significant? \n",
    "(gb_valuable\n",
    "     .withColumn('TOTAL_INSTANCES', ps_sum(col('count')).over(window)) # Total quantity of instances for each target.\n",
    "     .withColumn('PROP_OWN_VALUABLE', col('count')/ps_sum(col('count')).over(window)) # % of clients who own valuable assets per target.\n",
    "     .withColumnRenamed('count', 'INSTANCES_WITH_VALUABLE') \n",
    "     .where('`OWN_VALUABLE`==1')\n",
    "     .select(['NPL', 'INSTANCES_WITH_VALUABLE', 'TOTAL_INSTANCES', 'PROP_OWN_VALUABLE'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924e81e",
   "metadata": {
    "papermill": {
     "duration": 0.093775,
     "end_time": "2024-04-15T22:23:27.518073",
     "exception": false,
     "start_time": "2024-04-15T22:23:27.424298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# H0: People who honor their debts have the same probability of possessing a valuable asset as the clients in default.\n",
    "# H1: People who honor their debts have higher probability of possessing a valuable asset compared to clients in default.\n",
    "\n",
    "# By using the table's data in a Hypothesis Test for proportions, we get a p-value below our significance degree (alpha=.05).\n",
    "\n",
    "# Therefore, we are able to reject H0 and state we possess sufficient information to say that people who have never produced an NPL have a higher\n",
    "# tendency of possessing a valuable asset.\n",
    "\n",
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "test_proportions_2indep(5747, 7201, 126, 172, compare='ratio', alternative='larger').pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb599a",
   "metadata": {
    "papermill": {
     "duration": 0.068962,
     "end_time": "2024-04-15T22:23:27.641242",
     "exception": false,
     "start_time": "2024-04-15T22:23:27.572280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With the difference proved real, let's analyze it with the use of a Cohen's h.\n",
    "from numpy import arcsin, sqrt\n",
    "def cohen_h(p1:float, p2:float)->float:\n",
    "    '''\n",
    "        Computes the Cohen's h coefficient for the difference between two proportions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `p1`: float\n",
    "            The first probability.\n",
    "        `p2`: float\n",
    "            The second probability.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        The Cohen's h coefficient for the given proportions.\n",
    "    '''\n",
    "    phi1 = 2*arcsin(sqrt(p1))\n",
    "    phi2 = 2*arcsin(sqrt(p2))\n",
    "    return phi1-phi2\n",
    "\n",
    "# By observing the returned h, we can conclude that the difference is of a small magnitude, since it is lower than 0.2.\n",
    "cohen_h(0.7980835995000695, 0.7325581395348837)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf083c",
   "metadata": {
    "papermill": {
     "duration": 0.051016,
     "end_time": "2024-04-15T22:23:27.746685",
     "exception": false,
     "start_time": "2024-04-15T22:23:27.695669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Employment Impact Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Finally, it would be interesting if we could verify whether unemployment can impair loan payments.  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970577cd",
   "metadata": {
    "papermill": {
     "duration": 0.051851,
     "end_time": "2024-04-15T22:23:27.850613",
     "exception": false,
     "start_time": "2024-04-15T22:23:27.798762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Cleaning up Inconsistent Instances</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The dataset contains individuals who present their OCCUPATION_TYPE value as NULL, although they show up as employed (DAYS_EMPLOYED$\\leq{0}$)\n",
    "        </li>\n",
    "        <li>\n",
    "            In order to they not be regarded as unemployed clients (who also present OCCUPATION_TYPE as NULL), I thought it would be convenient to assign \n",
    "            another category to them.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f08f5",
   "metadata": {
    "papermill": {
     "duration": 0.065248,
     "end_time": "2024-04-15T22:23:27.968601",
     "exception": false,
     "start_time": "2024-04-15T22:23:27.903353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def impute_occupation(col_occupation:str, col_days_employed:int)->str:\n",
    "    '''\n",
    "        Assigns new categories to the OCCUPATION_TYPE column if it is null for any given row. \n",
    "        \n",
    "        In case the DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_occupation`: str\n",
    "            The row's OCCUPATION_TYPE value.\n",
    "        `col_days_employed`: int\n",
    "            The row's DAYS_EMPLOYED value.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The row's OCCUPATION_TYPE treated value.\n",
    "    '''\n",
    "    # 'Undefined' profession logic.\n",
    "    if (col_occupation is None) and (col_days_employed<=0):\n",
    "        return 'Undefined'\n",
    "    \n",
    "    # 'Unemployed' logic.\n",
    "    elif (col_occupation is None) and (col_days_employed>0):\n",
    "        return 'Unemployed'\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return col_occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b60b6",
   "metadata": {
    "papermill": {
     "duration": 0.093329,
     "end_time": "2024-04-15T22:23:28.114638",
     "exception": false,
     "start_time": "2024-04-15T22:23:28.021309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `list_select_occupation` will be used so that we can redefine the `train` DataFrame with the transformed OCCUPATION_TYPE feature.\n",
    "list_select_occupation = train.columns\n",
    "list_select_occupation.remove('OCCUPATION_TYPE')\n",
    "list_select_occupation.insert(-1, impute_occupation('OCCUPATION_TYPE', 'DAYS_EMPLOYED').alias('OCCUPATION_TYPE'))\n",
    "list_select_occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38689ac3",
   "metadata": {
    "papermill": {
     "duration": 0.48191,
     "end_time": "2024-04-15T22:23:28.650670",
     "exception": false,
     "start_time": "2024-04-15T22:23:28.168760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying the modifications to `train`.\n",
    "train = train.select(list_select_occupation)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb144c75",
   "metadata": {
    "papermill": {
     "duration": 0.051711,
     "end_time": "2024-04-15T22:23:28.767168",
     "exception": false,
     "start_time": "2024-04-15T22:23:28.715457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Back to the EDA...</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a37f2",
   "metadata": {
    "papermill": {
     "duration": 1.001375,
     "end_time": "2024-04-15T22:23:29.819859",
     "exception": false,
     "start_time": "2024-04-15T22:23:28.818484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting the amount of employed and unemployed individuals per target group.\n",
    "from pyspark.sql.functions import when\n",
    "(train\n",
    "     .withColumn('EMPLOYED', when(col('OCCUPATION_TYPE') == 'Unemployed', 0).otherwise(1))\n",
    "     .groupBy('NPL', 'EMPLOYED')\n",
    "     .count()\n",
    "     .orderBy('NPL', 'EMPLOYED')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8104b",
   "metadata": {
    "papermill": {
     "duration": 0.06543,
     "end_time": "2024-04-15T22:23:29.939523",
     "exception": false,
     "start_time": "2024-04-15T22:23:29.874093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# H0: The proportion of unemployed among people with honored debts is the same as with those who didn't.\n",
    "# H1: The proportion of unemployed among people with honored debts is lower than that of those who didn't.\n",
    "\n",
    "# By looking at the p-value, we don't have sufficient statistical evidence to reject H0.\n",
    "test_proportions_2indep(1269, 7201, 31, 172, compare='ratio', alternative='smaller', ).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed780c3",
   "metadata": {
    "papermill": {
     "duration": 1.455062,
     "end_time": "2024-04-15T22:23:31.446835",
     "exception": false,
     "start_time": "2024-04-15T22:23:29.991773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the newly transformed `train` dataset.\n",
    "train.write.parquet('/kaggle/working/train.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b92e5",
   "metadata": {
    "papermill": {
     "duration": 0.05197,
     "end_time": "2024-04-15T22:23:31.554045",
     "exception": false,
     "start_time": "2024-04-15T22:23:31.502075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Transforming </h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            We are very close to begin the creation of our models. But we must remind that the data firstly needs to go under a numeralization procedure.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cf7e1",
   "metadata": {
    "papermill": {
     "duration": 14.045849,
     "end_time": "2024-04-15T22:23:45.652605",
     "exception": false,
     "start_time": "2024-04-15T22:23:31.606756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e063b5d",
   "metadata": {
    "papermill": {
     "duration": 0.07063,
     "end_time": "2024-04-15T22:23:45.776640",
     "exception": false,
     "start_time": "2024-04-15T22:23:45.706010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70f584",
   "metadata": {
    "papermill": {
     "duration": 0.319207,
     "end_time": "2024-04-15T22:23:46.149319",
     "exception": false,
     "start_time": "2024-04-15T22:23:45.830112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = spark.read.parquet('/kaggle/input/npl-train/train-eda.parquet/')\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108da5d",
   "metadata": {
    "papermill": {
     "duration": 0.052089,
     "end_time": "2024-04-15T22:23:46.268976",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.216887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Job Labels' Transformer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Before making our numeralization process, we must re-implement the 'OCCUPATION_TYPE' column logic that we've written in a formal \n",
    "            PySpark Transformer class. In that way, we'll be able to apply the transformations in the test set as well.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa9819",
   "metadata": {
    "papermill": {
     "duration": 0.166685,
     "end_time": "2024-04-15T22:23:46.488366",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.321681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from typing import List\n",
    "\n",
    "class _BaseTransformer(Transformer, HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    '''\n",
    "        A class that provide the basic functionalities for the project's custom transformers.\n",
    "        \n",
    "        If you consider that your object will require even more customization, you can just overwrite any of the default methods when creating it. Also,\n",
    "        remember that any extra parameter must be set with a `self._setDefault` method right in the `__init__` function, mentioned in the `self.setParams`\n",
    "        method and own a getter function.\n",
    "        \n",
    "        Lastly, don't forget to define the `_transform` function!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `inputCol`: str\n",
    "            The name of the input column.\n",
    "        `inputCols`: List[str]\n",
    "            List containing the name of input columns.\n",
    "        `outputCol`: str\n",
    "            The name of the output column.\n",
    "        `outputCols`: List[str]\n",
    "            List containing the name of output columns.\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        https://medium.com/@zeid.zandi/utilizing-the-power-of-pyspark-pipelines-in-data-science-projects-benefits-and-limitations-2-2-9063e4bebd05\n",
    "        https://www.crowdstrike.com/blog/deep-dive-into-custom-spark-transformers-for-machine-learning-pipelines/\n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol:str=None, inputCols:List[str]=None, outputCol:str=None, outputCols:List[str]=None, *args)->None:\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, inputCols=None, outputCol=None, outputCols=None): \n",
    "        kwargs = self._input_kwargs                                                          \n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def setInputCol(self, value):\n",
    "        return self.setParams(inputCol=value)\n",
    "    \n",
    "    def setInputCols(self, value):\n",
    "        self.setParams(inputCols=value)\n",
    "    \n",
    "    def setOutputCol(self, value):\n",
    "        return self.setParams(outputCol=value)\n",
    "    \n",
    "    def setOutputCols(self, value):\n",
    "        return self.setParams(outputCols=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90a907",
   "metadata": {
    "papermill": {
     "duration": 0.068158,
     "end_time": "2024-04-15T22:23:46.609213",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.541055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class ImputeOccupation(_BaseTransformer):\n",
    "    '''\n",
    "        A Transformer encharged of imputing proper OCCUPATION_TYPE values for rows that are NULL in such column.  \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `inputCol`: str\n",
    "            The name of the input column.\n",
    "            \n",
    "        Method\n",
    "        ------\n",
    "        `_transform`: Applies the mentioned imputation process,\n",
    "    '''\n",
    "    @staticmethod\n",
    "    @udf(returnType=StringType())\n",
    "    def __impute_occupation(col_occupation:str, col_days_employed:int)->str:\n",
    "        '''\n",
    "        Assigns new categories to the OCCUPATION_TYPE column, if it is null. \n",
    "        \n",
    "        In case the row's DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_occupation`: str\n",
    "            The row's OCCUPATION_TYPE value.\n",
    "        `col_days_employed`: int\n",
    "            The row's DAYS_EMPLOYED value.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The row's OCCUPATION_TYPE treated value.\n",
    "        '''\n",
    "        # 'Undefined' profession logic.\n",
    "        if (col_occupation is None) and (col_days_employed<=0):\n",
    "            return 'Undefined'\n",
    "\n",
    "        # 'Unemployed' logic.\n",
    "        elif (col_occupation is None) and (col_days_employed>0):\n",
    "            return 'Unemployed'\n",
    "        \n",
    "        # Otherwise, it outputs the current value.\n",
    "        else:\n",
    "            return col_occupation\n",
    "    \n",
    "    def _transform(self, dataset:DataFrame)->DataFrame:\n",
    "        '''\n",
    "            Performs the new occupation categories imputation.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `dataset`: `pyspark.sql.DataFrame`\n",
    "                The project's independent variables.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            The DataFrame with the treated 'OCCUPATION_TYPE' column.\n",
    "        '''\n",
    "        x = self.getInputCol()\n",
    "        dataset = dataset.withColumn(x, self.__impute_occupation(x, 'DAYS_EMPLOYED'))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450c60f",
   "metadata": {
    "papermill": {
     "duration": 0.053003,
     "end_time": "2024-04-15T22:23:46.715972",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.662969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> String Columns Numeralization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            The first transformation we must conduct is turning string columns into float format.\n",
    "        </li>\n",
    "        <li> \n",
    "            As a solution, I'ĺl replace the given category for the its proportion of instances that are defaulted. I think that will be a worthier way \n",
    "            of generating numerical information than just providing a One-Hot Encoding.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b21ed",
   "metadata": {
    "papermill": {
     "duration": 0.067287,
     "end_time": "2024-04-15T22:23:46.836775",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.769488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline, PipelineModel\n",
    "from pyspark.ml.param.shared import HasPredictionCol, Param, Params, TypeConverters\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "doc = '''\n",
    "    Parameter that stores the proportions of defaulted individuals for each X features' categories.\n",
    "'''\n",
    "class HasDefaultProportions(Params):\n",
    "    '''\n",
    "        A class dedicated to store the categories' proportion of indebted individuals. \n",
    "    '''\n",
    "    defaultProportions:Dict[str, Dict[str, float]] = Param(Params._dummy(), 'defaultProportions', doc)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HasDefaultProportions, self).__init__()\n",
    "    \n",
    "    def setDefaultProportions(self, value):\n",
    "        return self._set(defaultProportions=value)\n",
    "    \n",
    "    def getDefaultProportions(self):\n",
    "        return self.getOrDefault(self.defaultProportions)\n",
    "    \n",
    "list_col_strings = [col for col, dtype in train.dtypes if dtype=='string' and col !='NPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480c194",
   "metadata": {
    "papermill": {
     "duration": 0.069621,
     "end_time": "2024-04-15T22:23:46.961128",
     "exception": false,
     "start_time": "2024-04-15T22:23:46.891507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "class DefaultProportionsModel(Model, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    '''\n",
    "        The `pyspark.ml.pipeline.Model` that replaces categorical values for their respective proportion of defaulted individuals.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        `inputCols: List[str]\n",
    "            A list of the categorical features names.\n",
    "        `defaultProportions`: Dict[str, DataFrame]\n",
    "            A dictionary mapping the name of the categorical column to a DataFrame storing the proportions.\n",
    "            \n",
    "        Method\n",
    "        -------\n",
    "        `_transform`: Performs the replacements.\n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols:List[str]=None, defaultProportions:Dict[str,List[Dict[str, Union[str, float]]]]=None):\n",
    "        super(DefaultProportionsModel, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols:List[str]=None, defaultProportions:Dict[str,List[Dict[str, Union[str, float]]]]=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset:DataFrame)->DataFrame:\n",
    "        '''\n",
    "            Method that exchanges the categorical values for their proportion of defaulted individuals of the training set.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `dataset`: `pyspark.sql.dataframe.DataFrame` \n",
    "                A `pyspark.sql.dataframe.DataFrame` with the project's data.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            The DataFrame with the mentioned transformation.\n",
    "        '''\n",
    "        x = self.getInputCols()\n",
    "        defaultProportions = self.getDefaultProportions()\n",
    "        for c in x:\n",
    "            df = spark.createDataFrame(defaultProportions[c]) # Converting our list of dicts into a DataFrame.\n",
    "            dataset = dataset.join(df, on=c, how='left').drop(c)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee8ff8",
   "metadata": {
    "papermill": {
     "duration": 0.074898,
     "end_time": "2024-04-15T22:23:47.089015",
     "exception": false,
     "start_time": "2024-04-15T22:23:47.014117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DecimalType, FloatType\n",
    "\n",
    "class DefaultProportions(Estimator, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    '''\n",
    "        An Estimator encharged for measuring the categorical values proportions of defaulted individuals.\n",
    "    \n",
    "        Parameter\n",
    "        ---------\n",
    "        `inputCols`: List[str]\n",
    "            A list with the categorical columns names.\n",
    "            \n",
    "        Method\n",
    "        ------\n",
    "        `_fit`: Measures the proportions and stores them in a DataFrame.\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        https://stackoverflow.com/questions/37270446/how-to-create-a-custom-estimator-in-pyspark\n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols:List[str]=None):\n",
    "        super(DefaultProportions, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols:List[str]=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "        \n",
    "    def setInputCols(self, value:List[str]):\n",
    "        return self.setParams(inputCols=value)\n",
    "    \n",
    "    def setPredictionCol(self, value:str):\n",
    "        return self.setParams(predictionCol=value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __measure_proportions(dataset:DataFrame, input_cols:List[str], target:str)->Dict[str, DataFrame]:\n",
    "        '''\n",
    "            Responsible for quantifying the categories' proportions of indebted individuals. They will be stored in a `pyspark.sql.dataframe.DataFrame`\n",
    "            object. Each dataframe is kept in the `defaultedProportions` dictionary, which maps them to the categorical column's name.\n",
    "            \n",
    "            Paramters\n",
    "            ---------\n",
    "            `dataset`: DataFrame\n",
    "                A `pyspark.sql.dataframe.DataFrame` with the project's data.\n",
    "            `inputCols`: List[str]\n",
    "                A list with the categorical columns names.    \n",
    "            `target`: str\n",
    "                The `dataset`'s target name.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            A dictionary mapping the categorical columns' names to the DataFrame that stores the classes proportions.\n",
    "        '''\n",
    "        defaultProportions = {}\n",
    "        len_df = dataset.count()\n",
    "        for c in input_cols:\n",
    "            col_prop = c+'_NPL_PROP'\n",
    "            df_gb = dataset.groupBy([c, target]).count()\n",
    "            df_props = (df_gb\n",
    "                        .where(f'{target}==1')\n",
    "                        .withColumn(col_prop, (col('count')/len_df)\n",
    "                        .cast(FloatType()))#DecimalType(8,7)))\n",
    "                        .select([c, col_prop])) # De onde vem esse `row`?\n",
    "            defaultProportions[c] = list(map(lambda row: row.asDict(), df_props.collect())) # DF to list so we can be able saving the class.\n",
    "            print(defaultProportions[c])\n",
    "        return defaultProportions\n",
    "            \n",
    "    def _fit(self, dataset:DataFrame, target:str='NPL')->DefaultProportionsModel:\n",
    "        '''\n",
    "            Measures the proportions of indebted individuals from each of the dataset's independent categories.\n",
    "            \n",
    "            Parameters\n",
    "            ---------\n",
    "            `dataset`: `pyspark.sql.dataframe.DataFrame`\n",
    "                A `pyspark.sql.dataframe.DataFrame` with the project's data.   \n",
    "            `target`: str\n",
    "                The `dataset`'s target name.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            A fitted DefaultProportionsModel ready for transforming the dataset.\n",
    "        '''\n",
    "        x = self.getInputCols()\n",
    "        defaultProportions = self.__measure_proportions(dataset, x, target)\n",
    "        return DefaultProportionsModel(inputCols=x, defaultProportions=defaultProportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc559c",
   "metadata": {
    "papermill": {
     "duration": 3.863364,
     "end_time": "2024-04-15T22:23:51.004995",
     "exception": false,
     "start_time": "2024-04-15T22:23:47.141631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, we can insert both custom classes inside a pyspark pipeline!\n",
    "impute_occupation = ImputeOccupation(inputCol='OCCUPATION_TYPE')\n",
    "default_prop = DefaultProportions(inputCols=list_col_strings[1:])\n",
    "\n",
    "# Creating the pipe.\n",
    "pipe1 = Pipeline(stages=[impute_occupation, default_prop]).fit(train)\n",
    "train_num = pipe1.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501db440",
   "metadata": {
    "papermill": {
     "duration": 8.034612,
     "end_time": "2024-04-15T22:23:59.108639",
     "exception": false,
     "start_time": "2024-04-15T22:23:51.074027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can discard the ID_CLIENT column, since it has no predictive value.\n",
    "train_num = train_num.drop('ID_CLIENT')\n",
    "train_num.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e51cb",
   "metadata": {
    "papermill": {
     "duration": 0.054706,
     "end_time": "2024-04-15T22:23:59.235364",
     "exception": false,
     "start_time": "2024-04-15T22:23:59.180658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Imputing and Standardization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            As a last step, we also need to guarantee that our instances will have no NULL values. Also, we must standardize our numbers, in line\n",
    "            with ML best practices.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111a633",
   "metadata": {
    "papermill": {
     "duration": 15.488706,
     "end_time": "2024-04-15T22:24:14.780096",
     "exception": false,
     "start_time": "2024-04-15T22:23:59.291390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With all our features properly in numerical format, we can add a second Pipeline that handles imputing and standardization.\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n",
    "\n",
    "list_features = [c for c in train_num.columns if c!='NPL']\n",
    "\n",
    "imputer = Imputer(inputCols=list_features, outputCols=list_features)\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=list_features, outputCol='features', handleInvalid='skip')\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True)\n",
    "pipe2 = Pipeline(stages=[imputer, vector_assembler, scaler]).fit(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10ca6a",
   "metadata": {
    "papermill": {
     "duration": 7.738801,
     "end_time": "2024-04-15T22:24:22.580462",
     "exception": false,
     "start_time": "2024-04-15T22:24:14.841661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, creating the project's official transformation Pipeline! \n",
    "# We can declare it as a PipelineModel, since its sub-pipelines have already been fitted!\n",
    "pipe = PipelineModel(stages=[pipe1, pipe2])\n",
    "train_num = pipe.transform(train).select(['scaled_features', 'NPL'])\n",
    "train_num.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8365d",
   "metadata": {
    "papermill": {
     "duration": 7.178145,
     "end_time": "2024-04-15T22:24:29.845656",
     "exception": false,
     "start_time": "2024-04-15T22:24:22.667511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's a good idea to save our numerical training set.\n",
    "train_num.write.parquet('/kaggle/working/train_num.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226a8ed",
   "metadata": {
    "papermill": {
     "duration": 0.070338,
     "end_time": "2024-04-15T22:24:30.005543",
     "exception": false,
     "start_time": "2024-04-15T22:24:29.935205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also, registering our test set.\n",
    "#test_num = pipe.transform(test).select(['scaled_features', 'NPL'])\n",
    "#test_num.write.parquet('/kaggle/working/test_num.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838f52d",
   "metadata": {
    "papermill": {
     "duration": 2.220815,
     "end_time": "2024-04-15T22:24:32.287506",
     "exception": false,
     "start_time": "2024-04-15T22:24:30.066691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the model for future use.\n",
    "pipe.save('../working/pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76716655",
   "metadata": {
    "papermill": {
     "duration": 0.059959,
     "end_time": "2024-04-15T22:24:32.409655",
     "exception": false,
     "start_time": "2024-04-15T22:24:32.349696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Modeling Phase </h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Since we already have our features in numerical format, we can begin this section by briefly analyzing the distribution of the features \n",
    "            throughout the space. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a0448",
   "metadata": {
    "papermill": {
     "duration": 14.094509,
     "end_time": "2024-04-15T22:24:46.565375",
     "exception": false,
     "start_time": "2024-04-15T22:24:32.470866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afbd49",
   "metadata": {
    "papermill": {
     "duration": 0.079519,
     "end_time": "2024-04-15T22:24:46.706299",
     "exception": false,
     "start_time": "2024-04-15T22:24:46.626780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f6dba",
   "metadata": {
    "papermill": {
     "duration": 0.328025,
     "end_time": "2024-04-15T22:24:47.096149",
     "exception": false,
     "start_time": "2024-04-15T22:24:46.768124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_num = spark.read.parquet('/kaggle/input/npl-train/train-num.parquet')\n",
    "train_num.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666367e5",
   "metadata": {
    "papermill": {
     "duration": 0.061121,
     "end_time": "2024-04-15T22:24:47.219961",
     "exception": false,
     "start_time": "2024-04-15T22:24:47.158840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Instances Distribution Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Before directly diving into model trainings, I consider prudent to firstly check how our instances are distributed throughout the \n",
    "            feature space.\n",
    "        </li>\n",
    "        <li>\n",
    "            To support us in such task, I've decided to resort to t-SNE which will enable us to reduce the dataset's dimensionality.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbab2a",
   "metadata": {
    "papermill": {
     "duration": 41.721796,
     "end_time": "2024-04-15T22:25:29.003119",
     "exception": false,
     "start_time": "2024-04-15T22:24:47.281323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since PySpark does not include a built-in implementation of t-SNE, I felt obliged to use numpy and sklearn\n",
    "# just for the sake of the analysis.\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Applying t-SNE to our dataset.\n",
    "X = np.array(train_num.select('scaled_features').collect())[:,0,:]\n",
    "X_tsne = TSNE().fit_transform(X)\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0937319",
   "metadata": {
    "papermill": {
     "duration": 1.238681,
     "end_time": "2024-04-15T22:25:30.302979",
     "exception": false,
     "start_time": "2024-04-15T22:25:29.064298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the instances distribution.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.array(train_num.select('NPL').collect()).astype('int').flatten()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, s=30,\n",
    "           cmap='Accent')\n",
    "\n",
    "plt.title('Original Dataset Instances Distribution (t-SNE)')\n",
    "plt.legend(*scatter.legend_elements(), title='NPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a7581",
   "metadata": {
    "papermill": {
     "duration": 0.064196,
     "end_time": "2024-04-15T22:25:30.430876",
     "exception": false,
     "start_time": "2024-04-15T22:25:30.366680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            It must be disappointing to see such inconsistent distribution. We positively cannot expect nothing great from methods such as Linear SVM\n",
    "            or even tree-based algorithms with such dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            What we can do is to perform an algorithm sanity check over a kernelized space. If it overfits, we can conclude that the instances are \n",
    "            separable in a non-linear manner. If not, we can attempt to prune out a handful of features that may be detrimental to our project.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d5590",
   "metadata": {
    "papermill": {
     "duration": 0.072725,
     "end_time": "2024-04-15T22:25:30.566825",
     "exception": false,
     "start_time": "2024-04-15T22:25:30.494100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As PySpark has no implementation of kernelized SVM's, I've felt obliged again to use scikit-learn to carry out the test.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# n_check = 5\n",
    "# X_sample = np.random.randint(low=0, high=array.shape[0], size=5)#np.random.choice(array, size=(5, array.shape[1]))\n",
    "# svc = SVC.fit(X[array_sample], y[array_sample])\n",
    "#svc = SVC().fit(array, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a251f2-0014-42b9-be3a-e95d16ec9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add .\n",
    "! git commit -am 'Erro do GroupBy parou, mas está acontecendo outro com o Py4J'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be75e64",
   "metadata": {
    "papermill": {
     "duration": 0.063909,
     "end_time": "2024-04-15T22:25:30.694991",
     "exception": false,
     "start_time": "2024-04-15T22:25:30.631082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Corrigi instalação; Erro do GroupBy parou, mas está acontecendo outro com o Py4J\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 426827,
     "sourceId": 1031720,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3941029,
     "sourceId": 6856406,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3950750,
     "sourceId": 8048943,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3950752,
     "sourceId": 8048950,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 443.953024,
   "end_time": "2024-04-15T22:25:33.380501",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-15T22:18:09.427477",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
