{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1031720,"sourceType":"datasetVersion","datasetId":426827},{"sourceId":6856406,"sourceType":"datasetVersion","datasetId":3941029},{"sourceId":6875350,"sourceType":"datasetVersion","datasetId":3950752},{"sourceId":7372877,"sourceType":"datasetVersion","datasetId":3950750},{"sourceId":21798,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":18047}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n<div style='font-size:20px'> \n    <ul> \n        <li> \n            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n        </li>\n        <li> \n            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n            credit strategy must be adopted for new contracts.\n        </li>\n        <li> \n            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n            <ul style='list-style-type:decimal'> \n                <li> \n                    It must be focused on predicting whether a given client might produce an NPL in the future.\n                </li>\n                <li> \n                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n                    an incisive \"yes or no\" answer.\n                </li>\n            </ul>\n        </li>\n    </ul>\n    <p style='margin-left:30px'> <strong> Note:</strong> The bank's NPL definition is any loan which payment is at least 90 days late.</p>\n</div>","metadata":{"papermill":{"duration":0.006906,"end_time":"2023-09-08T16:00:22.310851","exception":false,"start_time":"2023-09-08T16:00:22.303945","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Data Importing</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n            and the second lists the loans they've contracted over some period of time.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"papermill":{"duration":52.936605,"end_time":"2023-09-08T16:01:15.267137","exception":false,"start_time":"2023-09-08T16:00:22.330532","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:24:07.465346Z","iopub.execute_input":"2023-11-20T13:24:07.467459Z","iopub.status.idle":"2023-11-20T13:25:03.172489Z","shell.execute_reply.started":"2023-11-20T13:24:07.467393Z","shell.execute_reply":"2023-11-20T13:25:03.170426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"papermill":{"duration":6.106913,"end_time":"2023-09-08T16:01:21.392365","exception":false,"start_time":"2023-09-08T16:01:15.285452","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:03.175992Z","iopub.execute_input":"2023-11-20T13:25:03.176424Z","iopub.status.idle":"2023-11-20T13:25:09.935377Z","shell.execute_reply.started":"2023-11-20T13:25:03.176389Z","shell.execute_reply":"2023-11-20T13:25:09.934182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This dataset is comprised of general information about the loans' clients.\n        </li>    \n        <li> \n            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.018275,"end_time":"2023-09-08T16:01:21.429645","exception":false,"start_time":"2023-09-08T16:01:21.411370","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path_clients = '/kaggle/input/credit-card-approval-prediction/application_record.csv'\n\n# Defining the data types from the clients dataset.\nschema_clients = '''\n`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n'''\n\n# Reading the database with the created schema.\ndf_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\ndf_clients.show(5)","metadata":{"papermill":{"duration":6.913786,"end_time":"2023-09-08T16:01:28.361906","exception":false,"start_time":"2023-09-08T16:01:21.448120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:09.937719Z","iopub.execute_input":"2023-11-20T13:25:09.939134Z","iopub.status.idle":"2023-11-20T13:25:18.194029Z","shell.execute_reply.started":"2023-11-20T13:25:09.939053Z","shell.execute_reply":"2023-11-20T13:25:18.192789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n             Clients may not have unique rows in the dataset because the ID column identifies a contracted loan instead of a person.\n        </li>\n        <li> \n            Thus, I've found convenient for the project to create an ID column that assigns a code for each of the clients.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.026529,"end_time":"2023-09-08T16:01:28.418517","exception":false,"start_time":"2023-09-08T16:01:28.391988","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Another issue unnoticed by the Data Engineers is that the database contains repeated Loan ID's.\nfrom pyspark.sql.functions import max as ps_max\n\n# Observe that there are Loans mentioned two times. It would be proper to disconsider them. \ndata_duplicate_id = (df_clients\n                     .groupBy('ID')\n                     .count()\n                     .filter('`count`>1')\n                            )\ndata_duplicate_id.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:25:18.197337Z","iopub.execute_input":"2023-11-20T13:25:18.197797Z","iopub.status.idle":"2023-11-20T13:25:22.614005Z","shell.execute_reply.started":"2023-11-20T13:25:18.197756Z","shell.execute_reply":"2023-11-20T13:25:22.612774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If we take a peek on the first mentioned ID, we can notice that the presented loan actually is assigned to two different people!\n\n# Thus, the presence of such deals is potentially harmful for our model. It would be sensible to discard such ID's from the database.\ndf_clients.filter('`ID`==7742298').show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:25:22.615440Z","iopub.execute_input":"2023-11-20T13:25:22.615902Z","iopub.status.idle":"2023-11-20T13:25:25.316178Z","shell.execute_reply.started":"2023-11-20T13:25:22.615858Z","shell.execute_reply":"2023-11-20T13:25:25.314805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping out the problematic loans.\ndf_clients = data_duplicate_id.join(df_clients, how='right', on='ID').where('`count` IS NULL').drop('count')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:25:25.317681Z","iopub.execute_input":"2023-11-20T13:25:25.318213Z","iopub.status.idle":"2023-11-20T13:25:25.468552Z","shell.execute_reply.started":"2023-11-20T13:25:25.318166Z","shell.execute_reply":"2023-11-20T13:25:25.467141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Listing the `df_clients` features with the exception of ID.\nfeatures_clients = df_clients.columns\nfeatures_clients.remove('ID')\nfeatures_clients","metadata":{"papermill":{"duration":0.057962,"end_time":"2023-09-08T16:01:28.503200","exception":false,"start_time":"2023-09-08T16:01:28.445238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:25.469718Z","iopub.execute_input":"2023-11-20T13:25:25.470125Z","iopub.status.idle":"2023-11-20T13:25:25.515405Z","shell.execute_reply.started":"2023-11-20T13:25:25.470035Z","shell.execute_reply":"2023-11-20T13:25:25.514147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, getting back to the Client's ID issue, I'd like to present a brief analysis on it.\n# Note that the database's actual amount of clients is lower than its number of rows. \ndata_clients = df_clients.dropDuplicates(features_clients) \nprint(f'`df_clients` length: {df_clients.count()}')\nprint(f'Number of clients: {data_clients.count()}')","metadata":{"papermill":{"duration":5.927381,"end_time":"2023-09-08T16:01:34.450288","exception":false,"start_time":"2023-09-08T16:01:28.522907","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:25.517398Z","iopub.execute_input":"2023-11-20T13:25:25.518176Z","iopub.status.idle":"2023-11-20T13:25:35.492976Z","shell.execute_reply.started":"2023-11-20T13:25:25.518127Z","shell.execute_reply":"2023-11-20T13:25:35.491191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll assign an ID for every client mentioned in `df_clients`. \nfrom pyspark.sql.functions import cast, row_number\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.window import Window\n\nwindow = Window.orderBy(features_clients)\nrow_window = row_number().over(window)\n\n# A DataFrame with the clients' data and actual ID.\ndf_id_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')","metadata":{"papermill":{"duration":0.265699,"end_time":"2023-09-08T16:01:34.740304","exception":false,"start_time":"2023-09-08T16:01:34.474605","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:35.495771Z","iopub.execute_input":"2023-11-20T13:25:35.496393Z","iopub.status.idle":"2023-11-20T13:25:35.726922Z","shell.execute_reply.started":"2023-11-20T13:25:35.496343Z","shell.execute_reply":"2023-11-20T13:25:35.725478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll need to perform Null Safe JOIN's, since columns such as 'OCCUPATION_TYPE' contain null values.\nfrom functools import reduce\n\n# Creating the multiple null safe JOIN's condition.\ncondition_id_client = reduce(lambda x,y: x&y, [df_clients[col].eqNullSafe(df_id_clients[col]) for col in features_clients])\ncolumns_join_id = ['df_clients.*', 'df_id_clients.ID_CLIENT'] # Listing the JOIN columns.\n\n# Consolidating the final clients database.\ndf_clients = (df_clients.alias('df_clients') # Resorting to aliases for both DataFrames present columns with same names.\n                 .join(df_id_clients.alias('df_id_clients'), condition_id_client)\n                 .select(columns_join_id))\ndf_clients.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:25:35.733008Z","iopub.execute_input":"2023-11-20T13:25:35.733849Z","iopub.status.idle":"2023-11-20T13:25:50.689239Z","shell.execute_reply.started":"2023-11-20T13:25:35.733802Z","shell.execute_reply":"2023-11-20T13:25:50.687630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This table contains the payments records for every loan since its contraction. \n        </li>\n        <li> \n            But in order to the dataset be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.030141,"end_time":"2023-09-08T16:01:44.397684","exception":false,"start_time":"2023-09-08T16:01:44.367543","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Bringing the dataset into our notebook.\npath_loans = '/kaggle/input/credit-card-approval-prediction/credit_record.csv'\nschema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\ndf_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\ndf_loans.show(5) ","metadata":{"papermill":{"duration":0.238789,"end_time":"2023-09-08T16:01:44.667766","exception":false,"start_time":"2023-09-08T16:01:44.428977","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:50.691595Z","iopub.execute_input":"2023-11-20T13:25:50.692163Z","iopub.status.idle":"2023-11-20T13:25:50.931686Z","shell.execute_reply.started":"2023-11-20T13:25:50.692114Z","shell.execute_reply":"2023-11-20T13:25:50.930268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, providing the loans' client ID.\ndf_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\ndf_loans.show(5)","metadata":{"papermill":{"duration":10.005159,"end_time":"2023-09-08T16:01:54.704279","exception":false,"start_time":"2023-09-08T16:01:44.699120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:25:50.933158Z","iopub.execute_input":"2023-11-20T13:25:50.933669Z","iopub.status.idle":"2023-11-20T13:26:07.134517Z","shell.execute_reply.started":"2023-11-20T13:25:50.933622Z","shell.execute_reply":"2023-11-20T13:26:07.133164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n            <table style='font-size:15px;margin-top:20px'> \n                <tr>\n                    <th> Code</th>\n                    <th> Definition</th>\n                </tr>\n                <tr> \n                    <td> C</td>\n                    <td> Paid off that month</td>\n                </tr>\n                <tr> \n                    <td> 0</td>\n                    <td> 1-29 days past due</td>\n                </tr>\n                <tr> \n                    <td> 1</td>\n                    <td> 30-59 days past due </td>\n                </tr>\n                <tr> \n                    <td> 2</td>\n                    <td> 60-89 days past due </td>\n                </tr>\n                <tr> \n                    <td> 3</td>\n                    <td> 90-119 days past due </td>\n                </tr>\n                <tr> \n                    <td> 4</td>\n                    <td> 120-149 days past due </td>\n                </tr>\n                <tr> \n                    <td> 5</td>\n                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n                </tr>\n                <tr> \n                    <td> X</td>\n                    <td> No loan for the month</td>\n                </tr>\n            </table>\n        </li>\n        <li style='margin-top:20px'> \n            Observe that in our case only the 3, 4 and 5 codes are of our interest. Thus it would be convenient to create a binary flag that denounces whether \n            the individual has ever caused an NPL.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.031172,"end_time":"2023-09-08T16:01:54.767879","exception":false,"start_time":"2023-09-08T16:01:54.736707","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The dependent variable's conception needs a custom GroupBy that PySpark is unable to perform by itself. Hence, we \n# are going to resort to pandas in this section.\nimport pandas as pd\n\n# Defining the GroupBy's schema.\nschema_flag_npl = '`ID_CLIENT` STRING, `NPL` STRING'\n\n# This lambda expression signs whether a client has ever produced an NPL in the past.\nlambda_npl = lambda x: '1' if x.STATUS.isin(['3', '4', '5']).any() else '0'\n\ndef has_npl(df:pd.DataFrame)->pd.DataFrame:\n    '''\n        Verifies if a client's  records contain any sort of Non-Performing Loan.\n        \n        Parameter\n        ---------\n        `df`: pd.DataFrame \n            The loan records of a certain client.\n        \n        Returns\n        -------\n        A `pd.DataFrame` with the client's ID and a flag indicating NPL existence in their loan history. \n    '''\n    output = df.groupby(['ID_CLIENT']).apply(lambda_npl) # `lambda_npl` takes care of the flags creation.\n    output.name = 'NPL' # Setting the flag column's name.\n    return output.reset_index()\n\n# Finally, generating our target-variable.\ntarget = df_loans.groupBy('ID_CLIENT').applyInPandas(has_npl, schema_flag_npl)","metadata":{"papermill":{"duration":0.079718,"end_time":"2023-09-08T16:01:54.877696","exception":false,"start_time":"2023-09-08T16:01:54.797978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-20T13:26:07.136083Z","iopub.execute_input":"2023-11-20T13:26:07.136578Z","iopub.status.idle":"2023-11-20T13:26:08.054198Z","shell.execute_reply.started":"2023-11-20T13:26:07.136534Z","shell.execute_reply":"2023-11-20T13:26:08.052528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Consolidating the Data</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            With both datasets properly treated, we are able to JOIN them in a single table.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Finally, enriching the clients information with the NPL flag.\ndf = df_id_clients.join(target, how='inner', on='ID_CLIENT')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:26:08.057424Z","iopub.execute_input":"2023-11-20T13:26:08.058160Z","iopub.status.idle":"2023-11-20T13:26:08.132997Z","shell.execute_reply.started":"2023-11-20T13:26:08.058112Z","shell.execute_reply":"2023-11-20T13:26:08.130999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.write.parquet('/kaggle/input/df-parquet', mode='overwrite')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:26:08.135180Z","iopub.execute_input":"2023-11-20T13:26:08.135698Z","iopub.status.idle":"2023-11-20T13:26:08.141491Z","shell.execute_reply.started":"2023-11-20T13:26:08.135656Z","shell.execute_reply":"2023-11-20T13:26:08.140294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Dataset Splitting</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            With the dataset properly treated, we are able to begin our EDA and model creation. But firstly we have to separate the data in \n            the training and test tables.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# As we can see, we are dealing with an unbalanced dataset case. Thus it is interesting to maintain the target value proportions\n# in both training and test sets.\nfrom pyspark.sql.functions import col\n\ndf.groupBy('NPL').count().withColumn('proportion', col('count')/df.count()).show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:26:08.143124Z","iopub.execute_input":"2023-11-20T13:26:08.145550Z","iopub.status.idle":"2023-11-20T13:27:20.872288Z","shell.execute_reply.started":"2023-11-20T13:26:08.145485Z","shell.execute_reply":"2023-11-20T13:27:20.871090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the training and test sets.\ntrain = df.sampleBy('NPL', fractions={'0':.75, '1':.75}, seed=42)\ntest = df.subtract(train)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:27:20.873709Z","iopub.execute_input":"2023-11-20T13:27:20.874217Z","iopub.status.idle":"2023-11-20T13:27:21.456416Z","shell.execute_reply.started":"2023-11-20T13:27:20.874141Z","shell.execute_reply":"2023-11-20T13:27:21.454943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since the class proportions are relatively proximate, we are able to use them in our project.\nprint('*** TRAIN ***')\ntrain.groupBy('NPL').count().withColumn('proportion', col('count')/train.count()).show()\nprint('*** TEST ***')\ntest.groupBy('NPL').count().withColumn('proportion', col('count')/test.count()).show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:27:21.457910Z","iopub.execute_input":"2023-11-20T13:27:21.462083Z","iopub.status.idle":"2023-11-20T13:29:26.209289Z","shell.execute_reply.started":"2023-11-20T13:27:21.461992Z","shell.execute_reply":"2023-11-20T13:29:26.207883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the datasets in distinct .parquet files.\ntrain.write.parquet('/kaggle/working/train.parquet', mode='overwrite')\ntest.write.parquet('/kaggle/working/test.parquet', mode='overwrite')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:29:26.210783Z","iopub.execute_input":"2023-11-20T13:29:26.212361Z","iopub.status.idle":"2023-11-20T13:30:34.626587Z","shell.execute_reply.started":"2023-11-20T13:29:26.212310Z","shell.execute_reply":"2023-11-20T13:30:34.625346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Defining the Models' Metric</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            Providing False Negatives would clearly be more harmful for the bank's equity than False Positives.         \n        </li>\n        <li> \n            Following conversations with the credit analysts, we've ended up defining the case's official metric\n            as an f-score with $\\beta=4$. So, we are giving to Recall an importance 4x higher than the Precision's.\n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.030141,"end_time":"2023-09-08T16:01:44.397684","exception":false,"start_time":"2023-09-08T16:01:44.367543","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Exploratory Data Analysis</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            With the data properly segregated, let's briefly analyze its content and see whether we can spot differences \n            among the classes. \n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:44:32.890387Z","iopub.execute_input":"2024-01-10T20:44:32.891292Z","iopub.status.idle":"2024-01-10T20:45:32.430253Z","shell.execute_reply.started":"2024-01-10T20:44:32.891246Z","shell.execute_reply":"2024-01-10T20:45:32.428664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:32.432843Z","iopub.execute_input":"2024-01-10T20:45:32.433277Z","iopub.status.idle":"2024-01-10T20:45:38.965810Z","shell.execute_reply.started":"2024-01-10T20:45:32.433234Z","shell.execute_reply":"2024-01-10T20:45:38.964723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = spark.read.parquet('/kaggle/input/npl-train/train.parquet/')\ntrain.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:38.967673Z","iopub.execute_input":"2024-01-10T20:45:38.968643Z","iopub.status.idle":"2024-01-10T20:45:47.952193Z","shell.execute_reply.started":"2024-01-10T20:45:38.968603Z","shell.execute_reply":"2024-01-10T20:45:47.950885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Assessing the Classes' Incomes</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            Let's analyze whether there is any revenue difference between individuals who produced an NPL and those who didn't. \n        </li>\n        <li> \n            Since we are dealing with an continuous outcome from two independent samples, I'll use the z-score formula below to examine the \n            income differences:\n            <center style='margin-top:20px'> \n                    $z=\\frac{\\overline{X}_{1}-\\overline{X}_{2}}{S_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$ | $S_{p}=\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$\n           </center>\n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.030141,"end_time":"2023-09-08T16:01:44.397684","exception":false,"start_time":"2023-09-08T16:01:44.367543","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The formula above is only valid for samples with similar std's (0.5<s1/s2<2). So we ought to firstly guarantee that the incomes' std's\n# are in accordance to that rule.\ndf_income_std = train.groupBy('NPL').agg({'AMT_INCOME_TOTAL':'std'})\ndf_income_std.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:47.955057Z","iopub.execute_input":"2024-01-10T20:45:47.955512Z","iopub.status.idle":"2024-01-10T20:45:49.729908Z","shell.execute_reply.started":"2024-01-10T20:45:47.955470Z","shell.execute_reply":"2024-01-10T20:45:49.728810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Because the std ratio is in the desired interval, we can proceed in using the formula.\nlist_std = [row['stddev(AMT_INCOME_TOTAL)'] for row in df_income_std.collect()]\nlist_std[0] / list_std[1]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:49.731233Z","iopub.execute_input":"2024-01-10T20:45:49.731666Z","iopub.status.idle":"2024-01-10T20:45:50.326385Z","shell.execute_reply.started":"2024-01-10T20:45:49.731625Z","shell.execute_reply":"2024-01-10T20:45:50.325221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# By setting our alpha=.05, we can see that there is no statistical evidence that people with no NPL receive higher incomes\n# than the other group.\nfrom statsmodels.stats.weightstats import ztest\n\nzeros = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==0').collect()]\nones = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==1').collect()]\n\nztest(zeros, ones, alternative='larger')[1] # Computing our p-value.","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:50.331895Z","iopub.execute_input":"2024-01-10T20:45:50.332379Z","iopub.status.idle":"2024-01-10T20:45:52.267164Z","shell.execute_reply.started":"2024-01-10T20:45:50.332337Z","shell.execute_reply":"2024-01-10T20:45:52.265830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Valuable Assets Analysis</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The dataset contains columns representing whether the client possesses real state or a vehicle.\n        </li>\n        <li> \n            We can verify if people that own such properties have lower chance of producing an NPL, because they could sell them if they don't have enough \n            cash to pay the loans. \n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef own_valuable(col_car:str, col_realty:str)->str:\n    '''\n        Signs whether a client possesses a valuable asset that can be sold for paying their loans.\n        \n        Parameters\n        ----------\n        `col_car` str\n            The value of the flag that indicates the ownership of a car.\n        `col_realty`: str\n            The value of the flag that indicates the ownership of a real state.\n            \n        Returns\n        -------\n        A flag indicating the possession of a car or real state (valuable assets). \n    '''\n    if (col_car=='Y') or (col_realty=='Y'):\n        return '1'\n    return '0'\n\n# Creating the DataFrame that will be used for conducting the Hypothesis Test.\ndf_valuable = train.select(own_valuable('FLAG_OWN_CAR', 'FLAG_OWN_REALTY').alias('OWN_VALUABLE'), 'NPL')\ndf_valuable.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:52.269615Z","iopub.execute_input":"2024-01-10T20:45:52.271257Z","iopub.status.idle":"2024-01-10T20:45:54.279943Z","shell.execute_reply.started":"2024-01-10T20:45:52.271204Z","shell.execute_reply":"2024-01-10T20:45:54.278756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting the amount of who own or not a valuable asset per target.\ngb_valuable = (df_valuable\n     .groupBy(['NPL', 'OWN_VALUABLE'])\n     .count())\n\ngb_valuable.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:54.281257Z","iopub.execute_input":"2024-01-10T20:45:54.281680Z","iopub.status.idle":"2024-01-10T20:45:55.806760Z","shell.execute_reply.started":"2024-01-10T20:45:54.281638Z","shell.execute_reply":"2024-01-10T20:45:55.805454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, using `gb_valuable` in order to estimate the proportion of clients with a valuable asset for each target.\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import sum as ps_sum\nfrom pyspark.sql.window import Window\n\nwindow = Window.partitionBy('NPL')\n\n# It's noticeable that there is a rough 7% difference between the proportions. But can we regard that as statistically significant? \n(gb_valuable\n     .withColumn('TOTAL_INSTANCES', ps_sum(col('count')).over(window)) # Total quantity of instances for each target.\n     .withColumn('PROP_OWN_VALUABLE', col('count')/ps_sum(col('count')).over(window)) # % of clients who own valuable assets per target.\n     .withColumnRenamed('count', 'INSTANCES_WITH_VALUABLE') \n     .where('`OWN_VALUABLE`==1')\n     .select(['NPL', 'INSTANCES_WITH_VALUABLE', 'TOTAL_INSTANCES', 'PROP_OWN_VALUABLE'])).show()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:55.808119Z","iopub.execute_input":"2024-01-10T20:45:55.808548Z","iopub.status.idle":"2024-01-10T20:45:57.768633Z","shell.execute_reply.started":"2024-01-10T20:45:55.808507Z","shell.execute_reply":"2024-01-10T20:45:57.767363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# H0: People who honor their debts have the same probability of possessing a valuable asset as the clients in default.\n# H1: People who honor their debts have higher probability of possessing a valuable asset compared to clients in default.\n\n# By using the table's data in a Hypothesis Test for proportions, we get a p-value below our significance degree (alpha=.05).\n\n# Therefore, we are able to reject H0 and state we possess sufficient information to say that people who have never produced an NPL have a higher\n# tendency of possessing a valuable asset.\n\nfrom statsmodels.stats.proportion import test_proportions_2indep\ntest_proportions_2indep(5747, 7201, 126, 172, compare='ratio', alternative='larger').pvalue","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:57.774423Z","iopub.execute_input":"2024-01-10T20:45:57.774919Z","iopub.status.idle":"2024-01-10T20:45:57.819738Z","shell.execute_reply.started":"2024-01-10T20:45:57.774873Z","shell.execute_reply":"2024-01-10T20:45:57.818130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With the difference proved real, let's analyze it with the use of a Cohen's h.\nfrom numpy import arcsin, sqrt\ndef cohen_h(p1:float, p2:float)->float:\n    '''\n        Computes the Cohen's h coefficient for the difference between two proportions.\n        \n        Parameters\n        ----------\n        `p1`: float\n            The first probability.\n        `p2`: float\n            The second probability.\n            \n        Returns\n        -------\n        The Cohen's h coefficient for the given proportions.\n    '''\n    phi1 = 2*arcsin(sqrt(p1))\n    phi2 = 2*arcsin(sqrt(p2))\n    return phi1-phi2\n\n# By observing the returned h, we can conclude that the difference is of a small magnitude, since it is lower than 0.2.\ncohen_h(0.7980835995000695, 0.7325581395348837)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:57.821624Z","iopub.execute_input":"2024-01-10T20:45:57.822355Z","iopub.status.idle":"2024-01-10T20:45:57.837570Z","shell.execute_reply.started":"2024-01-10T20:45:57.822307Z","shell.execute_reply":"2024-01-10T20:45:57.836245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Employment Impact Analysis</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            Finally, it would be interesting if we could verify whether unemployment can impair loan payments.  \n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Cleaning up Inconsistent Instances</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The dataset contains individuals who present their OCCUPATION_TYPE value as NULL, although they show up as employed (DAYS_EMPLOYED$\\leq{0}$)\n        </li>\n        <li>\n            In order to they not be regarded as unemployed clients (who also present OCCUPATION_TYPE as NULL), I thought it would be convenient to assign \n            another category to them.\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"@udf(returnType=StringType())\ndef impute_occupation(col_occupation:str, col_days_employed:int)->str:\n    '''\n        Assigns new categories to the OCCUPATION_TYPE column if it is null for any given row. \n        \n        In case the DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n        \n        Parameters\n        ----------\n        `col_occupation`: str\n            The row's OCCUPATION_TYPE value.\n        `col_days_employed`: int\n            The row's DAYS_EMPLOYED value.\n        \n        Returns\n        -------\n        The row's OCCUPATION_TYPE treated value.\n    '''\n    # 'Undefined' profession logic.\n    if (col_occupation is None) and (col_days_employed<=0):\n        return 'Undefined'\n    \n    # 'Unemployed' logic.\n    elif (col_occupation is None) and (col_days_employed>0):\n        return 'Unemployed'\n    \n    else:\n        pass\n    \n    return col_occupation","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:57.839945Z","iopub.execute_input":"2024-01-10T20:45:57.840881Z","iopub.status.idle":"2024-01-10T20:45:57.850637Z","shell.execute_reply.started":"2024-01-10T20:45:57.840832Z","shell.execute_reply":"2024-01-10T20:45:57.849694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# `list_select_occupation` will be used so that we can redefine the `train` DataFrame with the transformed OCCUPATION_TYPE feature.\nlist_select_occupation = train.columns\nlist_select_occupation.remove('OCCUPATION_TYPE')\nlist_select_occupation.insert(-1, impute_occupation('OCCUPATION_TYPE', 'DAYS_EMPLOYED').alias('OCCUPATION_TYPE'))\nlist_select_occupation","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:57.852793Z","iopub.execute_input":"2024-01-10T20:45:57.853676Z","iopub.status.idle":"2024-01-10T20:45:57.902823Z","shell.execute_reply.started":"2024-01-10T20:45:57.853626Z","shell.execute_reply":"2024-01-10T20:45:57.901651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the modifications to `train`.\ntrain = train.select(list_select_occupation)\ntrain.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:57.904384Z","iopub.execute_input":"2024-01-10T20:45:57.905323Z","iopub.status.idle":"2024-01-10T20:45:58.593728Z","shell.execute_reply.started":"2024-01-10T20:45:57.905287Z","shell.execute_reply":"2024-01-10T20:45:58.592424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Back to the EDA...</h4>","metadata":{}},{"cell_type":"code","source":"# Counting the amount of employed and unemployed individuals per target group.\nfrom pyspark.sql.functions import when\n(train\n     .withColumn('EMPLOYED', when(col('OCCUPATION_TYPE') == 'Unemployed', 0).otherwise(1))\n     .groupBy('NPL', 'EMPLOYED')\n     .count()\n     .orderBy('NPL', 'EMPLOYED')).show(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:58.595018Z","iopub.execute_input":"2024-01-10T20:45:58.595471Z","iopub.status.idle":"2024-01-10T20:45:59.933973Z","shell.execute_reply.started":"2024-01-10T20:45:58.595431Z","shell.execute_reply":"2024-01-10T20:45:59.932664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# H0: The proportion of unemployed among people with honored debts is the same as with those who didn't.\n# H1: The proportion of unemployed among people with honored debts is lower than that of those who didn't.\n\n# By looking at the p-value, we don't have sufficient statistical evidence to reject H0.\ntest_proportions_2indep(1269, 7201, 31, 172, compare='ratio', alternative='smaller', ).pvalue","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:59.935218Z","iopub.execute_input":"2024-01-10T20:45:59.935656Z","iopub.status.idle":"2024-01-10T20:45:59.946971Z","shell.execute_reply.started":"2024-01-10T20:45:59.935616Z","shell.execute_reply":"2024-01-10T20:45:59.945552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the newly transformed `train` dataset.\ntrain.write.parquet('/kaggle/working/train.parquet', mode='overwrite')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T20:45:59.949425Z","iopub.execute_input":"2024-01-10T20:45:59.950466Z","iopub.status.idle":"2024-01-10T20:46:01.889965Z","shell.execute_reply.started":"2024-01-10T20:45:59.950420Z","shell.execute_reply":"2024-01-10T20:46:01.888605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Data Transforming </h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            We are very close to begin the creation of our models. But we must remind that the data needs to go under a numeralization procedure before.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:54:40.433196Z","iopub.execute_input":"2024-03-30T21:54:40.433699Z","iopub.status.idle":"2024-03-30T21:55:37.169549Z","shell.execute_reply.started":"2024-03-30T21:54:40.433659Z","shell.execute_reply":"2024-03-30T21:55:37.167875Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488497 sha256=99aade1043db209219b61be4761760ee8b52c4d54ebbfba82f2489c19e5b1ac5\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:37.172041Z","iopub.execute_input":"2024-03-30T21:55:37.172453Z","iopub.status.idle":"2024-03-30T21:55:43.836732Z","shell.execute_reply.started":"2024-03-30T21:55:37.172410Z","shell.execute_reply":"2024-03-30T21:55:43.835661Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/03/30 21:55:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>pre { white-space: pre !important; }</style>"},"metadata":{}}]},{"cell_type":"code","source":"train = spark.read.parquet('/kaggle/input/npl-train/train-eda.parquet/')\ntrain.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:43.838697Z","iopub.execute_input":"2024-03-30T21:55:43.839677Z","iopub.status.idle":"2024-03-30T21:55:53.937777Z","shell.execute_reply.started":"2024-03-30T21:55:43.839638Z","shell.execute_reply":"2024-03-30T21:55:53.936265Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n|ID_CLIENT|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|CNT_FAM_MEMBERS|OCCUPATION_TYPE|NPL|\n+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n|      691|          F|           N|              N|           0|         67500.0|         Working|Secondary / secon...|           Married|House / apartment|    -20075|        -7013|         1|              1|         1|         0|            2.0|    Sales staff|  0|\n|     3606|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|     -9865|         -196|         1|              1|         0|         0|            2.0|       Laborers|  0|\n|     4821|          F|           N|              N|           0|        135000.0|   State servant|    Higher education|           Married|House / apartment|    -12490|        -1191|         1|              1|         1|         0|            2.0|     Core staff|  0|\n|     5925|          F|           N|              N|           0|        157500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22828|       365243|         1|              0|         0|         0|            2.0|     Unemployed|  0|\n|     6194|          F|           N|              N|           0|        157500.0|         Working|   Incomplete higher|           Married|House / apartment|    -16888|        -2687|         1|              0|         0|         0|            2.0|     Core staff|  0|\n+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Job Labels' Transformer</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li>   \n            Before making our numeralization process, we must re-implement the 'OCCUPATION_TYPE' column logic that we've written in a formal \n            PySpark Transformer class. In that way, we'll be able to apply the transformations in the test set as well.\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param, Params, TypeConverters\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom pyspark import keyword_only\nfrom pyspark.ml import Transformer\nfrom typing import List\n\nclass _BaseTransformer(Transformer, HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n    '''\n        A class that provide the basic functionalities for the project's custom transformers.\n        \n        If you consider that your object will require even more customization, you can just overwrite any of the default methods when creating it. Also,\n        remember that any extra parameter must be set with a `self._setDefault` method right in the `__init__` function, mentioned in the `self.setParams`\n        method and own a getter function.\n        \n        Lastly, don't forget to define the `_transform` function!\n        \n        Parameters\n        ----------\n        `inputCol`: str\n            The name of the input column.\n        `inputCols`: List[str]\n            List containing the name of input columns.\n        `outputCol`: str\n            The name of the output column.\n        `outputCols`: List[str]\n            List containing the name of output columns.\n        \n        References\n        ----------\n        https://medium.com/@zeid.zandi/utilizing-the-power-of-pyspark-pipelines-in-data-science-projects-benefits-and-limitations-2-2-9063e4bebd05\n        https://www.crowdstrike.com/blog/deep-dive-into-custom-spark-transformers-for-machine-learning-pipelines/\n    '''\n    @keyword_only\n    def __init__(self, inputCol:str=None, inputCols:List[str]=None, outputCol:str=None, outputCols:List[str]=None, *args)->None:\n        super().__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n        \n    @keyword_only\n    def setParams(self, inputCol=None, inputCols=None, outputCol=None, outputCols=None): \n        kwargs = self._input_kwargs                                                          \n        return self._set(**kwargs)\n    \n    def setInputCol(self, value):\n        return self.setParams(inputCol=value)\n    \n    def setInputCols(self, value):\n        self.setParams(inputCols=value)\n    \n    def setOutputCol(self, value):\n        return self.setParams(outputCol=value)\n    \n    def setOutputCols(self, value):\n        return self.setParams(outputCols=value)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:53.943852Z","iopub.execute_input":"2024-03-30T21:55:53.944437Z","iopub.status.idle":"2024-03-30T21:55:54.396935Z","shell.execute_reply.started":"2024-03-30T21:55:53.944389Z","shell.execute_reply":"2024-03-30T21:55:54.395657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nclass ImputeOccupation(_BaseTransformer):\n    '''\n        A Transformer encharged of imputing proper OCCUPATION_TYPE values for rows that are NULL in such column.  \n        \n        Parameters\n        ----------\n        `inputCol`: str\n            The name of the input column.\n            \n        Method\n        ------\n        `_transform`: Applies the mentioned imputation process,\n    '''\n    @staticmethod\n    @udf(returnType=StringType())\n    def __impute_occupation(col_occupation:str, col_days_employed:int)->str:\n        '''\n        Assigns new categories to the OCCUPATION_TYPE column, if it is null. \n        \n        In case the row's DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n        \n        Parameters\n        ----------\n        `col_occupation`: str\n            The row's OCCUPATION_TYPE value.\n        `col_days_employed`: int\n            The row's DAYS_EMPLOYED value.\n        \n        Returns\n        -------\n        The row's OCCUPATION_TYPE treated value.\n        '''\n        # 'Undefined' profession logic.\n        if (col_occupation is None) and (col_days_employed<=0):\n            return 'Undefined'\n\n        # 'Unemployed' logic.\n        elif (col_occupation is None) and (col_days_employed>0):\n            return 'Unemployed'\n        \n        # Otherwise, it outputs the current value.\n        else:\n            return col_occupation\n    \n    def _transform(self, dataset:DataFrame)->DataFrame:\n        '''\n            Performs the new occupation categories imputation.\n            \n            Parameter\n            ---------\n            `dataset`: `pyspark.sql.DataFrame`\n                The project's independent variables.\n                \n            Returns\n            -------\n            The DataFrame with the treated 'OCCUPATION_TYPE' column.\n        '''\n        x = self.getInputCol()\n        dataset = dataset.withColumn(x, self.__impute_occupation(x, 'DAYS_EMPLOYED'))\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:54.398920Z","iopub.execute_input":"2024-03-30T21:55:54.399390Z","iopub.status.idle":"2024-03-30T21:55:54.415064Z","shell.execute_reply.started":"2024-03-30T21:55:54.399345Z","shell.execute_reply":"2024-03-30T21:55:54.413708Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> String Columns Numeralization</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li>   \n            The first transformation we must conduct is turning string columns into float format.\n        </li>\n        <li> \n            As a solution, I'ĺl replace the given category for the its proportion of instances that are defaulted. I think that will be a worthier way \n            of generating numerical information than just providing a One-Hot Encoding.\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.pipeline import Estimator, Model, Pipeline, PipelineModel\nfrom pyspark.ml.param.shared import HasPredictionCol, Param, Params, TypeConverters\nfrom typing import Dict, List, Union\n\ndoc = '''\n    Parameter that stores the proportions of defaulted individuals for each X features' categories.\n'''\nclass HasDefaultProportions(Params):\n    '''\n        A class dedicated to store the categories' proportion of indebted individuals. \n    '''\n    defaultProportions:Dict[str, Dict[str, float]] = Param(Params._dummy(), 'defaultProportions', doc)\n    \n    def __init__(self):\n        super(HasDefaultProportions, self).__init__()\n    \n    def setDefaultProportions(self, value):\n        return self._set(defaultProportions=value)\n    \n    def getDefaultProportions(self):\n        return self.getOrDefault(self.defaultProportions)\n    \nlist_col_strings = [col for col, dtype in train.dtypes if dtype=='string' and col !='NPL']","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:54.416941Z","iopub.execute_input":"2024-03-30T21:55:54.417343Z","iopub.status.idle":"2024-03-30T21:55:54.435338Z","shell.execute_reply.started":"2024-03-30T21:55:54.417311Z","shell.execute_reply":"2024-03-30T21:55:54.434149Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.dataframe import DataFrame\n\nclass DefaultProportionsModel(Model, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n    '''\n        The `pyspark.ml.pipeline.Model` that replaces categorical values for their respective proportion of defaulted individuals.\n        \n        Parameters\n        ---------\n        `inputCols: List[str]\n            A list of the categorical features names.\n        `defaultProportions`: Dict[str, DataFrame]\n            A dictionary mapping the name of the categorical column to a DataFrame storing the proportions.\n            \n        Method\n        -------\n        `_transform`: Performs the replacements.\n    '''\n    @keyword_only\n    def __init__(self, inputCols:List[str]=None, defaultProportions:Dict[str,List[Dict[str, Union[str, float]]]]=None):\n        super(DefaultProportionsModel, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n        \n    @keyword_only\n    def setParams(self, inputCols:List[str]=None, defaultProportions:Dict[str,List[Dict[str, Union[str, float]]]]=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n    \n    def _transform(self, dataset:DataFrame)->DataFrame:\n        '''\n            Method that exchanges the categorical values for their proportion of defaulted individuals of the training set.\n            \n            Parameter\n            ---------\n            `dataset`: DataFrame\n                A `pyspark.sql.dataframe.DataFrame` with the project's data.\n                \n            Returns\n            -------\n            The DataFrame with the mentioned transformation.\n        '''\n        x = self.getInputCols()\n        defaultProportions = self.getDefaultProportions()\n        for c in x:\n            df = spark.createDataFrame(defaultProportions[c]) # Converting our list of dicts into a DataFrame.\n            dataset = dataset.join(df, on=c, how='left').drop(c)\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:54.437105Z","iopub.execute_input":"2024-03-30T21:55:54.437926Z","iopub.status.idle":"2024-03-30T21:55:54.452018Z","shell.execute_reply.started":"2024-03-30T21:55:54.437849Z","shell.execute_reply":"2024-03-30T21:55:54.450616Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col\nfrom pyspark.sql.types import DecimalType, FloatType\n\nclass DefaultProportions(Estimator, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n    '''\n        An Estimator encharged for measuring the categorical values proportions of defaulted individuals.\n    \n        Parameter\n        ---------\n        `inputCols`: List[str]\n            A list with the categorical columns names.\n            \n        Method\n        ------\n        `_fit`: Measures the proportions and stores them in a DataFrame.\n        \n        References\n        ----------\n        https://stackoverflow.com/questions/37270446/how-to-create-a-custom-estimator-in-pyspark\n    '''\n    @keyword_only\n    def __init__(self, inputCols:List[str]=None):\n        super(DefaultProportions, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n    \n    @keyword_only\n    def setParams(self, inputCols:List[str]=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n        \n    def setInputCols(self, value:List[str]):\n        return self.setParams(inputCols=value)\n    \n    def setPredictionCol(self, value:str):\n        return self.setParams(predictionCol=value)\n    \n    @staticmethod\n    def __measure_proportions(dataset:DataFrame, input_cols:List[str], target:str)->Dict[str, DataFrame]:\n        '''\n            Responsible for quantifying the categories' proportions of indebted individuals. They will be stored in a `pyspark.sql.dataframe.DataFrame`\n            object. Each dataframe is kept in the `defaultedProportions` dictionary, which maps them to the categorical column's name.\n            \n            Paramters\n            ---------\n            `dataset`: DataFrame\n                A `pyspark.sql.dataframe.DataFrame` with the project's data.\n            `inputCols`: List[str]\n                A list with the categorical columns names.    \n            `target`: str\n                The `dataset`'s target name.\n            \n            Returns\n            -------\n            A dictionary mapping the categorical columns' names to the DataFrame that stores the classes proportions.\n        '''\n        defaultProportions = {}\n        len_df = dataset.count()\n        for c in input_cols:\n            col_prop = c+'_NPL_PROP'\n            df_gb = dataset.groupBy([c, target]).count()\n            df_props = (df_gb\n                        .where(f'{target}==1')\n                        .withColumn(col_prop, (col('count')/len_df)\n                        .cast(FloatType()))#DecimalType(8,7)))\n                        .select([c, col_prop]))\n            defaultProportions[c] = list(map(lambda row: row.asDict(), df_props.collect())) # DF to list to able saving the class.\n        return defaultProportions\n            \n    def _fit(self, dataset:DataFrame, target:str='NPL')->DefaultProportionsModel:\n        '''\n            Measures the proportions of indebted individuals from each dataset's categorical columns.\n            \n            Parameters\n            ---------\n            `dataset`: DataFrame\n                A `pyspark.sql.dataframe.DataFrame` with the project's data.   \n            `target`: str\n                The `dataset`'s target name.\n                \n            Returns\n            -------\n            A fitted DefaultProportionsModel ready for transforming the dataset.\n        '''\n        x = self.getInputCols()\n        defaultProportions = self.__measure_proportions(dataset, x, target)\n        return DefaultProportionsModel(inputCols=x, defaultProportions=defaultProportions)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:54.454105Z","iopub.execute_input":"2024-03-30T21:55:54.455065Z","iopub.status.idle":"2024-03-30T21:55:54.474595Z","shell.execute_reply.started":"2024-03-30T21:55:54.455027Z","shell.execute_reply":"2024-03-30T21:55:54.473241Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Finally, we can insert both custom classes inside a pyspark pipeline!\nimpute_occupation = ImputeOccupation(inputCol='OCCUPATION_TYPE')\ndefault_prop = DefaultProportions(inputCols=list_col_strings[1:])\n\n# Creating the pipe.\npipe1 = Pipeline(stages=[impute_occupation, default_prop]).fit(train)\ntrain_num = pipe1.transform(train)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:55:54.476249Z","iopub.execute_input":"2024-03-30T21:55:54.476653Z","iopub.status.idle":"2024-03-30T21:56:06.749021Z","shell.execute_reply.started":"2024-03-30T21:55:54.476619Z","shell.execute_reply":"2024-03-30T21:56:06.747232Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# We can discard the ID_CLIENT column, since it has no predictive value.\ntrain_num = train_num.drop('ID_CLIENT')\ntrain_num.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:56:06.753129Z","iopub.execute_input":"2024-03-30T21:56:06.755152Z","iopub.status.idle":"2024-03-30T21:56:18.222393Z","shell.execute_reply.started":"2024-03-30T21:56:06.755099Z","shell.execute_reply":"2024-03-30T21:56:18.220769Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+\n|CNT_CHILDREN|AMT_INCOME_TOTAL|DAYS_BIRTH|DAYS_EMPLOYED|CNT_FAM_MEMBERS|NPL|CODE_GENDER_NPL_PROP|FLAG_OWN_CAR_NPL_PROP|FLAG_OWN_REALTY_NPL_PROP|NAME_INCOME_TYPE_NPL_PROP|NAME_EDUCATION_TYPE_NPL_PROP|NAME_FAMILY_STATUS_NPL_PROP|NAME_HOUSING_TYPE_NPL_PROP|FLAG_MOBIL_NPL_PROP|FLAG_WORK_PHONE_NPL_PROP| FLAG_PHONE_NPL_PROP|FLAG_EMAIL_NPL_PROP|OCCUPATION_TYPE_NPL_PROP|\n+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+\n|           0|        112500.0|     -9865|         -196|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.010443510487675667|         0.01478367019444704|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.015461820177733898|0.02142954058945179|     0.00366200995631516|\n|           0|         67500.0|    -20075|        -7013|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.010443510487675667|         0.01478367019444704|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.007866539992392063|0.02142954058945179|    0.001763190026395...|\n|           0|        364500.0|    -17204|        -1028|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.010443510487675667|         0.01478367019444704|       5.425200215540826E-4|      0.020073240622878075|0.02332836017012596|    0.017360640689730644|0.015461820177733898|0.02142954058945179|    0.001763190026395...|\n|           0|        135000.0|    -12490|        -1191|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.001898820046335...|        0.006374610122293234|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.007866539992392063|0.02142954058945179|    0.003255120012909174|\n|           0|        157500.0|    -16888|        -2687|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.010443510487675667|        0.001491929986514151|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.017360640689730644|0.015461820177733898|0.02142954058945179|    0.003255120012909174|\n+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Imputing and Standardization</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li>   \n            As a last step, we also need to guarantee that our instances will have no NULL values. I'll use standardization (...)\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# With all our features properly in numerical format, we can add a second Pipeline that handles imputing and standardization.\nfrom pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n\nlist_features = [c for c in train_num.columns if c!='NPL']\n\nimputer = Imputer(inputCols=list_features, outputCols=list_features)\n\nvector_assembler = VectorAssembler(inputCols=list_features, outputCol='features', handleInvalid='skip')\nscaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True)\npipe2 = Pipeline(stages=[imputer, vector_assembler, scaler]).fit(train_num)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:56:18.224069Z","iopub.execute_input":"2024-03-30T21:56:18.224549Z","iopub.status.idle":"2024-03-30T21:56:39.188400Z","shell.execute_reply.started":"2024-03-30T21:56:18.224506Z","shell.execute_reply":"2024-03-30T21:56:39.186922Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Finally, creating the project's official transformation Pipeline! \n# We can declare it as a PipelineModel, since its sub-pipelines have already been fitted!\npipe = PipelineModel(stages=[pipe1, pipe2])\ntrain_num = pipe.transform(train)\ntrain_num.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:56:39.194953Z","iopub.execute_input":"2024-03-30T21:56:39.196391Z","iopub.status.idle":"2024-03-30T21:56:49.577354Z","shell.execute_reply.started":"2024-03-30T21:56:39.196334Z","shell.execute_reply":"2024-03-30T21:56:49.575743Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+--------------------+--------------------+\n|ID_CLIENT|CNT_CHILDREN|AMT_INCOME_TOTAL|DAYS_BIRTH|DAYS_EMPLOYED|CNT_FAM_MEMBERS|NPL|CODE_GENDER_NPL_PROP|FLAG_OWN_CAR_NPL_PROP|FLAG_OWN_REALTY_NPL_PROP|NAME_INCOME_TYPE_NPL_PROP|NAME_EDUCATION_TYPE_NPL_PROP|NAME_FAMILY_STATUS_NPL_PROP|NAME_HOUSING_TYPE_NPL_PROP|FLAG_MOBIL_NPL_PROP|FLAG_WORK_PHONE_NPL_PROP| FLAG_PHONE_NPL_PROP|FLAG_EMAIL_NPL_PROP|OCCUPATION_TYPE_NPL_PROP|            features|     scaled_features|\n+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+--------------------+--------------------+\n|    54798|           0|        720000.0|    -20791|        -4814|            1.0|  0|0.013969889841973782| 0.008002170361578465|    0.013698630034923553|     0.006103349849581...|        0.006374610122293234|       0.001763190026395...|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.007866539992392063|0.02142954058945179|    0.001898820046335...|[0.0,720000.0,-20...|[-0.5716041269164...|\n|    27130|           0|        157500.0|    -19221|        -2926|            2.0|  0|0.013969889841973782| 0.015326189808547497|    0.013698630034923553|     0.010443510487675667|         0.01478367019444704|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.007866539992392063|0.02142954058945179|    0.001898820046335...|[0.0,157500.0,-19...|[-0.5716041269164...|\n|    54097|           0|        315000.0|    -12883|        -3627|            2.0|  0|0.013969889841973782| 0.008002170361578465|    0.013698630034923553|     0.006103349849581...|        0.006374610122293234|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.017360640689730644|0.015461820177733898|0.02142954058945179|    0.001898820046335...|[0.0,315000.0,-12...|[-0.5716041269164...|\n|    11667|           1|        202500.0|    -17971|        -3625|            3.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.006103349849581...|         0.01478367019444704|        0.01600434072315693|      0.001763190026395...|0.02332836017012596|    0.017360640689730644|0.007866539992392063|0.02142954058945179|    0.001898820046335...|[1.0,202500.0,-17...|[0.79229141604638...|\n|    13107|           2|        315000.0|    -12932|         -309|            4.0|  0|0.013969889841973782| 0.015326189808547497|    0.009629730135202408|     0.006103349849581...|        0.006374610122293234|        0.01600434072315693|      0.020073240622878075|0.02332836017012596|    0.005967719946056604|0.007866539992392063|0.02142954058945179|    0.001898820046335...|[2.0,315000.0,-12...|[2.15618695900926...|\n+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+--------------------+-------------------+------------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving the model for future use.\npipe.save('../working/pipe')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T21:56:49.578833Z","iopub.execute_input":"2024-03-30T21:56:49.579324Z","iopub.status.idle":"2024-03-30T21:56:52.741687Z","shell.execute_reply.started":"2024-03-30T21:56:49.579277Z","shell.execute_reply":"2024-03-30T21:56:52.740363Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Modeling Phase </h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            Since we already have our features in numerical format, we can begin this section by briefly analyzing the distribution of the features \n            throughout the space. \n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-04-01T21:57:01.792540Z","iopub.execute_input":"2024-04-01T21:57:01.792926Z","iopub.status.idle":"2024-04-01T21:57:58.206416Z","shell.execute_reply.started":"2024-04-01T21:57:01.792896Z","shell.execute_reply":"2024-04-01T21:57:58.203945Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488497 sha256=5b4c709e860d36d5c9a5019ac22d85ce02b98d5ad8049743e9ee295e733f2fb5\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-01T21:57:58.208651Z","iopub.execute_input":"2024-04-01T21:57:58.210356Z","iopub.status.idle":"2024-04-01T21:58:04.994639Z","shell.execute_reply.started":"2024-04-01T21:57:58.210309Z","shell.execute_reply":"2024-04-01T21:58:04.993194Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/01 21:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>pre { white-space: pre !important; }</style>"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.ml.pipeline import PipelineModel\ntrain = spark.read.parquet('/kaggle/input/npl-train/train-eda.parquet/')\n# train = PipelineModel.load('/kaggle/input/nlp-pipe/other/npl-pipe/1/pipe').transform(train)\n# train.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T21:59:41.067028Z","iopub.execute_input":"2024-04-01T21:59:41.067576Z","iopub.status.idle":"2024-04-01T21:59:42.985040Z","shell.execute_reply.started":"2024-04-01T21:59:41.067528Z","shell.execute_reply":"2024-04-01T21:59:42.983315Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelineModel\n\u001b[1;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/npl-train/train-eda.parquet/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/nlp-pipe/other/npl-pipe/1/pipe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(train)\n\u001b[1;32m      4\u001b[0m train\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:286\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     uid, stages \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineSharedReadWrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PipelineModel(stages\u001b[38;5;241m=\u001b[39mcast(List[Transformer], stages))\u001b[38;5;241m.\u001b[39m_resetUid(uid)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:439\u001b[0m, in \u001b[0;36mPipelineSharedReadWrite.load\u001b[0;34m(metadata, sc, path)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, stageUid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stageUids):\n\u001b[1;32m    436\u001b[0m     stagePath \u001b[38;5;241m=\u001b[39m PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39mgetStagePath(\n\u001b[1;32m    437\u001b[0m         stageUid, index, \u001b[38;5;28mlen\u001b[39m(stageUids), stagesDir\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 439\u001b[0m     stage: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineStage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadParamsInstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstagePath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     stages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m], stages)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/util.py:650\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadParamsInstance\u001b[0;34m(path, sc)\u001b[0m\n\u001b[1;32m    648\u001b[0m     pythonClassName \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyspark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    649\u001b[0m py_type: Type[RL] \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m__get_class(pythonClassName)\n\u001b[0;32m--> 650\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[43mpy_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:286\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     uid, stages \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineSharedReadWrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PipelineModel(stages\u001b[38;5;241m=\u001b[39mcast(List[Transformer], stages))\u001b[38;5;241m.\u001b[39m_resetUid(uid)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:439\u001b[0m, in \u001b[0;36mPipelineSharedReadWrite.load\u001b[0;34m(metadata, sc, path)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, stageUid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stageUids):\n\u001b[1;32m    436\u001b[0m     stagePath \u001b[38;5;241m=\u001b[39m PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39mgetStagePath(\n\u001b[1;32m    437\u001b[0m         stageUid, index, \u001b[38;5;28mlen\u001b[39m(stageUids), stagesDir\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 439\u001b[0m     stage: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineStage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadParamsInstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstagePath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     stages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m], stages)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/util.py:649\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadParamsInstance\u001b[0;34m(path, sc)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     pythonClassName \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyspark\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 649\u001b[0m py_type: Type[RL] \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpythonClassName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m instance \u001b[38;5;241m=\u001b[39m py_type\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/util.py:556\u001b[0m, in \u001b[0;36mDefaultParamsReader.__get_class\u001b[0;34m(clazz)\u001b[0m\n\u001b[1;32m    554\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    555\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(module, fromlist\u001b[38;5;241m=\u001b[39m[parts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'ImputeOccupation'"],"ename":"AttributeError","evalue":"module '__main__' has no attribute 'ImputeOccupation'","output_type":"error"}]},{"cell_type":"markdown","source":"<p style='color:red'> Fazer o Estimator de DefaultProportions</p>","metadata":{}}]}