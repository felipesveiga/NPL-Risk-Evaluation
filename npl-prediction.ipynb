{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n        </li>\n        <li> \n            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n            credit strategy must be adopted for new contracts.\n        </li>\n        <li> \n            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n            <ul style='list-style-type:decimal'> \n                <li> \n                    It must be focused on predicting whether a given client might produce an NPL in the future.\n                </li>\n                <li> \n                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n                    an incisive \"yes or no\" answer.\n                </li>\n            </ul>\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.006906,"end_time":"2023-09-08T16:00:22.310851","exception":false,"start_time":"2023-09-08T16:00:22.303945","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Data Importing</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n            and the second lists the loans they've contracted over some period of time.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"papermill":{"duration":52.936605,"end_time":"2023-09-08T16:01:15.267137","exception":false,"start_time":"2023-09-08T16:00:22.330532","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:00:26.364266Z","iopub.execute_input":"2023-09-18T21:00:26.364629Z","iopub.status.idle":"2023-09-18T21:01:19.196508Z","shell.execute_reply.started":"2023-09-18T21:00:26.364594Z","shell.execute_reply":"2023-09-18T21:01:19.195317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"papermill":{"duration":6.106913,"end_time":"2023-09-08T16:01:21.392365","exception":false,"start_time":"2023-09-08T16:01:15.285452","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:19.198735Z","iopub.execute_input":"2023-09-18T21:01:19.199047Z","iopub.status.idle":"2023-09-18T21:01:25.038534Z","shell.execute_reply.started":"2023-09-18T21:01:19.199019Z","shell.execute_reply":"2023-09-18T21:01:25.037313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This dataset is comprised of general personal and professional information about the loans' clients.\n        </li>    \n        <li> \n            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.018275,"end_time":"2023-09-08T16:01:21.429645","exception":false,"start_time":"2023-09-08T16:01:21.411370","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path_clients = '/kaggle/input/credit-card-approval-prediction/application_record.csv'\n\n# Defining the data types from the clients dataset.\nschema_clients = '''\n`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n'''\n\n# Reading the database with the created schema.\ndf_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\ndf_clients.show(5)","metadata":{"papermill":{"duration":6.913786,"end_time":"2023-09-08T16:01:28.361906","exception":false,"start_time":"2023-09-08T16:01:21.448120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:25.040125Z","iopub.execute_input":"2023-09-18T21:01:25.040868Z","iopub.status.idle":"2023-09-18T21:01:31.838748Z","shell.execute_reply.started":"2023-09-18T21:01:25.040828Z","shell.execute_reply":"2023-09-18T21:01:31.837651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n             Clients may not have unique rows in the dataset because the ID column identifies a loan contracted instead of a person.\n        </li>\n        <li> \n            Thus, I've found convenient for the project to create an ID column that assigns a code for the clients\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.026529,"end_time":"2023-09-08T16:01:28.418517","exception":false,"start_time":"2023-09-08T16:01:28.391988","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Listing the `df_clients` features with the exception of ID.\nfeatures_clients = df_clients.columns\nfeatures_clients.remove('ID')\nfeatures_clients","metadata":{"papermill":{"duration":0.057962,"end_time":"2023-09-08T16:01:28.503200","exception":false,"start_time":"2023-09-08T16:01:28.445238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:31.841417Z","iopub.execute_input":"2023-09-18T21:01:31.842499Z","iopub.status.idle":"2023-09-18T21:01:31.881164Z","shell.execute_reply.started":"2023-09-18T21:01:31.842470Z","shell.execute_reply":"2023-09-18T21:01:31.880034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note that the database's actual amount of clients is lower than its number of rows. \ndata_clients = df_clients.dropDuplicates(features_clients)\nprint(f'`df_clients` length: {df_clients.count()}')\nprint(f'Number of clients: {data_clients.count()}')","metadata":{"papermill":{"duration":5.927381,"end_time":"2023-09-08T16:01:34.450288","exception":false,"start_time":"2023-09-08T16:01:28.522907","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:31.882809Z","iopub.execute_input":"2023-09-18T21:01:31.884110Z","iopub.status.idle":"2023-09-18T21:01:36.915182Z","shell.execute_reply.started":"2023-09-18T21:01:31.884071Z","shell.execute_reply":"2023-09-18T21:01:36.913788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll assign a Client ID for every Loan ID present in `df_clients`. \nfrom pyspark.sql.functions import cast, row_number\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.window import Window\n\nwindow = Window.orderBy(features_clients)\nrow_window = row_number().over(window)\nid_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')","metadata":{"papermill":{"duration":0.265699,"end_time":"2023-09-08T16:01:34.740304","exception":false,"start_time":"2023-09-08T16:01:34.474605","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:36.916776Z","iopub.execute_input":"2023-09-18T21:01:36.917577Z","iopub.status.idle":"2023-09-18T21:01:37.118830Z","shell.execute_reply.started":"2023-09-18T21:01:36.917537Z","shell.execute_reply":"2023-09-18T21:01:37.117997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we only need to enrich `df_clients` with the clients' actual identification.\n\n# Performing an INNER JOIN between `df_clients` and `id_clients` using all non-ID columns as keys.\ndf_clients = df_clients.join(id_clients, on=features_clients)\ndf_clients.show(5)","metadata":{"papermill":{"duration":9.568932,"end_time":"2023-09-08T16:01:44.337438","exception":false,"start_time":"2023-09-08T16:01:34.768506","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:37.119793Z","iopub.execute_input":"2023-09-18T21:01:37.120077Z","iopub.status.idle":"2023-09-18T21:01:46.710397Z","shell.execute_reply.started":"2023-09-18T21:01:37.120053Z","shell.execute_reply":"2023-09-18T21:01:46.709276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This table contains the payments records for every loan since its contraction. \n        </li>\n        <li> \n            But in order to the dataset to be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.030141,"end_time":"2023-09-08T16:01:44.397684","exception":false,"start_time":"2023-09-08T16:01:44.367543","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Bringing the dataset into our notebook.\npath_loans = '/kaggle/input/credit-card-approval-prediction/credit_record.csv'\nschema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\ndf_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\ndf_loans.show(5)","metadata":{"papermill":{"duration":0.238789,"end_time":"2023-09-08T16:01:44.667766","exception":false,"start_time":"2023-09-08T16:01:44.428977","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:46.711764Z","iopub.execute_input":"2023-09-18T21:01:46.712913Z","iopub.status.idle":"2023-09-18T21:01:46.932556Z","shell.execute_reply.started":"2023-09-18T21:01:46.712847Z","shell.execute_reply":"2023-09-18T21:01:46.931401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, providing the loans' client ID.\ndf_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\ndf_loans.show(5)","metadata":{"papermill":{"duration":10.005159,"end_time":"2023-09-08T16:01:54.704279","exception":false,"start_time":"2023-09-08T16:01:44.699120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:01:46.933754Z","iopub.execute_input":"2023-09-18T21:01:46.934143Z","iopub.status.idle":"2023-09-18T21:01:57.722532Z","shell.execute_reply.started":"2023-09-18T21:01:46.934108Z","shell.execute_reply":"2023-09-18T21:01:57.721461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n            <table style='font-size:15px;margin-top:20px'> \n                <tr>\n                    <th> Code</th>\n                    <th> Definition</th>\n                </tr>\n                <tr> \n                    <td> C</td>\n                    <td> Paid off that month</td>\n                </tr>\n                <tr> \n                    <td> 0</td>\n                    <td> 1-29 days past due</td>\n                </tr>\n                <tr> \n                    <td> 1</td>\n                    <td> 30-59 days past due </td>\n                </tr>\n                <tr> \n                    <td> 2</td>\n                    <td> 60-89 days past due </td>\n                </tr>\n                <tr> \n                    <td> 3</td>\n                    <td> 90-119 days past due </td>\n                </tr>\n                <tr> \n                    <td> 4</td>\n                    <td> 120-149 days past due </td>\n                </tr>\n                <tr> \n                    <td> 5</td>\n                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n                </tr>\n                <tr> \n                    <td> X</td>\n                    <td> No loan for the month</td>\n                </tr>\n            </table>\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.031172,"end_time":"2023-09-08T16:01:54.767879","exception":false,"start_time":"2023-09-08T16:01:54.736707","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.dataframe import DataFrame\n\n# Defining the GroupBy's schema.\nschema_flag_npl = '`ID_CLIENT` STRING, `NPL_FLAG` STRING'\n\n@pandas_udf(schema_flag_npl, functionType=PandasUDFType.GROUPED_MAP)\ndef has_npl(df:DataFrame)->pd.DataFrame:\n    output = pd.DataFrame(df.groupby('ID_CLIENT').apply(lambda x:x))","metadata":{"papermill":{"duration":0.079718,"end_time":"2023-09-08T16:01:54.877696","exception":false,"start_time":"2023-09-08T16:01:54.797978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-18T21:33:06.655120Z","iopub.execute_input":"2023-09-18T21:33:06.655847Z","iopub.status.idle":"2023-09-18T21:33:06.671118Z","shell.execute_reply.started":"2023-09-18T21:33:06.655808Z","shell.execute_reply":"2023-09-18T21:33:06.669940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='color:red'> Fazer GroupBy conforme <a href='https://stackoverflow.com/questions/40006395/applying-udfs-on-groupeddata-in-pyspark-with-functioning-python-example'> este link</a></p>","metadata":{"papermill":{"duration":0.022705,"end_time":"2023-09-08T16:02:02.469402","exception":false,"start_time":"2023-09-08T16:02:02.446697","status":"completed"},"tags":[]}}]}