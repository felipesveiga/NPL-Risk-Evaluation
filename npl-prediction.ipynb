{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa4c9c3",
   "metadata": {
    "papermill": {
     "duration": 0.005081,
     "end_time": "2023-10-13T16:13:37.000251",
     "exception": false,
     "start_time": "2023-10-13T16:13:36.995170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n",
    "<div style='font-size:20px'> \n",
    "    <ul> \n",
    "        <li> \n",
    "            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n",
    "        </li>\n",
    "        <li> \n",
    "            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n",
    "            credit strategy must be adopted for new contracts.\n",
    "        </li>\n",
    "        <li> \n",
    "            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n",
    "            <ul style='list-style-type:decimal'> \n",
    "                <li> \n",
    "                    It must be focused on predicting whether a given client might produce an NPL in the future.\n",
    "                </li>\n",
    "                <li> \n",
    "                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n",
    "                    an incisive \"yes or no\" answer.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <p style='margin-left:30px'> <strong> Note:</strong> The bank's NPL definition is any loan which payment is at least 90 days late.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c07d6d",
   "metadata": {
    "papermill": {
     "duration": 0.004383,
     "end_time": "2023-10-13T16:13:37.009462",
     "exception": false,
     "start_time": "2023-10-13T16:13:37.005079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Importing</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n",
    "            and the second lists the loans they've contracted over some period of time.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a0f23f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:13:37.020657Z",
     "iopub.status.busy": "2023-10-13T16:13:37.020261Z",
     "iopub.status.idle": "2023-10-13T16:14:22.309268Z",
     "shell.execute_reply": "2023-10-13T16:14:22.307972Z"
    },
    "papermill": {
     "duration": 45.29756,
     "end_time": "2023-10-13T16:14:22.311711",
     "exception": false,
     "start_time": "2023-10-13T16:13:37.014151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=c96055a586d2cff3d57e6af55fd7bfdd3be985909352c7ebe51d95f63e925a32\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7c012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:22.334475Z",
     "iopub.status.busy": "2023-10-13T16:14:22.334078Z",
     "iopub.status.idle": "2023-10-13T16:14:27.853553Z",
     "shell.execute_reply": "2023-10-13T16:14:27.852753Z"
    },
    "papermill": {
     "duration": 5.533579,
     "end_time": "2023-10-13T16:14:27.855875",
     "exception": false,
     "start_time": "2023-10-13T16:14:22.322296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/13 16:14:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ebf64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.011046,
     "end_time": "2023-10-13T16:14:27.877997",
     "exception": false,
     "start_time": "2023-10-13T16:14:27.866951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This dataset is comprised of general information about the loans' clients.\n",
    "        </li>    \n",
    "        <li> \n",
    "            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n",
    "            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cbf75e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:27.901024Z",
     "iopub.status.busy": "2023-10-13T16:14:27.900415Z",
     "iopub.status.idle": "2023-10-13T16:14:35.042397Z",
     "shell.execute_reply": "2023-10-13T16:14:35.041277Z"
    },
    "papermill": {
     "duration": 7.157043,
     "end_time": "2023-10-13T16:14:35.045525",
     "exception": false,
     "start_time": "2023-10-13T16:14:27.888482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
      "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_clients = '/kaggle/input/credit-card-approval-prediction/application_record.csv'\n",
    "\n",
    "# Defining the data types from the clients dataset.\n",
    "schema_clients = '''\n",
    "`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n",
    "`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n",
    "`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n",
    "`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n",
    "'''\n",
    "\n",
    "# Reading the database with the created schema.\n",
    "df_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cbe80",
   "metadata": {
    "papermill": {
     "duration": 0.018615,
     "end_time": "2023-10-13T16:14:35.083440",
     "exception": false,
     "start_time": "2023-10-13T16:14:35.064825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "             Clients may not have unique rows in the dataset because the ID column identifies a contracted loan instead of a person.\n",
    "        </li>\n",
    "        <li> \n",
    "            Thus, I've found convenient for the project to create an ID column that assigns a code for each of the clients\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d858b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:35.123855Z",
     "iopub.status.busy": "2023-10-13T16:14:35.123280Z",
     "iopub.status.idle": "2023-10-13T16:14:38.225027Z",
     "shell.execute_reply": "2023-10-13T16:14:38.223895Z"
    },
    "papermill": {
     "duration": 3.125388,
     "end_time": "2023-10-13T16:14:38.228367",
     "exception": false,
     "start_time": "2023-10-13T16:14:35.102979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     ID|count|\n",
      "+-------+-----+\n",
      "|7742298|    2|\n",
      "|7174719|    2|\n",
      "|7091721|    2|\n",
      "|7089090|    2|\n",
      "|7022197|    2|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Another issue unnoticed by the Data Engineers is that the database contains repeated Loan ID's.\n",
    "from pyspark.sql.functions import max as ps_max\n",
    "\n",
    "# Observe that there are Loans mentioned two times. It would be proper to disconsider such cases. \n",
    "data_duplicate_id = (df_clients\n",
    "                     .groupBy('ID')\n",
    "                     .count()\n",
    "                     .filter('`count`>1')\n",
    "                            )\n",
    "data_duplicate_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e78de85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:38.270657Z",
     "iopub.status.busy": "2023-10-13T16:14:38.270194Z",
     "iopub.status.idle": "2023-10-13T16:14:40.822416Z",
     "shell.execute_reply": "2023-10-13T16:14:40.821313Z"
    },
    "papermill": {
     "duration": 2.57654,
     "end_time": "2023-10-13T16:14:40.825755",
     "exception": false,
     "start_time": "2023-10-13T16:14:38.249215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|7742298|          F|           N|              Y|           0|        144000.0|         Working|Secondary / secon...|             Widow|House / apartment|    -20626|        -1455|         1|              0|         0|         0|  Cooking staff|            1.0|\n",
      "|7742298|          M|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -18239|        -5428|         1|              1|         0|         0|           NULL|            2.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# If we take a peek on the first mentioned ID, we see that the presented loan actually is assigned to two different people!\n",
    "\n",
    "# Thus, the presence of such deals is potentially harmful to our model. It would be sensible to discard such ID's from the database.\n",
    "df_clients.filter('`ID`==7742298').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01088124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:40.851927Z",
     "iopub.status.busy": "2023-10-13T16:14:40.850906Z",
     "iopub.status.idle": "2023-10-13T16:14:40.960841Z",
     "shell.execute_reply": "2023-10-13T16:14:40.959620Z"
    },
    "papermill": {
     "duration": 0.125938,
     "end_time": "2023-10-13T16:14:40.964080",
     "exception": false,
     "start_time": "2023-10-13T16:14:40.838142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping out the problematic loans.\n",
    "df_clients = data_duplicate_id.join(df_clients, how='right', on='ID').drop('count').where('`count` IS NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef53fad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:40.989743Z",
     "iopub.status.busy": "2023-10-13T16:14:40.989342Z",
     "iopub.status.idle": "2023-10-13T16:14:41.028432Z",
     "shell.execute_reply": "2023-10-13T16:14:41.027392Z"
    },
    "papermill": {
     "duration": 0.054631,
     "end_time": "2023-10-13T16:14:41.031293",
     "exception": false,
     "start_time": "2023-10-13T16:14:40.976662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'CNT_CHILDREN',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'FLAG_MOBIL',\n",
       " 'FLAG_WORK_PHONE',\n",
       " 'FLAG_PHONE',\n",
       " 'FLAG_EMAIL',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'CNT_FAM_MEMBERS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the `df_clients` features with the exception of ID.\n",
    "features_clients = df_clients.columns\n",
    "features_clients.remove('ID')\n",
    "features_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a263565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:41.074422Z",
     "iopub.status.busy": "2023-10-13T16:14:41.073939Z",
     "iopub.status.idle": "2023-10-13T16:14:48.732346Z",
     "shell.execute_reply": "2023-10-13T16:14:48.731171Z"
    },
    "papermill": {
     "duration": 7.683872,
     "end_time": "2023-10-13T16:14:48.735711",
     "exception": false,
     "start_time": "2023-10-13T16:14:41.051839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`df_clients` length: 438463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 90084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Note that the database's actual amount of clients is lower than its number of rows. \n",
    "data_clients = df_clients.dropDuplicates(features_clients) \n",
    "print(f'`df_clients` length: {df_clients.count()}')\n",
    "print(f'Number of clients: {data_clients.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc781280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:48.767449Z",
     "iopub.status.busy": "2023-10-13T16:14:48.767111Z",
     "iopub.status.idle": "2023-10-13T16:14:48.926586Z",
     "shell.execute_reply": "2023-10-13T16:14:48.925423Z"
    },
    "papermill": {
     "duration": 0.175726,
     "end_time": "2023-10-13T16:14:48.929838",
     "exception": false,
     "start_time": "2023-10-13T16:14:48.754112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll assign a Client ID for every loan mentioned in `df_clients`. \n",
    "from pyspark.sql.functions import cast, row_number\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(features_clients)\n",
    "row_window = row_number().over(window)\n",
    "id_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb58dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:14:48.969627Z",
     "iopub.status.busy": "2023-10-13T16:14:48.968653Z",
     "iopub.status.idle": "2023-10-13T16:14:59.910597Z",
     "shell.execute_reply": "2023-10-13T16:14:59.909584Z"
    },
    "papermill": {
     "duration": 10.962398,
     "end_time": "2023-10-13T16:14:59.914836",
     "exception": false,
     "start_time": "2023-10-13T16:14:48.952438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|ID_CLIENT|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|5996382|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996383|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996384|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|6499066|          F|           N|              N|           0|         29133.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20945|       365243|         1|              0|         1|         0|           NULL|            2.0|        8|\n",
      "|6499067|          F|           N|              N|           0|         29133.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20945|       365243|         1|              0|         1|         0|           NULL|            2.0|        8|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We'll need to perform Null Safe JOIN's, since columns such as 'OCCUPATION_TYPE' contain null values.\n",
    "from functools import reduce\n",
    "\n",
    "# Creating the multiple null safe JOIN's condition.\n",
    "condition_id_client = reduce(lambda x,y: x&y, [df_clients[col].eqNullSafe(id_clients[col]) for col in features_clients])\n",
    "columns_join_id = ['df_clients.*', 'id_clients.ID_CLIENT'] # Listing the JOIN columns.\n",
    "\n",
    "# Consolidating the final client database.\n",
    "df_clients = (df_clients.alias('df_clients') # Resorting to aliases for both DataFrames present columns with same names.\n",
    "                 .join(id_clients.alias('id_clients'), condition_id_client)\n",
    "                 .select(columns_join_id))\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8ee95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.014232,
     "end_time": "2023-10-13T16:14:59.952881",
     "exception": false,
     "start_time": "2023-10-13T16:14:59.938649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This table contains the payments records for every loan since its contraction. \n",
    "        </li>\n",
    "        <li> \n",
    "            But in order to the dataset be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n",
    "            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4956ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:00.040657Z",
     "iopub.status.busy": "2023-10-13T16:15:00.040330Z",
     "iopub.status.idle": "2023-10-13T16:15:00.231022Z",
     "shell.execute_reply": "2023-10-13T16:15:00.230041Z"
    },
    "papermill": {
     "duration": 0.269073,
     "end_time": "2023-10-13T16:15:00.234536",
     "exception": false,
     "start_time": "2023-10-13T16:14:59.965463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------+\n",
      "|     ID|MONTHS_BALANCE|STATUS|\n",
      "+-------+--------------+------+\n",
      "|5001711|             0|     X|\n",
      "|5001711|            -1|     0|\n",
      "|5001711|            -2|     0|\n",
      "|5001711|            -3|     0|\n",
      "|5001712|             0|     C|\n",
      "+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bringing the dataset into our notebook.\n",
    "path_loans = '/kaggle/input/credit-card-approval-prediction/credit_record.csv'\n",
    "schema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\n",
    "df_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\n",
    "df_loans.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40760904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:00.282958Z",
     "iopub.status.busy": "2023-10-13T16:15:00.282581Z",
     "iopub.status.idle": "2023-10-13T16:15:12.764399Z",
     "shell.execute_reply": "2023-10-13T16:15:12.762702Z"
    },
    "papermill": {
     "duration": 12.509149,
     "end_time": "2023-10-13T16:15:12.767181",
     "exception": false,
     "start_time": "2023-10-13T16:15:00.258032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------+------+\n",
      "|ID_CLIENT|     ID|MONTHS_BALANCE|STATUS|\n",
      "+---------+-------+--------------+------+\n",
      "|    34269|5008810|             0|     C|\n",
      "|    34269|5008810|            -1|     C|\n",
      "|    34269|5008810|            -2|     C|\n",
      "|    34269|5008810|            -3|     C|\n",
      "|    34269|5008810|            -4|     C|\n",
      "+---------+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now, providing the loans' client ID.\n",
    "df_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\n",
    "df_loans.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f0501",
   "metadata": {
    "papermill": {
     "duration": 0.01991,
     "end_time": "2023-10-13T16:15:12.822982",
     "exception": false,
     "start_time": "2023-10-13T16:15:12.803072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n",
    "            <table style='font-size:15px;margin-top:20px'> \n",
    "                <tr>\n",
    "                    <th> Code</th>\n",
    "                    <th> Definition</th>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> C</td>\n",
    "                    <td> Paid off that month</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 0</td>\n",
    "                    <td> 1-29 days past due</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 1</td>\n",
    "                    <td> 30-59 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 2</td>\n",
    "                    <td> 60-89 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 3</td>\n",
    "                    <td> 90-119 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 4</td>\n",
    "                    <td> 120-149 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 5</td>\n",
    "                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> X</td>\n",
    "                    <td> No loan for the month</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Observe that in our case only the 3, 4 and 5 codes are of our interest. Thus it would be convenient to create a binary flag that denounces whether \n",
    "            the individual has ever caused an NPL.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21af561e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:12.851595Z",
     "iopub.status.busy": "2023-10-13T16:15:12.851181Z",
     "iopub.status.idle": "2023-10-13T16:15:13.367344Z",
     "shell.execute_reply": "2023-10-13T16:15:13.366060Z"
    },
    "papermill": {
     "duration": 0.534211,
     "end_time": "2023-10-13T16:15:13.370746",
     "exception": false,
     "start_time": "2023-10-13T16:15:12.836535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dependent variable's conception needs a custom GroupBy that PySpark is unable to perform. Hence, we are going to resort to pandas\n",
    "# in this section.\n",
    "import pandas as pd\n",
    "\n",
    "# Defining the GroupBy's schema.\n",
    "schema_flag_npl = '`ID_CLIENT` STRING, `NPL` BOOLEAN'\n",
    "\n",
    "# This lambda expression signs whether a client has produced an NPL in the past.\n",
    "lambda_npl = lambda x: any(i in x for i in ('3', '4', '5'))\n",
    "\n",
    "def has_npl(df:pd.DataFrame)->pd.DataFrame:\n",
    "    '''\n",
    "        Verifies if a client's  records contain any sort of Non-Performing Loan.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `df`: The loan records of a certain client.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A `pd.DataFrame` with the client's ID and a flag indicating NPL existence in their loan history. \n",
    "    '''\n",
    "    df['NPL'] = df.STATUS.map(lambda_npl)\n",
    "    return df[['ID_CLIENT', 'NPL']].drop_duplicates()\n",
    "\n",
    "# Finally, generating our target-variable.\n",
    "target = df_loans.groupBy('ID_CLIENT').applyInPandas(has_npl, schema_flag_npl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b2e64f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:13.405419Z",
     "iopub.status.busy": "2023-10-13T16:15:13.404824Z",
     "iopub.status.idle": "2023-10-13T16:15:45.058089Z",
     "shell.execute_reply": "2023-10-13T16:15:45.056329Z"
    },
    "papermill": {
     "duration": 31.671119,
     "end_time": "2023-10-13T16:15:45.061024",
     "exception": false,
     "start_time": "2023-10-13T16:15:13.389905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Corrigir GroupBy!\n",
    "from pyspark.sql.functions import col, max\n",
    "target.groupBy('ID_CLIENT').count().select(max('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0ae87",
   "metadata": {
    "papermill": {
     "duration": 0.027997,
     "end_time": "2023-10-13T16:15:45.127566",
     "exception": false,
     "start_time": "2023-10-13T16:15:45.099569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Consolidating the Data</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With both datasets properly treated, we are able to JOIN them in a single table.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee95cb32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:45.165388Z",
     "iopub.status.busy": "2023-10-13T16:15:45.165036Z",
     "iopub.status.idle": "2023-10-13T16:15:55.305050Z",
     "shell.execute_reply": "2023-10-13T16:15:55.300471Z"
    },
    "papermill": {
     "duration": 10.159186,
     "end_time": "2023-10-13T16:15:55.308055",
     "exception": false,
     "start_time": "2023-10-13T16:15:45.148869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|ID_CLIENT|count|\n",
      "+---------+-----+\n",
      "|    54525|  799|\n",
      "|    89966|  784|\n",
      "|    35123|  740|\n",
      "|    68797|  699|\n",
      "|    88993|  671|\n",
      "|    46756|  631|\n",
      "|    50437|  616|\n",
      "|    62908|  607|\n",
      "|    12971|  578|\n",
      "|    28902|  577|\n",
      "|    87811|  575|\n",
      "|    88979|  563|\n",
      "|    70705|  563|\n",
      "|    77258|  554|\n",
      "|    47840|  553|\n",
      "|    52484|  544|\n",
      "|    71403|  544|\n",
      "|    20241|  543|\n",
      "|    16144|  543|\n",
      "|     8068|  526|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Como os ID's têm mais de uma aparição em `target`?\n",
    "from pyspark.sql.functions import col\n",
    "df_loans.groupBy('ID_CLIENT').count().filter('`count`>1').orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd03634e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T16:15:55.435020Z",
     "iopub.status.busy": "2023-10-13T16:15:55.434406Z",
     "iopub.status.idle": "2023-10-13T16:16:28.039906Z",
     "shell.execute_reply": "2023-10-13T16:16:28.038869Z"
    },
    "papermill": {
     "duration": 32.6771,
     "end_time": "2023-10-13T16:16:28.043671",
     "exception": false,
     "start_time": "2023-10-13T16:15:55.366571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|ID_CLIENT|NPL|\n",
      "+---------+---+\n",
      "+---------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "target.filter('`ID_CLIENT`==65897').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710dd4d",
   "metadata": {
    "papermill": {
     "duration": 0.025455,
     "end_time": "2023-10-13T16:16:28.099423",
     "exception": false,
     "start_time": "2023-10-13T16:16:28.073968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Corrigir GroupBy target! Output lambda_npl em string.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.688565,
   "end_time": "2023-10-13T16:16:30.736774",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-13T16:13:34.048209",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
