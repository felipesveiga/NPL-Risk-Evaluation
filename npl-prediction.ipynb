{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d73a0e9",
   "metadata": {
    "papermill": {
     "duration": 0.012601,
     "end_time": "2024-03-14T21:49:43.251686",
     "exception": false,
     "start_time": "2024-03-14T21:49:43.239085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n",
    "<div style='font-size:20px'> \n",
    "    <ul> \n",
    "        <li> \n",
    "            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n",
    "        </li>\n",
    "        <li> \n",
    "            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n",
    "            credit strategy must be adopted for new contracts.\n",
    "        </li>\n",
    "        <li> \n",
    "            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n",
    "            <ul style='list-style-type:decimal'> \n",
    "                <li> \n",
    "                    It must be focused on predicting whether a given client might produce an NPL in the future.\n",
    "                </li>\n",
    "                <li> \n",
    "                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n",
    "                    an incisive \"yes or no\" answer.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <p style='margin-left:30px'> <strong> Note:</strong> The bank's NPL definition is any loan which payment is at least 90 days late.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6031fef",
   "metadata": {
    "papermill": {
     "duration": 0.011963,
     "end_time": "2024-03-14T21:49:43.275956",
     "exception": false,
     "start_time": "2024-03-14T21:49:43.263993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Data Importing</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n",
    "            and the second lists the loans they've contracted over some period of time.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb13e81d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:49:43.302043Z",
     "iopub.status.busy": "2024-03-14T21:49:43.301457Z",
     "iopub.status.idle": "2024-03-14T21:50:23.575725Z",
     "shell.execute_reply": "2024-03-14T21:50:23.574198Z"
    },
    "papermill": {
     "duration": 40.290146,
     "end_time": "2024-03-14T21:50:23.578192",
     "exception": false,
     "start_time": "2024-03-14T21:49:43.288046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488497 sha256=c4d12f402c4b64b9c74638d430e508c5658a68731fa9baf38a8e80b52b70b997\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287ff0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:23.617791Z",
     "iopub.status.busy": "2024-03-14T21:50:23.617246Z",
     "iopub.status.idle": "2024-03-14T21:50:28.699314Z",
     "shell.execute_reply": "2024-03-14T21:50:28.698145Z"
    },
    "papermill": {
     "duration": 5.104508,
     "end_time": "2024-03-14T21:50:28.701664",
     "exception": false,
     "start_time": "2024-03-14T21:50:23.597156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 21:50:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffab7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.018029,
     "end_time": "2024-03-14T21:50:28.738360",
     "exception": false,
     "start_time": "2024-03-14T21:50:28.720331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This dataset is comprised of general information about the loans' clients.\n",
    "        </li>    \n",
    "        <li> \n",
    "            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n",
    "            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f606895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:28.778326Z",
     "iopub.status.busy": "2024-03-14T21:50:28.777525Z",
     "iopub.status.idle": "2024-03-14T21:50:34.337267Z",
     "shell.execute_reply": "2024-03-14T21:50:34.335557Z"
    },
    "papermill": {
     "duration": 5.582218,
     "end_time": "2024-03-14T21:50:34.339837",
     "exception": false,
     "start_time": "2024-03-14T21:50:28.757619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
      "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_clients = '/kaggle/input/credit-card-approval-prediction/application_record.csv'\n",
    "\n",
    "# Defining the data types from the clients dataset.\n",
    "schema_clients = '''\n",
    "`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n",
    "`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n",
    "`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n",
    "`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n",
    "'''\n",
    "\n",
    "# Reading the database with the created schema.\n",
    "df_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a0924",
   "metadata": {
    "papermill": {
     "duration": 0.017405,
     "end_time": "2024-03-14T21:50:34.379462",
     "exception": false,
     "start_time": "2024-03-14T21:50:34.362057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "             Clients may not have unique rows in the dataset because the ID column identifies a contracted loan instead of a person.\n",
    "        </li>\n",
    "        <li> \n",
    "            Thus, I've found convenient for the project to create an ID column that assigns a code for each of the clients.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c88efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:34.416510Z",
     "iopub.status.busy": "2024-03-14T21:50:34.416173Z",
     "iopub.status.idle": "2024-03-14T21:50:36.998303Z",
     "shell.execute_reply": "2024-03-14T21:50:36.997215Z"
    },
    "papermill": {
     "duration": 2.604683,
     "end_time": "2024-03-14T21:50:37.001746",
     "exception": false,
     "start_time": "2024-03-14T21:50:34.397063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     ID|count|\n",
      "+-------+-----+\n",
      "|7742298|    2|\n",
      "|7174719|    2|\n",
      "|7091721|    2|\n",
      "|7089090|    2|\n",
      "|7022197|    2|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Another issue unnoticed by the Data Engineers is that the database contains repeated Loan ID's.\n",
    "from pyspark.sql.functions import max as ps_max\n",
    "\n",
    "# Observe that there are Loans mentioned two times. It would be proper to disconsider them. \n",
    "data_duplicate_id = (df_clients\n",
    "                     .groupBy('ID')\n",
    "                     .count()\n",
    "                     .filter('`count`>1')\n",
    "                            )\n",
    "data_duplicate_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bec2f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:37.043175Z",
     "iopub.status.busy": "2024-03-14T21:50:37.042755Z",
     "iopub.status.idle": "2024-03-14T21:50:38.913409Z",
     "shell.execute_reply": "2024-03-14T21:50:38.912488Z"
    },
    "papermill": {
     "duration": 1.892659,
     "end_time": "2024-03-14T21:50:38.915615",
     "exception": false,
     "start_time": "2024-03-14T21:50:37.022956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|7742298|          F|           N|              Y|           0|        144000.0|         Working|Secondary / secon...|             Widow|House / apartment|    -20626|        -1455|         1|              0|         0|         0|  Cooking staff|            1.0|\n",
      "|7742298|          M|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -18239|        -5428|         1|              1|         0|         0|           NULL|            2.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# If we take a peek on the first mentioned ID, we can notice that the presented loan actually is assigned to two different people!\n",
    "\n",
    "# Thus, the presence of such deals is potentially harmful for our model. It would be sensible to discard such ID's from the database.\n",
    "df_clients.filter('`ID`==7742298').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe44603e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:39.021357Z",
     "iopub.status.busy": "2024-03-14T21:50:39.020745Z",
     "iopub.status.idle": "2024-03-14T21:50:39.120094Z",
     "shell.execute_reply": "2024-03-14T21:50:39.119286Z"
    },
    "papermill": {
     "duration": 0.18181,
     "end_time": "2024-03-14T21:50:39.122860",
     "exception": false,
     "start_time": "2024-03-14T21:50:38.941050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping out the problematic loans.\n",
    "df_clients = data_duplicate_id.join(df_clients, how='right', on='ID').where('`count` IS NULL').drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a505b2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:39.163986Z",
     "iopub.status.busy": "2024-03-14T21:50:39.163666Z",
     "iopub.status.idle": "2024-03-14T21:50:39.190302Z",
     "shell.execute_reply": "2024-03-14T21:50:39.189504Z"
    },
    "papermill": {
     "duration": 0.049454,
     "end_time": "2024-03-14T21:50:39.192175",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.142721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'CNT_CHILDREN',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'FLAG_MOBIL',\n",
       " 'FLAG_WORK_PHONE',\n",
       " 'FLAG_PHONE',\n",
       " 'FLAG_EMAIL',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'CNT_FAM_MEMBERS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the `df_clients` features with the exception of ID.\n",
    "features_clients = df_clients.columns\n",
    "features_clients.remove('ID')\n",
    "features_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a9de1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:39.233711Z",
     "iopub.status.busy": "2024-03-14T21:50:39.233361Z",
     "iopub.status.idle": "2024-03-14T21:50:45.302386Z",
     "shell.execute_reply": "2024-03-14T21:50:45.301336Z"
    },
    "papermill": {
     "duration": 6.093104,
     "end_time": "2024-03-14T21:50:45.305414",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.212310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`df_clients` length: 438463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 90084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now, getting back to the Client's ID issue, I'd like to present a brief analysis on it.\n",
    "# Note that the database's actual amount of clients is lower than its number of rows. \n",
    "data_clients = df_clients.dropDuplicates(features_clients) \n",
    "print(f'`df_clients` length: {df_clients.count()}')\n",
    "print(f'Number of clients: {data_clients.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5fb965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:45.364711Z",
     "iopub.status.busy": "2024-03-14T21:50:45.364306Z",
     "iopub.status.idle": "2024-03-14T21:50:45.489833Z",
     "shell.execute_reply": "2024-03-14T21:50:45.488875Z"
    },
    "papermill": {
     "duration": 0.158051,
     "end_time": "2024-03-14T21:50:45.492594",
     "exception": false,
     "start_time": "2024-03-14T21:50:45.334543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll assign an ID for every client mentioned in `df_clients`. \n",
    "from pyspark.sql.functions import cast, row_number\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(features_clients)\n",
    "row_window = row_number().over(window)\n",
    "\n",
    "# A DataFrame with the clients' data and actual ID.\n",
    "df_id_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70340fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:45.548060Z",
     "iopub.status.busy": "2024-03-14T21:50:45.547611Z",
     "iopub.status.idle": "2024-03-14T21:50:54.026645Z",
     "shell.execute_reply": "2024-03-14T21:50:54.025898Z"
    },
    "papermill": {
     "duration": 8.513116,
     "end_time": "2024-03-14T21:50:54.033087",
     "exception": false,
     "start_time": "2024-03-14T21:50:45.519971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|ID_CLIENT|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "|5996382|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996383|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|5996384|          F|           N|              N|           0|         28800.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20298|       365243|         1|              0|         1|         0|           NULL|            2.0|        7|\n",
      "|6499066|          F|           N|              N|           0|         29133.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20945|       365243|         1|              0|         1|         0|           NULL|            2.0|        8|\n",
      "|6499067|          F|           N|              N|           0|         29133.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -20945|       365243|         1|              0|         1|         0|           NULL|            2.0|        8|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We'll need to perform Null Safe JOIN's, since columns such as 'OCCUPATION_TYPE' contain null values.\n",
    "from functools import reduce\n",
    "\n",
    "# Creating the multiple null safe JOIN's condition.\n",
    "condition_id_client = reduce(lambda x,y: x&y, [df_clients[col].eqNullSafe(df_id_clients[col]) for col in features_clients])\n",
    "columns_join_id = ['df_clients.*', 'df_id_clients.ID_CLIENT'] # Listing the JOIN columns.\n",
    "\n",
    "# Consolidating the final clients database.\n",
    "df_clients = (df_clients.alias('df_clients') # Resorting to aliases for both DataFrames present columns with same names.\n",
    "                 .join(df_id_clients.alias('df_id_clients'), condition_id_client)\n",
    "                 .select(columns_join_id))\n",
    "df_clients.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273811da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.028513,
     "end_time": "2024-03-14T21:50:54.091661",
     "exception": false,
     "start_time": "2024-03-14T21:50:54.063148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            This table contains the payments records for every loan since its contraction. \n",
    "        </li>\n",
    "        <li> \n",
    "            But in order to the dataset be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n",
    "            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7396f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:54.142489Z",
     "iopub.status.busy": "2024-03-14T21:50:54.142150Z",
     "iopub.status.idle": "2024-03-14T21:50:54.291835Z",
     "shell.execute_reply": "2024-03-14T21:50:54.290898Z"
    },
    "papermill": {
     "duration": 0.173829,
     "end_time": "2024-03-14T21:50:54.294198",
     "exception": false,
     "start_time": "2024-03-14T21:50:54.120369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------+\n",
      "|     ID|MONTHS_BALANCE|STATUS|\n",
      "+-------+--------------+------+\n",
      "|5001711|             0|     X|\n",
      "|5001711|            -1|     0|\n",
      "|5001711|            -2|     0|\n",
      "|5001711|            -3|     0|\n",
      "|5001712|             0|     C|\n",
      "+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bringing the dataset into our notebook.\n",
    "path_loans = '/kaggle/input/credit-card-approval-prediction/credit_record.csv'\n",
    "schema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\n",
    "df_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\n",
    "df_loans.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c106abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:50:54.353648Z",
     "iopub.status.busy": "2024-03-14T21:50:54.353248Z",
     "iopub.status.idle": "2024-03-14T21:51:04.080677Z",
     "shell.execute_reply": "2024-03-14T21:51:04.079860Z"
    },
    "papermill": {
     "duration": 9.760278,
     "end_time": "2024-03-14T21:51:04.083972",
     "exception": false,
     "start_time": "2024-03-14T21:50:54.323694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------+------+\n",
      "|ID_CLIENT|     ID|MONTHS_BALANCE|STATUS|\n",
      "+---------+-------+--------------+------+\n",
      "|    34269|5008810|             0|     C|\n",
      "|    34269|5008810|            -1|     C|\n",
      "|    34269|5008810|            -2|     C|\n",
      "|    34269|5008810|            -3|     C|\n",
      "|    34269|5008810|            -4|     C|\n",
      "+---------+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now, providing the loans' client ID.\n",
    "df_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\n",
    "df_loans.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824af173",
   "metadata": {
    "papermill": {
     "duration": 0.030198,
     "end_time": "2024-03-14T21:51:04.148728",
     "exception": false,
     "start_time": "2024-03-14T21:51:04.118530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n",
    "            <table style='font-size:15px;margin-top:20px'> \n",
    "                <tr>\n",
    "                    <th> Code</th>\n",
    "                    <th> Definition</th>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> C</td>\n",
    "                    <td> Paid off that month</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 0</td>\n",
    "                    <td> 1-29 days past due</td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 1</td>\n",
    "                    <td> 30-59 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 2</td>\n",
    "                    <td> 60-89 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 3</td>\n",
    "                    <td> 90-119 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 4</td>\n",
    "                    <td> 120-149 days past due </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> 5</td>\n",
    "                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n",
    "                </tr>\n",
    "                <tr> \n",
    "                    <td> X</td>\n",
    "                    <td> No loan for the month</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Observe that in our case only the 3, 4 and 5 codes are of our interest. Thus it would be convenient to create a binary flag that denounces whether \n",
    "            the individual has ever caused an NPL.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27afeea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:04.209483Z",
     "iopub.status.busy": "2024-03-14T21:51:04.209046Z",
     "iopub.status.idle": "2024-03-14T21:51:05.204032Z",
     "shell.execute_reply": "2024-03-14T21:51:05.202985Z"
    },
    "papermill": {
     "duration": 1.027822,
     "end_time": "2024-03-14T21:51:05.206336",
     "exception": false,
     "start_time": "2024-03-14T21:51:04.178514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dependent variable's conception needs a custom GroupBy that PySpark is unable to perform by itself. Hence, we \n",
    "# are going to resort to pandas in this section.\n",
    "import pandas as pd\n",
    "\n",
    "# Defining the GroupBy's schema.\n",
    "schema_flag_npl = '`ID_CLIENT` STRING, `NPL` STRING'\n",
    "\n",
    "# This lambda expression signs whether a client has ever produced an NPL in the past.\n",
    "lambda_npl = lambda x: '1' if x.STATUS.isin(['3', '4', '5']).any() else '0'\n",
    "\n",
    "def has_npl(df:pd.DataFrame)->pd.DataFrame:\n",
    "    '''\n",
    "        Verifies if a client's  records contain any sort of Non-Performing Loan.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `df`: pd.DataFrame \n",
    "            The loan records of a certain client.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A `pd.DataFrame` with the client's ID and a flag indicating NPL existence in their loan history. \n",
    "    '''\n",
    "    output = df.groupby(['ID_CLIENT']).apply(lambda_npl) # `lambda_npl` takes care of the flags creation.\n",
    "    output.name = 'NPL' # Setting the flag column's name.\n",
    "    return output.reset_index()\n",
    "\n",
    "# Finally, generating our target-variable.\n",
    "target = df_loans.groupBy('ID_CLIENT').applyInPandas(has_npl, schema_flag_npl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75aaf1d",
   "metadata": {
    "papermill": {
     "duration": 0.020519,
     "end_time": "2024-03-14T21:51:05.247973",
     "exception": false,
     "start_time": "2024-03-14T21:51:05.227454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Consolidating the Data</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With both datasets properly treated, we are able to JOIN them in a single table.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba01ebb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:05.291692Z",
     "iopub.status.busy": "2024-03-14T21:51:05.291237Z",
     "iopub.status.idle": "2024-03-14T21:51:05.329481Z",
     "shell.execute_reply": "2024-03-14T21:51:05.328774Z"
    },
    "papermill": {
     "duration": 0.063095,
     "end_time": "2024-03-14T21:51:05.332176",
     "exception": false,
     "start_time": "2024-03-14T21:51:05.269081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, enriching the clients information with the NPL flag.\n",
    "df = df_id_clients.join(target, how='inner', on='ID_CLIENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdde7633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:05.395375Z",
     "iopub.status.busy": "2024-03-14T21:51:05.395013Z",
     "iopub.status.idle": "2024-03-14T21:51:05.398772Z",
     "shell.execute_reply": "2024-03-14T21:51:05.398019Z"
    },
    "papermill": {
     "duration": 0.037766,
     "end_time": "2024-03-14T21:51:05.400856",
     "exception": false,
     "start_time": "2024-03-14T21:51:05.363090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.write.parquet('/kaggle/input/df-parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a321a",
   "metadata": {
    "papermill": {
     "duration": 0.025728,
     "end_time": "2024-03-14T21:51:05.455021",
     "exception": false,
     "start_time": "2024-03-14T21:51:05.429293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Dataset Splitting</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the dataset properly treated, we are able to begin our EDA and model creation. But firstly we have to separate the data in \n",
    "            the training and test tables.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25592231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:05.499317Z",
     "iopub.status.busy": "2024-03-14T21:51:05.498659Z",
     "iopub.status.idle": "2024-03-14T21:51:51.405634Z",
     "shell.execute_reply": "2024-03-14T21:51:51.404748Z"
    },
    "papermill": {
     "duration": 45.930956,
     "end_time": "2024-03-14T21:51:51.408067",
     "exception": false,
     "start_time": "2024-03-14T21:51:05.477111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------------+\n",
      "|NPL|count|         proportion|\n",
      "+---+-----+-------------------+\n",
      "|  0| 9509| 0.9774876644736842|\n",
      "|  1|  219|0.02251233552631579|\n",
      "+---+-----+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# As we can see, we are dealing with an unbalanced dataset case. Thus it is interesting to maintain the target value proportions\n",
    "# in both training and test sets.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.groupBy('NPL').count().withColumn('proportion', col('count')/df.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c557485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:51.464605Z",
     "iopub.status.busy": "2024-03-14T21:51:51.464323Z",
     "iopub.status.idle": "2024-03-14T21:51:51.816196Z",
     "shell.execute_reply": "2024-03-14T21:51:51.815240Z"
    },
    "papermill": {
     "duration": 0.378294,
     "end_time": "2024-03-14T21:51:51.818972",
     "exception": false,
     "start_time": "2024-03-14T21:51:51.440678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the training and test sets.\n",
    "train = df.sampleBy('NPL', fractions={'0':.75, '1':.75}, seed=42)\n",
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54a0cda4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:51:51.869433Z",
     "iopub.status.busy": "2024-03-14T21:51:51.869153Z",
     "iopub.status.idle": "2024-03-14T21:53:09.446711Z",
     "shell.execute_reply": "2024-03-14T21:53:09.445877Z"
    },
    "papermill": {
     "duration": 77.604606,
     "end_time": "2024-03-14T21:53:09.449239",
     "exception": false,
     "start_time": "2024-03-14T21:51:51.844633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|NPL|count|          proportion|\n",
      "+---+-----+--------------------+\n",
      "|  0| 7205|  0.9785413554257776|\n",
      "|  1|  158|0.021458644574222464|\n",
      "+---+-----+--------------------+\n",
      "\n",
      "*** TEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|NPL|count|          proportion|\n",
      "+---+-----+--------------------+\n",
      "|  0| 2308|  0.9800424628450106|\n",
      "|  1|   47|0.019957537154989383|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since the class proportions are relatively proximate, we are able to use them in our project.\n",
    "print('*** TRAIN ***')\n",
    "train.groupBy('NPL').count().withColumn('proportion', col('count')/train.count()).show()\n",
    "print('*** TEST ***')\n",
    "test.groupBy('NPL').count().withColumn('proportion', col('count')/test.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87056ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:53:09.529853Z",
     "iopub.status.busy": "2024-03-14T21:53:09.529526Z",
     "iopub.status.idle": "2024-03-14T21:53:52.016980Z",
     "shell.execute_reply": "2024-03-14T21:53:52.015850Z"
    },
    "papermill": {
     "duration": 42.531961,
     "end_time": "2024-03-14T21:53:52.019736",
     "exception": false,
     "start_time": "2024-03-14T21:53:09.487775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the datasets in distinct .parquet files.\n",
    "train.write.parquet('/kaggle/working/train.parquet', mode='overwrite')\n",
    "test.write.parquet('/kaggle/working/test.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7de30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.027531,
     "end_time": "2024-03-14T21:53:52.081838",
     "exception": false,
     "start_time": "2024-03-14T21:53:52.054307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Defining the Models' Metric</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Providing False Negatives would clearly be more harmful for the bank's equity than False Positives.         \n",
    "        </li>\n",
    "        <li> \n",
    "            Following conversations with the credit analysts, we've ended up defining the case's official metric\n",
    "            as an f-score with $\\beta=4$. So, we are giving to Recall an importance 4x higher than the Precision's.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fbc42",
   "metadata": {
    "papermill": {
     "duration": 0.027186,
     "end_time": "2024-03-14T21:53:52.136145",
     "exception": false,
     "start_time": "2024-03-14T21:53:52.108959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Exploratory Data Analysis</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the data properly segregated, let's briefly analyze its content and see whether we can spot differences \n",
    "            among the classes. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "881bb200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:53:52.191353Z",
     "iopub.status.busy": "2024-03-14T21:53:52.191017Z",
     "iopub.status.idle": "2024-03-14T21:54:01.549463Z",
     "shell.execute_reply": "2024-03-14T21:54:01.548390Z"
    },
    "papermill": {
     "duration": 9.388498,
     "end_time": "2024-03-14T21:54:01.551466",
     "exception": false,
     "start_time": "2024-03-14T21:53:52.162968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f912c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:01.608598Z",
     "iopub.status.busy": "2024-03-14T21:54:01.608251Z",
     "iopub.status.idle": "2024-03-14T21:54:01.620278Z",
     "shell.execute_reply": "2024-03-14T21:54:01.619396Z"
    },
    "papermill": {
     "duration": 0.042469,
     "end_time": "2024-03-14T21:54:01.622176",
     "exception": false,
     "start_time": "2024-03-14T21:54:01.579707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe87c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:01.680260Z",
     "iopub.status.busy": "2024-03-14T21:54:01.679926Z",
     "iopub.status.idle": "2024-03-14T21:54:02.082716Z",
     "shell.execute_reply": "2024-03-14T21:54:02.081722Z"
    },
    "papermill": {
     "duration": 0.435395,
     "end_time": "2024-03-14T21:54:02.085032",
     "exception": false,
     "start_time": "2024-03-14T21:54:01.649637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|ID_CLIENT|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|NPL|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|      691|          F|           N|              N|           0|         67500.0|         Working|Secondary / secon...|           Married|House / apartment|    -20075|        -7013|         1|              1|         1|         0|    Sales staff|            2.0|  0|\n",
      "|     3606|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|     -9865|         -196|         1|              1|         0|         0|       Laborers|            2.0|  0|\n",
      "|     4821|          F|           N|              N|           0|        135000.0|   State servant|    Higher education|           Married|House / apartment|    -12490|        -1191|         1|              1|         1|         0|     Core staff|            2.0|  0|\n",
      "|     5925|          F|           N|              N|           0|        157500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22828|       365243|         1|              0|         0|         0|           NULL|            2.0|  0|\n",
      "|     6194|          F|           N|              N|           0|        157500.0|         Working|   Incomplete higher|           Married|House / apartment|    -16888|        -2687|         1|              0|         0|         0|     Core staff|            2.0|  0|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.parquet('/kaggle/input/npl-train/train.parquet/')\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868cf6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T16:05:33.192895Z",
     "iopub.status.busy": "2023-08-15T16:05:33.192467Z",
     "iopub.status.idle": "2023-08-15T16:05:34.343874Z",
     "shell.execute_reply": "2023-08-15T16:05:34.342739Z",
     "shell.execute_reply.started": "2023-08-15T16:05:33.192858Z"
    },
    "papermill": {
     "duration": 0.027315,
     "end_time": "2024-03-14T21:54:02.140934",
     "exception": false,
     "start_time": "2024-03-14T21:54:02.113619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Assessing the Classes' Incomes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Let's analyze whether there is any revenue difference between individuals who produced an NPL and those who didn't. \n",
    "        </li>\n",
    "        <li> \n",
    "            Since we are dealing with an continuous outcome from two independent samples, I'll use the z-score formula below to examine the \n",
    "            income differences:\n",
    "            <center style='margin-top:20px'> \n",
    "                    $z=\\frac{\\overline{X}_{1}-\\overline{X}_{2}}{S_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$ | $S_{p}=\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$\n",
    "           </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e4d900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:02.200286Z",
     "iopub.status.busy": "2024-03-14T21:54:02.199799Z",
     "iopub.status.idle": "2024-03-14T21:54:02.496564Z",
     "shell.execute_reply": "2024-03-14T21:54:02.495655Z"
    },
    "papermill": {
     "duration": 0.328285,
     "end_time": "2024-03-14T21:54:02.498776",
     "exception": false,
     "start_time": "2024-03-14T21:54:02.170491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+\n",
      "|NPL|stddev(AMT_INCOME_TOTAL)|\n",
      "+---+------------------------+\n",
      "|  0|        99647.7510722535|\n",
      "|  1|       96639.80531139452|\n",
      "+---+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The formula above is only valid for samples with similar std's (0.5<s1/s2<2). So we ought to firstly guarantee that the incomes' std's\n",
    "# are in accordance to that rule.\n",
    "df_income_std = train.groupBy('NPL').agg({'AMT_INCOME_TOTAL':'std'})\n",
    "df_income_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2682909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:02.563434Z",
     "iopub.status.busy": "2024-03-14T21:54:02.563118Z",
     "iopub.status.idle": "2024-03-14T21:54:02.727975Z",
     "shell.execute_reply": "2024-03-14T21:54:02.727093Z"
    },
    "papermill": {
     "duration": 0.19588,
     "end_time": "2024-03-14T21:54:02.729908",
     "exception": false,
     "start_time": "2024-03-14T21:54:02.534028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0311253292695148"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because the std ratio is in the desired interval, we can proceed in using the formula.\n",
    "list_std = [row['stddev(AMT_INCOME_TOTAL)'] for row in df_income_std.collect()]\n",
    "list_std[0] / list_std[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c0c1df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:02.788639Z",
     "iopub.status.busy": "2024-03-14T21:54:02.788023Z",
     "iopub.status.idle": "2024-03-14T21:54:04.500441Z",
     "shell.execute_reply": "2024-03-14T21:54:04.499617Z"
    },
    "papermill": {
     "duration": 1.744298,
     "end_time": "2024-03-14T21:54:04.502662",
     "exception": false,
     "start_time": "2024-03-14T21:54:02.758364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7972574858186637"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By setting our alpha=.05, we can see that there is no statistical evidence that people with no NPL receive higher incomes\n",
    "# than the other group.\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "zeros = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==0').collect()]\n",
    "ones = [row['AMT_INCOME_TOTAL'] for row in train.select('AMT_INCOME_TOTAL').where('NPL==1').collect()]\n",
    "\n",
    "ztest(zeros, ones, alternative='larger')[1] # Computing our p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df897e",
   "metadata": {
    "papermill": {
     "duration": 0.028372,
     "end_time": "2024-03-14T21:54:04.562279",
     "exception": false,
     "start_time": "2024-03-14T21:54:04.533907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Valuable Assets Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The dataset contains columns representing whether the client possesses real state or a vehicle.\n",
    "        </li>\n",
    "        <li> \n",
    "            We can verify if people that own such properties have lower chance of producing an NPL, because they could sell them if they don't have enough \n",
    "            cash to pay the loans. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4daf3ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:04.621118Z",
     "iopub.status.busy": "2024-03-14T21:54:04.620771Z",
     "iopub.status.idle": "2024-03-14T21:54:04.880617Z",
     "shell.execute_reply": "2024-03-14T21:54:04.879544Z"
    },
    "papermill": {
     "duration": 0.291346,
     "end_time": "2024-03-14T21:54:04.883051",
     "exception": false,
     "start_time": "2024-03-14T21:54:04.591705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|OWN_VALUABLE|NPL|\n",
      "+------------+---+\n",
      "|           0|  0|\n",
      "|           0|  0|\n",
      "|           0|  0|\n",
      "|           0|  0|\n",
      "|           0|  0|\n",
      "+------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def own_valuable(col_car:str, col_realty:str)->str:\n",
    "    '''\n",
    "        Signs whether a client possesses a valuable asset that can be sold for paying their loans.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_car` str\n",
    "            The value of the flag that indicates the ownership of a car.\n",
    "        `col_realty`: str\n",
    "            The value of the flag that indicates the ownership of a real state.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A flag indicating the possession of a car or real state (valuable assets). \n",
    "    '''\n",
    "    if (col_car=='Y') or (col_realty=='Y'):\n",
    "        return '1'\n",
    "    return '0'\n",
    "\n",
    "# Creating the DataFrame that will be used for conducting the Hypothesis Test.\n",
    "df_valuable = train.select(own_valuable('FLAG_OWN_CAR', 'FLAG_OWN_REALTY').alias('OWN_VALUABLE'), 'NPL')\n",
    "df_valuable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c24be644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:04.953140Z",
     "iopub.status.busy": "2024-03-14T21:54:04.952760Z",
     "iopub.status.idle": "2024-03-14T21:54:05.524531Z",
     "shell.execute_reply": "2024-03-14T21:54:05.523584Z"
    },
    "papermill": {
     "duration": 0.606753,
     "end_time": "2024-03-14T21:54:05.526416",
     "exception": false,
     "start_time": "2024-03-14T21:54:04.919663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----+\n",
      "|NPL|OWN_VALUABLE|count|\n",
      "+---+------------+-----+\n",
      "|  0|           1| 5747|\n",
      "|  1|           1|  126|\n",
      "|  0|           0| 1454|\n",
      "|  1|           0|   46|\n",
      "+---+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting the amount of who own or not a valuable asset per target.\n",
    "gb_valuable = (df_valuable\n",
    "     .groupBy(['NPL', 'OWN_VALUABLE'])\n",
    "     .count())\n",
    "\n",
    "gb_valuable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0d28fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:05.585613Z",
     "iopub.status.busy": "2024-03-14T21:54:05.585326Z",
     "iopub.status.idle": "2024-03-14T21:54:06.229788Z",
     "shell.execute_reply": "2024-03-14T21:54:06.228880Z"
    },
    "papermill": {
     "duration": 0.676507,
     "end_time": "2024-03-14T21:54:06.231958",
     "exception": false,
     "start_time": "2024-03-14T21:54:05.555451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---------------+------------------+\n",
      "|NPL|INSTANCES_WITH_VALUABLE|TOTAL_INSTANCES| PROP_OWN_VALUABLE|\n",
      "+---+-----------------------+---------------+------------------+\n",
      "|  0|                   5747|           7201|0.7980835995000695|\n",
      "|  1|                    126|            172|0.7325581395348837|\n",
      "+---+-----------------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, using `gb_valuable` in order to estimate the proportion of clients with a valuable asset for each target.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as ps_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('NPL')\n",
    "\n",
    "# It's noticeable that there is a rough 7% difference between the proportions. But can we regard that as statistically significant? \n",
    "(gb_valuable\n",
    "     .withColumn('TOTAL_INSTANCES', ps_sum(col('count')).over(window)) # Total quantity of instances for each target.\n",
    "     .withColumn('PROP_OWN_VALUABLE', col('count')/ps_sum(col('count')).over(window)) # % of clients who own valuable assets per target.\n",
    "     .withColumnRenamed('count', 'INSTANCES_WITH_VALUABLE') \n",
    "     .where('`OWN_VALUABLE`==1')\n",
    "     .select(['NPL', 'INSTANCES_WITH_VALUABLE', 'TOTAL_INSTANCES', 'PROP_OWN_VALUABLE'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d80fe4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:06.290938Z",
     "iopub.status.busy": "2024-03-14T21:54:06.289590Z",
     "iopub.status.idle": "2024-03-14T21:54:06.315287Z",
     "shell.execute_reply": "2024-03-14T21:54:06.314482Z"
    },
    "papermill": {
     "duration": 0.056919,
     "end_time": "2024-03-14T21:54:06.317157",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.260238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033767448873365"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H0: People who honor their debts have the same probability of possessing a valuable asset as the clients in default.\n",
    "# H1: People who honor their debts have higher probability of possessing a valuable asset compared to clients in default.\n",
    "\n",
    "# By using the table's data in a Hypothesis Test for proportions, we get a p-value below our significance degree (alpha=.05).\n",
    "\n",
    "# Therefore, we are able to reject H0 and state we possess sufficient information to say that people who have never produced an NPL have a higher\n",
    "# tendency of possessing a valuable asset.\n",
    "\n",
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "test_proportions_2indep(5747, 7201, 126, 172, compare='ratio', alternative='larger').pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55504b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:06.378652Z",
     "iopub.status.busy": "2024-03-14T21:54:06.377459Z",
     "iopub.status.idle": "2024-03-14T21:54:06.387349Z",
     "shell.execute_reply": "2024-03-14T21:54:06.386594Z"
    },
    "papermill": {
     "duration": 0.04224,
     "end_time": "2024-03-14T21:54:06.389188",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.346948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15495270778483095"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With the difference proved real, let's analyze it with the use of a Cohen's h.\n",
    "from numpy import arcsin, sqrt\n",
    "def cohen_h(p1:float, p2:float)->float:\n",
    "    '''\n",
    "        Computes the Cohen's h coefficient for the difference between two proportions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `p1`: float\n",
    "            The first probability.\n",
    "        `p2`: float\n",
    "            The second probability.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        The Cohen's h coefficient for the given proportions.\n",
    "    '''\n",
    "    phi1 = 2*arcsin(sqrt(p1))\n",
    "    phi2 = 2*arcsin(sqrt(p2))\n",
    "    return phi1-phi2\n",
    "\n",
    "# By observing the returned h, we can conclude that the difference is of a small magnitude, since it is lower than 0.2.\n",
    "cohen_h(0.7980835995000695, 0.7325581395348837)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59689e",
   "metadata": {
    "papermill": {
     "duration": 0.02894,
     "end_time": "2024-03-14T21:54:06.447176",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.418236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Employment Impact Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Finally, it would be interesting if we could verify whether unemployment can impair loan payments.  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9499f2",
   "metadata": {
    "papermill": {
     "duration": 0.028927,
     "end_time": "2024-03-14T21:54:06.505219",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.476292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Cleaning up Inconsistent Instances</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            The dataset contains individuals who present their OCCUPATION_TYPE value as NULL, although they show up as employed (DAYS_EMPLOYED$\\leq{0}$)\n",
    "        </li>\n",
    "        <li>\n",
    "            In order to they not be regarded as unemployed clients (who also present OCCUPATION_TYPE as NULL), I thought it would be convenient to assign \n",
    "            another category to them.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bd6cdc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:06.564152Z",
     "iopub.status.busy": "2024-03-14T21:54:06.563551Z",
     "iopub.status.idle": "2024-03-14T21:54:06.570410Z",
     "shell.execute_reply": "2024-03-14T21:54:06.568972Z"
    },
    "papermill": {
     "duration": 0.039274,
     "end_time": "2024-03-14T21:54:06.572968",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.533694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def impute_occupation(col_occupation:str, col_days_employed:int)->str:\n",
    "    '''\n",
    "        Assigns new categories to the OCCUPATION_TYPE column if it is null for any given row. \n",
    "        \n",
    "        In case the DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_occupation`: str\n",
    "            The row's OCCUPATION_TYPE value.\n",
    "        `col_days_employed`: int\n",
    "            The row's DAYS_EMPLOYED value.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The row's OCCUPATION_TYPE treated value.\n",
    "    '''\n",
    "    # 'Undefined' profession logic.\n",
    "    if (col_occupation is None) and (col_days_employed<=0):\n",
    "        return 'Undefined'\n",
    "    \n",
    "    # 'Unemployed' logic.\n",
    "    elif (col_occupation is None) and (col_days_employed>0):\n",
    "        return 'Unemployed'\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return col_occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc9d123f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:06.632552Z",
     "iopub.status.busy": "2024-03-14T21:54:06.632261Z",
     "iopub.status.idle": "2024-03-14T21:54:06.659800Z",
     "shell.execute_reply": "2024-03-14T21:54:06.659175Z"
    },
    "papermill": {
     "duration": 0.05973,
     "end_time": "2024-03-14T21:54:06.661827",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.602097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID_CLIENT',\n",
       " 'CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'CNT_CHILDREN',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'FLAG_MOBIL',\n",
       " 'FLAG_WORK_PHONE',\n",
       " 'FLAG_PHONE',\n",
       " 'FLAG_EMAIL',\n",
       " 'CNT_FAM_MEMBERS',\n",
       " Column<'impute_occupation(OCCUPATION_TYPE, DAYS_EMPLOYED) AS OCCUPATION_TYPE'>,\n",
       " 'NPL']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `list_select_occupation` will be used so that we can redefine the `train` DataFrame with the transformed OCCUPATION_TYPE feature.\n",
    "list_select_occupation = train.columns\n",
    "list_select_occupation.remove('OCCUPATION_TYPE')\n",
    "list_select_occupation.insert(-1, impute_occupation('OCCUPATION_TYPE', 'DAYS_EMPLOYED').alias('OCCUPATION_TYPE'))\n",
    "list_select_occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a36bb89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:06.719940Z",
     "iopub.status.busy": "2024-03-14T21:54:06.719400Z",
     "iopub.status.idle": "2024-03-14T21:54:06.969561Z",
     "shell.execute_reply": "2024-03-14T21:54:06.966965Z"
    },
    "papermill": {
     "duration": 0.28182,
     "end_time": "2024-03-14T21:54:06.972115",
     "exception": false,
     "start_time": "2024-03-14T21:54:06.690295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|ID_CLIENT|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|CNT_FAM_MEMBERS|OCCUPATION_TYPE|NPL|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|      691|          F|           N|              N|           0|         67500.0|         Working|Secondary / secon...|           Married|House / apartment|    -20075|        -7013|         1|              1|         1|         0|            2.0|    Sales staff|  0|\n",
      "|     3606|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|     -9865|         -196|         1|              1|         0|         0|            2.0|       Laborers|  0|\n",
      "|     4821|          F|           N|              N|           0|        135000.0|   State servant|    Higher education|           Married|House / apartment|    -12490|        -1191|         1|              1|         1|         0|            2.0|     Core staff|  0|\n",
      "|     5925|          F|           N|              N|           0|        157500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22828|       365243|         1|              0|         0|         0|            2.0|     Unemployed|  0|\n",
      "|     6194|          F|           N|              N|           0|        157500.0|         Working|   Incomplete higher|           Married|House / apartment|    -16888|        -2687|         1|              0|         0|         0|            2.0|     Core staff|  0|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying the modifications to `train`.\n",
    "train = train.select(list_select_occupation)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f641659",
   "metadata": {
    "papermill": {
     "duration": 0.030615,
     "end_time": "2024-03-14T21:54:07.037247",
     "exception": false,
     "start_time": "2024-03-14T21:54:07.006632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Back to the EDA...</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bc35ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:07.095199Z",
     "iopub.status.busy": "2024-03-14T21:54:07.094847Z",
     "iopub.status.idle": "2024-03-14T21:54:07.758402Z",
     "shell.execute_reply": "2024-03-14T21:54:07.757477Z"
    },
    "papermill": {
     "duration": 0.69497,
     "end_time": "2024-03-14T21:54:07.760486",
     "exception": false,
     "start_time": "2024-03-14T21:54:07.065516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "|NPL|EMPLOYED|count|\n",
      "+---+--------+-----+\n",
      "|  0|       0| 1269|\n",
      "|  0|       1| 5932|\n",
      "|  1|       0|   31|\n",
      "|  1|       1|  141|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting the amount of employed and unemployed individuals per target group.\n",
    "from pyspark.sql.functions import when\n",
    "(train\n",
    "     .withColumn('EMPLOYED', when(col('OCCUPATION_TYPE') == 'Unemployed', 0).otherwise(1))\n",
    "     .groupBy('NPL', 'EMPLOYED')\n",
    "     .count()\n",
    "     .orderBy('NPL', 'EMPLOYED')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4561210e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:07.821270Z",
     "iopub.status.busy": "2024-03-14T21:54:07.820946Z",
     "iopub.status.idle": "2024-03-14T21:54:07.827970Z",
     "shell.execute_reply": "2024-03-14T21:54:07.827019Z"
    },
    "papermill": {
     "duration": 0.039439,
     "end_time": "2024-03-14T21:54:07.829707",
     "exception": false,
     "start_time": "2024-03-14T21:54:07.790268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41442347321737816"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H0: The proportion of unemployed among people with honored debts is the same as with those who didn't.\n",
    "# H1: The proportion of unemployed among people with honored debts is lower than that of those who didn't.\n",
    "\n",
    "# By looking at the p-value, we don't have sufficient statistical evidence to reject H0.\n",
    "test_proportions_2indep(1269, 7201, 31, 172, compare='ratio', alternative='smaller', ).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69fa9c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:07.889332Z",
     "iopub.status.busy": "2024-03-14T21:54:07.888307Z",
     "iopub.status.idle": "2024-03-14T21:54:08.498134Z",
     "shell.execute_reply": "2024-03-14T21:54:08.496656Z"
    },
    "papermill": {
     "duration": 0.641917,
     "end_time": "2024-03-14T21:54:08.500471",
     "exception": false,
     "start_time": "2024-03-14T21:54:07.858554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the newly transformed `train` dataset.\n",
    "train.write.parquet('/kaggle/working/train.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505bf4a",
   "metadata": {
    "papermill": {
     "duration": 0.029658,
     "end_time": "2024-03-14T21:54:08.566911",
     "exception": false,
     "start_time": "2024-03-14T21:54:08.537253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Modeling Phase</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            With the Exploratory Analysis concluded, we can move towards making our AI models that we've promised to the credit team.\n",
    "        </li>\n",
    "        <li> \n",
    "            But firstly, we have to conceive a Pipeline that will transform all our features into numerical format.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "251e2313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:08.625089Z",
     "iopub.status.busy": "2024-03-14T21:54:08.624708Z",
     "iopub.status.idle": "2024-03-14T21:54:17.884442Z",
     "shell.execute_reply": "2024-03-14T21:54:17.883447Z"
    },
    "papermill": {
     "duration": 9.291514,
     "end_time": "2024-03-14T21:54:17.886730",
     "exception": false,
     "start_time": "2024-03-14T21:54:08.595216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79f3bb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:17.946406Z",
     "iopub.status.busy": "2024-03-14T21:54:17.946067Z",
     "iopub.status.idle": "2024-03-14T21:54:17.958236Z",
     "shell.execute_reply": "2024-03-14T21:54:17.957356Z"
    },
    "papermill": {
     "duration": 0.044446,
     "end_time": "2024-03-14T21:54:17.959946",
     "exception": false,
     "start_time": "2024-03-14T21:54:17.915500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Creating the project's SparkSession.\n",
    "spark = SparkSession.builder.appName('NPL').getOrCreate()\n",
    "\n",
    "# Also, modifying the session's log level.\n",
    "log_level = spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# This tiny config enables us to scroll along the DataFrame's columns.\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9bd77c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.020194Z",
     "iopub.status.busy": "2024-03-14T21:54:18.019879Z",
     "iopub.status.idle": "2024-03-14T21:54:18.193283Z",
     "shell.execute_reply": "2024-03-14T21:54:18.192319Z"
    },
    "papermill": {
     "duration": 0.206213,
     "end_time": "2024-03-14T21:54:18.195349",
     "exception": false,
     "start_time": "2024-03-14T21:54:17.989136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|ID_CLIENT|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|CNT_FAM_MEMBERS|OCCUPATION_TYPE|NPL|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|      691|          F|           N|              N|           0|         67500.0|         Working|Secondary / secon...|           Married|House / apartment|    -20075|        -7013|         1|              1|         1|         0|            2.0|    Sales staff|  0|\n",
      "|     3606|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|     -9865|         -196|         1|              1|         0|         0|            2.0|       Laborers|  0|\n",
      "|     4821|          F|           N|              N|           0|        135000.0|   State servant|    Higher education|           Married|House / apartment|    -12490|        -1191|         1|              1|         1|         0|            2.0|     Core staff|  0|\n",
      "|     5925|          F|           N|              N|           0|        157500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22828|       365243|         1|              0|         0|         0|            2.0|     Unemployed|  0|\n",
      "|     6194|          F|           N|              N|           0|        157500.0|         Working|   Incomplete higher|           Married|House / apartment|    -16888|        -2687|         1|              0|         0|         0|            2.0|     Core staff|  0|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.parquet('/kaggle/input/npl-train/train-eda.parquet/')\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9189432",
   "metadata": {
    "papermill": {
     "duration": 0.031364,
     "end_time": "2024-03-14T21:54:18.268406",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.237042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Job Labels' Transformer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Before making our numeralization process, we must re-implement the 'OCCUPATION_TYPE' column logic that we've written in a formal \n",
    "            PySpark Transformer class. In that way, we'll be able to apply the transformations in the test set as well.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf837a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.329293Z",
     "iopub.status.busy": "2024-03-14T21:54:18.328963Z",
     "iopub.status.idle": "2024-03-14T21:54:18.406856Z",
     "shell.execute_reply": "2024-03-14T21:54:18.405586Z"
    },
    "papermill": {
     "duration": 0.111246,
     "end_time": "2024-03-14T21:54:18.409417",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.298171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from typing import List\n",
    "\n",
    "class _BaseTransformer(Transformer, HasInputCol, HasInputCols, HasOutputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    '''\n",
    "        A class that provide the basic functionalities for the project's custom transformers.\n",
    "        \n",
    "        If you consider that your object will require even more customization, you can just overwrite any of the default methods when creating it. Also,\n",
    "        remember that any extra parameter must be set with a `self._setDefault` method right in the `__init__` function, mentioned in the `self.setParams`\n",
    "        method and own a getter function.\n",
    "        \n",
    "        Lastly, don't forget to define the `_transform` function!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `inputCol`: str\n",
    "            The name of the input column.\n",
    "        `inputCols`: List[str]\n",
    "            List containing the name of input columns.\n",
    "        `outputCol`: str\n",
    "            The name of the output column.\n",
    "        `outputCols`: List[str]\n",
    "            List containing the name of output columns.\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        https://medium.com/@zeid.zandi/utilizing-the-power-of-pyspark-pipelines-in-data-science-projects-benefits-and-limitations-2-2-9063e4bebd05\n",
    "        https://www.crowdstrike.com/blog/deep-dive-into-custom-spark-transformers-for-machine-learning-pipelines/\n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol:str=None, inputCols:List[str]=None, outputCol:str=None, outputCols:List[str]=None, *args)->None:\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, inputCols=None, outputCol=None, outputCols=None): \n",
    "        kwargs = self._input_kwargs                                                          \n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def setInputCol(self, value):\n",
    "        return self.setParams(inputCol=value)\n",
    "    \n",
    "    def setInputCols(self, value):\n",
    "        self.setParams(inputCols=value)\n",
    "    \n",
    "    def setOutputCol(self, value):\n",
    "        return self.setParams(outputCol=value)\n",
    "    \n",
    "    def setOutputCols(self, value):\n",
    "        return self.setParams(outputCols=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b6c49ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.470221Z",
     "iopub.status.busy": "2024-03-14T21:54:18.469870Z",
     "iopub.status.idle": "2024-03-14T21:54:18.478394Z",
     "shell.execute_reply": "2024-03-14T21:54:18.477357Z"
    },
    "papermill": {
     "duration": 0.042378,
     "end_time": "2024-03-14T21:54:18.481364",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.438986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class ImputeOccupation(_BaseTransformer):\n",
    "    \n",
    "    @staticmethod\n",
    "    @udf(returnType=StringType())\n",
    "    def __impute_occupation(col_occupation:str, col_days_employed:int)->str:\n",
    "        '''\n",
    "        Assigns new categories to the OCCUPATION_TYPE column if it is null. \n",
    "        \n",
    "        In case the row's DAYS_EMPLOYED shows that the client is currently employed, we impute 'UNDEFINED'; otherwise, we insert 'UNEMPLOYED'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `col_occupation`: str\n",
    "            The row's OCCUPATION_TYPE value.\n",
    "        `col_days_employed`: int\n",
    "            The row's DAYS_EMPLOYED value.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The row's OCCUPATION_TYPE treated value.\n",
    "        '''\n",
    "        # 'Undefined' profession logic.\n",
    "        if (col_occupation is None) and (col_days_employed<=0):\n",
    "            return 'Undefined'\n",
    "\n",
    "        # 'Unemployed' logic.\n",
    "        elif (col_occupation is None) and (col_days_employed>0):\n",
    "            return 'Unemployed'\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return col_occupation\n",
    "    \n",
    "    def _transform(self, dataset:DataFrame)->DataFrame:\n",
    "        '''\n",
    "            Performs the new occupation categories imputation.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `dataset`: `pyspark.sql.DataFrame`\n",
    "                The project's independent variables.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            The DataFrame with the treated 'OCCUPATION_TYPE' column.\n",
    "        '''\n",
    "        x = self.getInputCol()\n",
    "        dataset = dataset.withColumn(x, self.__impute_occupation(x, 'DAYS_EMPLOYED'))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9c0af",
   "metadata": {
    "papermill": {
     "duration": 0.030443,
     "end_time": "2024-03-14T21:54:18.542077",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.511634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> String Columns Numeralization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            The first transformation we must conduct is turning string columns into float format.\n",
    "        </li>\n",
    "        <li> \n",
    "            As a solution, I'ĺl replace the given category for the its proportion of instances that are defaulted. I think that will be a worthier way \n",
    "            of generating numerical information than just providing a One-Hot Encoding.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a360dbbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.602366Z",
     "iopub.status.busy": "2024-03-14T21:54:18.602033Z",
     "iopub.status.idle": "2024-03-14T21:54:18.610549Z",
     "shell.execute_reply": "2024-03-14T21:54:18.609867Z"
    },
    "papermill": {
     "duration": 0.040374,
     "end_time": "2024-03-14T21:54:18.612203",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.571829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
    "from pyspark.ml.param.shared import HasPredictionCol, Param, Params, TypeConverters\n",
    "from typing import Dict\n",
    "\n",
    "doc = '''\n",
    "    Parameter that stores the proportions of defaulted individuals for each X features' categories.\n",
    "'''\n",
    "class HasDefaultProportions(Params):\n",
    "    '''\n",
    "        A class dedicated to store the categories' proportion of indebted individuals. \n",
    "    '''\n",
    "    defaultProportions:Dict[str, Dict[str, float]] = Param(Params._dummy(), 'defaultProportions', doc)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HasDefaultProportions, self).__init__()\n",
    "    \n",
    "    def setDefaultProportions(self, value):\n",
    "        return self._set(defaultProportions=value)\n",
    "    \n",
    "    def getDefaultProportions(self):\n",
    "        return self.getOrDefault(self.defaultProportions)\n",
    "    \n",
    "list_col_strings = [col for col, dtype in train.dtypes if dtype=='string' and col !='NPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe021654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.672155Z",
     "iopub.status.busy": "2024-03-14T21:54:18.671602Z",
     "iopub.status.idle": "2024-03-14T21:54:18.678315Z",
     "shell.execute_reply": "2024-03-14T21:54:18.677700Z"
    },
    "papermill": {
     "duration": 0.03881,
     "end_time": "2024-03-14T21:54:18.680078",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.641268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "class DefaultProportionsModel(Model, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols:List[str], #predictionCol:str, \n",
    "                 defaultProportions:Dict[str, DataFrame]):\n",
    "        super(DefaultProportionsModel, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols:List[str], #predictionCol:str, \n",
    "                  defaultProportions:Dict[str, DataFrame]):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset:DataFrame)->DataFrame:\n",
    "        x = self.getInputCols()\n",
    "        defaultProportions = self.getDefaultProportions()\n",
    "        for c in x:\n",
    "            dataset = dataset.join(defaultProportions[c], on=c, how='left').drop(c)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0da12eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:18.739228Z",
     "iopub.status.busy": "2024-03-14T21:54:18.738737Z",
     "iopub.status.idle": "2024-03-14T21:54:21.403457Z",
     "shell.execute_reply": "2024-03-14T21:54:21.402606Z"
    },
    "papermill": {
     "duration": 2.696868,
     "end_time": "2024-03-14T21:54:21.405841",
     "exception": false,
     "start_time": "2024-03-14T21:54:18.708973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "|ID_CLIENT|CNT_CHILDREN|AMT_INCOME_TOTAL|DAYS_BIRTH|DAYS_EMPLOYED|CNT_FAM_MEMBERS|NPL|CODE_GENDER_NPL_PROP|FLAG_OWN_CAR_NPL_PROP|FLAG_OWN_REALTY_NPL_PROP|NAME_INCOME_TYPE_NPL_PROP|NAME_EDUCATION_TYPE_NPL_PROP|NAME_FAMILY_STATUS_NPL_PROP|NAME_HOUSING_TYPE_NPL_PROP|FLAG_MOBIL_NPL_PROP|FLAG_WORK_PHONE_NPL_PROP|FLAG_PHONE_NPL_PROP|FLAG_EMAIL_NPL_PROP|OCCUPATION_TYPE_NPL_PROP|\n",
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "|      691|           0|         67500.0|    -20075|        -7013|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0078665|          0.0214295|               0.0017632|\n",
      "|     3606|           0|        112500.0|     -9865|         -196|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0154618|          0.0214295|               0.0036620|\n",
      "|     4821|           0|        135000.0|    -12490|        -1191|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0018988|                   0.0063746|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0078665|          0.0214295|               0.0032551|\n",
      "|     5925|           0|        157500.0|    -22828|       365243|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0048827|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0173606|          0.0154618|          0.0214295|               0.0042045|\n",
      "|     6194|           0|        157500.0|    -16888|        -2687|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0014919|                  0.0160043|                 0.0200732|          0.0233284|               0.0173606|          0.0154618|          0.0214295|               0.0032551|\n",
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "class DefaultProportions(Estimator, HasInputCols, HasDefaultProportions, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols:List[str], #predictionCol:str\n",
    "                ):\n",
    "        super(DefaultProportions, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols:List[str], #predictionCol:str\n",
    "                 ):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "        \n",
    "    def setInputCols(self, value:List[str]):\n",
    "        return self.setParams(inputCols=value)\n",
    "    \n",
    "    def setPredictionCol(self, value:str):\n",
    "        return self.setParams(predictionCol=value)\n",
    "    \n",
    "    # staticmethod que criará o dicionário <coluna>:<DataFrame>\n",
    "    @staticmethod\n",
    "    def __measure_proportions(dataset:DataFrame, input_cols:List[str], target:str)->Dict[str, DataFrame]:\n",
    "        defaultProportions = {}\n",
    "        len_df = dataset.count()\n",
    "        for c in input_cols:\n",
    "            col_prop = c+'_NPL_PROP'\n",
    "            df_gb = dataset.groupBy([c, target]).count()\n",
    "            #df_gb.show()\n",
    "            df_props = (df_gb\n",
    "                        .where(f'{target}==1')\n",
    "                        .withColumn(col_prop, (col('count')/len_df)\n",
    "                        .cast(DecimalType(8,7)))\n",
    "                        .select([c, col_prop]))\n",
    "            defaultProportions[c] = df_props\n",
    "        return defaultProportions\n",
    "            \n",
    "    def _fit(self, dataset:DataFrame, target:str='NPL')->DefaultProportionsModel:\n",
    "        # Desconsiderar coluna `ID_CLIENT`.\n",
    "        x = self.getInputCols()\n",
    "        defaultProportions = self.__measure_proportions(dataset, x, target)\n",
    "        return DefaultProportionsModel(inputCols=x, defaultProportions=defaultProportions)\n",
    "    \n",
    "DefaultProportions(inputCols=list_col_strings[1:])._fit(train)._transform(train).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae879e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:21.478544Z",
     "iopub.status.busy": "2024-03-14T21:54:21.478210Z",
     "iopub.status.idle": "2024-03-14T21:54:21.551718Z",
     "shell.execute_reply": "2024-03-14T21:54:21.550428Z"
    },
    "papermill": {
     "duration": 0.107346,
     "end_time": "2024-03-14T21:54:21.554232",
     "exception": false,
     "start_time": "2024-03-14T21:54:21.446886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/kaggle/input/df-parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ef1b02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:21.615268Z",
     "iopub.status.busy": "2024-03-14T21:54:21.614950Z",
     "iopub.status.idle": "2024-03-14T21:54:22.103664Z",
     "shell.execute_reply": "2024-03-14T21:54:22.102363Z"
    },
    "papermill": {
     "duration": 0.522321,
     "end_time": "2024-03-14T21:54:22.106197",
     "exception": false,
     "start_time": "2024-03-14T21:54:21.583876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "impute_occupation = ImputeOccupation(inputCol='OCCUPATION_TYPE')\n",
    "default_prop = DefaultProportions(inputCols=list_col_strings[1:])\n",
    "#impute_occupation._transform(df).groupBy('OCCUPATION_TYPE').count().show()\n",
    "pipe = Pipeline(stages=[impute_occupation, default_prop])._fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3435e9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:22.167203Z",
     "iopub.status.busy": "2024-03-14T21:54:22.166827Z",
     "iopub.status.idle": "2024-03-14T21:54:22.233275Z",
     "shell.execute_reply": "2024-03-14T21:54:22.231718Z"
    },
    "papermill": {
     "duration": 0.099222,
     "end_time": "2024-03-14T21:54:22.235150",
     "exception": false,
     "start_time": "2024-03-14T21:54:22.135928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|ID_CLIENT|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|CNT_FAM_MEMBERS|OCCUPATION_TYPE|NPL|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "|      691|          F|           N|              N|           0|         67500.0|         Working|Secondary / secon...|           Married|House / apartment|    -20075|        -7013|         1|              1|         1|         0|            2.0|    Sales staff|  0|\n",
      "|     3606|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|     -9865|         -196|         1|              1|         0|         0|            2.0|       Laborers|  0|\n",
      "|     4821|          F|           N|              N|           0|        135000.0|   State servant|    Higher education|           Married|House / apartment|    -12490|        -1191|         1|              1|         1|         0|            2.0|     Core staff|  0|\n",
      "|     5925|          F|           N|              N|           0|        157500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -22828|       365243|         1|              0|         0|         0|            2.0|     Unemployed|  0|\n",
      "|     6194|          F|           N|              N|           0|        157500.0|         Working|   Incomplete higher|           Married|House / apartment|    -16888|        -2687|         1|              0|         0|         0|            2.0|     Core staff|  0|\n",
      "+---------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11229f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:22.296051Z",
     "iopub.status.busy": "2024-03-14T21:54:22.295666Z",
     "iopub.status.idle": "2024-03-14T21:54:24.669578Z",
     "shell.execute_reply": "2024-03-14T21:54:24.668537Z"
    },
    "papermill": {
     "duration": 2.407288,
     "end_time": "2024-03-14T21:54:24.672193",
     "exception": false,
     "start_time": "2024-03-14T21:54:22.264905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "|ID_CLIENT|CNT_CHILDREN|AMT_INCOME_TOTAL|DAYS_BIRTH|DAYS_EMPLOYED|CNT_FAM_MEMBERS|NPL|CODE_GENDER_NPL_PROP|FLAG_OWN_CAR_NPL_PROP|FLAG_OWN_REALTY_NPL_PROP|NAME_INCOME_TYPE_NPL_PROP|NAME_EDUCATION_TYPE_NPL_PROP|NAME_FAMILY_STATUS_NPL_PROP|NAME_HOUSING_TYPE_NPL_PROP|FLAG_MOBIL_NPL_PROP|FLAG_WORK_PHONE_NPL_PROP|FLAG_PHONE_NPL_PROP|FLAG_EMAIL_NPL_PROP|OCCUPATION_TYPE_NPL_PROP|\n",
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "|      691|           0|         67500.0|    -20075|        -7013|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0078665|          0.0214295|               0.0017632|\n",
      "|     3606|           0|        112500.0|     -9865|         -196|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0154618|          0.0214295|               0.0036620|\n",
      "|     4821|           0|        135000.0|    -12490|        -1191|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0018988|                   0.0063746|                  0.0160043|                 0.0200732|          0.0233284|               0.0059677|          0.0078665|          0.0214295|               0.0032551|\n",
      "|     5925|           0|        157500.0|    -22828|       365243|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0048827|                   0.0147837|                  0.0160043|                 0.0200732|          0.0233284|               0.0173606|          0.0154618|          0.0214295|               0.0042045|\n",
      "|     6194|           0|        157500.0|    -16888|        -2687|            2.0|  0|           0.0139699|            0.0153262|               0.0096297|                0.0104435|                   0.0014919|                  0.0160043|                 0.0200732|          0.0233284|               0.0173606|          0.0154618|          0.0214295|               0.0032551|\n",
      "+---------+------------+----------------+----------+-------------+---------------+---+--------------------+---------------------+------------------------+-------------------------+----------------------------+---------------------------+--------------------------+-------------------+------------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_t = pipe._transform(train)\n",
    "train_t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1100e",
   "metadata": {
    "papermill": {
     "duration": 0.035469,
     "end_time": "2024-03-14T21:54:24.752227",
     "exception": false,
     "start_time": "2024-03-14T21:54:24.716758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df5a5438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T21:54:24.813568Z",
     "iopub.status.busy": "2024-03-14T21:54:24.813249Z",
     "iopub.status.idle": "2024-03-14T21:54:24.817928Z",
     "shell.execute_reply": "2024-03-14T21:54:24.816384Z"
    },
    "papermill": {
     "duration": 0.037345,
     "end_time": "2024-03-14T21:54:24.819637",
     "exception": false,
     "start_time": "2024-03-14T21:54:24.782292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Montar ImputeProp\n",
    "# TRANSFORMERS\n",
    "# Ler https://www.crowdstrike.com/blog/deep-dive-into-custom-spark-transformers-for-machine-learning-pipelines/\n",
    "# e depois o https://medium.com/@zeid.zandi/utilizing-the-power-of-pyspark-pipelines-in-data-science-projects-benefits-and-limitations-2-2-9063e4bebd05\n",
    "# antes de prosseguir com o código (nosso Transformer demandará mais de uma coluna de input)\n",
    "# Referência extra: https://stackoverflow.com/questions/41828391/transformer-operating-on-multiple-features-in-pyspark-ml\n",
    "\n",
    "# ESTIMATORS\n",
    "# https://stackoverflow.com/questions/37270446/how-to-create-a-custom-estimatgo/or-in-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50393ebc",
   "metadata": {
    "papermill": {
     "duration": 0.028884,
     "end_time": "2024-03-14T21:54:24.877558",
     "exception": false,
     "start_time": "2024-03-14T21:54:24.848674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Pipeline funcionando; Documentar `ImputeOccupation`, `DefaultProportionsModel` e `DefaultProportions` </p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 426827,
     "sourceId": 1031720,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3941029,
     "sourceId": 6856406,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3950752,
     "sourceId": 6875350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3950750,
     "sourceId": 7372877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 287.71859,
   "end_time": "2024-03-14T21:54:27.535413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-14T21:49:39.816823",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
