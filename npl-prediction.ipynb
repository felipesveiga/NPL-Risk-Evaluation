{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='font-size:40px'> NPL Risk Evaluation Modeling</h1>\n<div style='font-size:20px'> \n    <ul> \n        <li> \n            This project aims the conceiving of a Machine Learning Model focused on assisting a bank on its credit approval strategy.\n        </li>\n        <li> \n            The corporation has been scolded for its recent NPL levels by its shareholders. Thus, the executive team has decided that a more conservative \n            credit strategy must be adopted for new contracts.\n        </li>\n        <li> \n            During the planning meetings, the business team has made two major requests concerning the nature of the model.\n            <ul style='list-style-type:decimal'> \n                <li> \n                    It must be focused on predicting whether a given client might produce an NPL in the future.\n                </li>\n                <li> \n                    The output must be some kind of score suggesting the likelihood of the event to happen. They are not looking for \n                    an incisive \"yes or no\" answer.\n                </li>\n            </ul>\n        </li>\n    </ul>\n    <p style='margin-left:30px'> <strong> Note:</strong> The bank's NPL definition is any loan which payment is at least 90 days late.</p>\n</div>","metadata":{"papermill":{"duration":0.006906,"end_time":"2023-09-08T16:00:22.310851","exception":false,"start_time":"2023-09-08T16:00:22.303945","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Data Importing</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The Data Engineers were able to provide two .csv views from the bank's database. The first one contains general information over the clients \n            and the second lists the loans they've contracted over some period of time.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install pyspark","metadata":{"papermill":{"duration":52.936605,"end_time":"2023-09-08T16:01:15.267137","exception":false,"start_time":"2023-09-08T16:00:22.330532","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:01:50.690129Z","iopub.execute_input":"2023-10-03T16:01:50.690462Z","iopub.status.idle":"2023-10-03T16:02:25.918189Z","shell.execute_reply.started":"2023-10-03T16:01:50.690436Z","shell.execute_reply":"2023-10-03T16:02:25.917312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom IPython.core.display import HTML\n\n# Creating the project's SparkSession.\nspark = SparkSession.builder.appName('NPL').getOrCreate()\n\n# Also, modifying the session's log level.\nlog_level = spark.sparkContext.setLogLevel('ERROR')\n\n# This tiny config enables us to scroll along the DataFrame's columns.\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","metadata":{"papermill":{"duration":6.106913,"end_time":"2023-09-08T16:01:21.392365","exception":false,"start_time":"2023-09-08T16:01:15.285452","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:25.920017Z","iopub.execute_input":"2023-10-03T16:02:25.920337Z","iopub.status.idle":"2023-10-03T16:02:30.542936Z","shell.execute_reply.started":"2023-10-03T16:02:25.920311Z","shell.execute_reply":"2023-10-03T16:02:30.542148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Clients Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This dataset is comprised of general information about the loans' clients.\n        </li>    \n        <li> \n            A particularity worth noting is that date columns show the negative amount of days since the given event took place. Positive numbers \n            indicate the number of days since the occurence ceased to exist - as it might happen with unemployed borrowers in the DAYS_EMPLOYED feature.\n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.018275,"end_time":"2023-09-08T16:01:21.429645","exception":false,"start_time":"2023-09-08T16:01:21.411370","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path_clients = '/kaggle/input/credit-card-approval-prediction/application_record.csv'\n\n# Defining the data types from the clients dataset.\nschema_clients = '''\n`ID` STRING, `CODE_GENDER` STRING, `FLAG_OWN_CAR` STRING, `FLAG_OWN_REALTY` STRING, `CNT_CHILDREN` INT,\n`AMT_INCOME_TOTAL` FLOAT, `NAME_INCOME_TYPE` STRING, `NAME_EDUCATION_TYPE` STRING, `NAME_FAMILY_STATUS` STRING, `NAME_HOUSING_TYPE` STRING,\n`DAYS_BIRTH` INT, `DAYS_EMPLOYED` INT, `FLAG_MOBIL` STRING, `FLAG_WORK_PHONE` STRING, `FLAG_PHONE` STRING, `FLAG_EMAIL` STRING, \n`OCCUPATION_TYPE` STRING, `CNT_FAM_MEMBERS` DOUBLE\n'''\n\n# Reading the database with the created schema.\ndf_clients = spark.read.csv(path_clients, header=True, schema=schema_clients)\ndf_clients.show(5)","metadata":{"papermill":{"duration":6.913786,"end_time":"2023-09-08T16:01:28.361906","exception":false,"start_time":"2023-09-08T16:01:21.448120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:30.546707Z","iopub.execute_input":"2023-10-03T16:02:30.548676Z","iopub.status.idle":"2023-10-03T16:02:35.837367Z","shell.execute_reply.started":"2023-10-03T16:02:30.548643Z","shell.execute_reply":"2023-10-03T16:02:35.836529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Duplicates Disclaimer</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n             Clients may not have unique rows in the dataset because the ID column identifies a contracted loan instead of a person.\n        </li>\n        <li> \n            Thus, I've found convenient for the project to create an ID column that assigns a code for each of the clients\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.026529,"end_time":"2023-09-08T16:01:28.418517","exception":false,"start_time":"2023-09-08T16:01:28.391988","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Another issue unnoticed by the Data Engineers is that the database contains repeated Loan ID's.\nfrom pyspark.sql.functions import max as ps_max\nfrom pyspark.sql.functions import col\n\n# Observe that there are Loans mentioned two times. It would be proper to disconsider such duplicates. \n(df_clients\n     .groupBy('ID')\n     .count()\n     .select(ps_max(col('count')))\n     .show())","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:02:35.840599Z","iopub.execute_input":"2023-10-03T16:02:35.840939Z","iopub.status.idle":"2023-10-03T16:02:38.487356Z","shell.execute_reply.started":"2023-10-03T16:02:35.840912Z","shell.execute_reply":"2023-10-03T16:02:38.486491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping out rows with repeated ID's.\ndf_clients = df_clients.dropDuplicates(['ID'])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:02:38.488267Z","iopub.execute_input":"2023-10-03T16:02:38.488596Z","iopub.status.idle":"2023-10-03T16:02:38.507987Z","shell.execute_reply.started":"2023-10-03T16:02:38.488567Z","shell.execute_reply":"2023-10-03T16:02:38.506693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Listing the `df_clients` features with the exception of ID.\nfeatures_clients = df_clients.columns\nfeatures_clients.remove('ID')\nfeatures_clients","metadata":{"papermill":{"duration":0.057962,"end_time":"2023-09-08T16:01:28.503200","exception":false,"start_time":"2023-09-08T16:01:28.445238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:38.508841Z","iopub.execute_input":"2023-10-03T16:02:38.509119Z","iopub.status.idle":"2023-10-03T16:02:38.537418Z","shell.execute_reply.started":"2023-10-03T16:02:38.509096Z","shell.execute_reply":"2023-10-03T16:02:38.536725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note that the database's actual amount of clients is lower than its number of rows. \ndata_clients = df_clients.dropDuplicates(features_clients) \nprint(f'`df_clients` length: {df_clients.count()}')\nprint(f'Number of clients: {data_clients.count()}')","metadata":{"papermill":{"duration":5.927381,"end_time":"2023-09-08T16:01:34.450288","exception":false,"start_time":"2023-09-08T16:01:28.522907","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:38.538586Z","iopub.execute_input":"2023-10-03T16:02:38.539033Z","iopub.status.idle":"2023-10-03T16:02:47.902155Z","shell.execute_reply.started":"2023-10-03T16:02:38.539010Z","shell.execute_reply":"2023-10-03T16:02:47.901231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll assign a Client ID for every loan mentioned in `df_clients`. \nfrom pyspark.sql.functions import cast, row_number\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.window import Window\n\nwindow = Window.orderBy(features_clients)\nrow_window = row_number().over(window)\nid_clients = data_clients.withColumn('ID_CLIENT', row_window.cast(StringType())).drop('ID')","metadata":{"papermill":{"duration":0.265699,"end_time":"2023-09-08T16:01:34.740304","exception":false,"start_time":"2023-09-08T16:01:34.474605","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:47.903094Z","iopub.execute_input":"2023-10-03T16:02:47.903398Z","iopub.status.idle":"2023-10-03T16:02:48.036000Z","shell.execute_reply.started":"2023-10-03T16:02:47.903369Z","shell.execute_reply":"2023-10-03T16:02:48.035284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we only need to enrich `df_clients` with the clients' actual identification.\n\n# Performing an INNER JOIN between `df_clients` and `id_clients` using all non-ID columns as keys.\ndf_clients = df_clients.join(id_clients, on=features_clients)\ndf_clients.show(5)","metadata":{"papermill":{"duration":9.568932,"end_time":"2023-09-08T16:01:44.337438","exception":false,"start_time":"2023-09-08T16:01:34.768506","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:02:48.036829Z","iopub.execute_input":"2023-10-03T16:02:48.037050Z","iopub.status.idle":"2023-10-03T16:02:56.702958Z","shell.execute_reply.started":"2023-10-03T16:02:48.037030Z","shell.execute_reply":"2023-10-03T16:02:56.702084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style='font-size:30px;font-style:italic'> Loans Database</h3>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            This table contains the payments records for every loan since its contraction. \n        </li>\n        <li> \n            But in order to the dataset be adequate to our project's intent, two transformations are necessary: first, we need to bring the `ID_CLIENT`\n            column to it and after that, group the database so that it denounces individuals who've produced an NPL at least once.            \n        </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.execute_input":"2023-08-15T16:05:33.192895Z","iopub.status.busy":"2023-08-15T16:05:33.192467Z","iopub.status.idle":"2023-08-15T16:05:34.343874Z","shell.execute_reply":"2023-08-15T16:05:34.342739Z","shell.execute_reply.started":"2023-08-15T16:05:33.192858Z"},"papermill":{"duration":0.030141,"end_time":"2023-09-08T16:01:44.397684","exception":false,"start_time":"2023-09-08T16:01:44.367543","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Bringing the dataset into our notebook.\npath_loans = '/kaggle/input/credit-card-approval-prediction/credit_record.csv'\nschema_loans = '`ID` STRING, `MONTHS_BALANCE` INT, `STATUS` STRING'\ndf_loans = spark.read.csv(path_loans, header=True, schema=schema_loans)\ndf_loans.show(5) ","metadata":{"papermill":{"duration":0.238789,"end_time":"2023-09-08T16:01:44.667766","exception":false,"start_time":"2023-09-08T16:01:44.428977","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:05:35.147288Z","iopub.execute_input":"2023-10-03T16:05:35.147596Z","iopub.status.idle":"2023-10-03T16:05:35.245117Z","shell.execute_reply.started":"2023-10-03T16:05:35.147573Z","shell.execute_reply":"2023-10-03T16:05:35.244368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, providing the loans' client ID.\ndf_loans = df_loans.join(df_clients, ['ID']).select(['ID_CLIENT', 'ID', 'MONTHS_BALANCE', 'STATUS'])\ndf_loans.show(5)","metadata":{"papermill":{"duration":10.005159,"end_time":"2023-09-08T16:01:54.704279","exception":false,"start_time":"2023-09-08T16:01:44.699120","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:05:53.115357Z","iopub.execute_input":"2023-10-03T16:05:53.115645Z","iopub.status.idle":"2023-10-03T16:06:02.112296Z","shell.execute_reply.started":"2023-10-03T16:05:53.115626Z","shell.execute_reply":"2023-10-03T16:06:02.111520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Conceiving the Target Variable</h4>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            The `STATUS` column presents a handful of codes that represent distinct status for a loan's payment. Their definition is as follows:\n            <table style='font-size:15px;margin-top:20px'> \n                <tr>\n                    <th> Code</th>\n                    <th> Definition</th>\n                </tr>\n                <tr> \n                    <td> C</td>\n                    <td> Paid off that month</td>\n                </tr>\n                <tr> \n                    <td> 0</td>\n                    <td> 1-29 days past due</td>\n                </tr>\n                <tr> \n                    <td> 1</td>\n                    <td> 30-59 days past due </td>\n                </tr>\n                <tr> \n                    <td> 2</td>\n                    <td> 60-89 days past due </td>\n                </tr>\n                <tr> \n                    <td> 3</td>\n                    <td> 90-119 days past due </td>\n                </tr>\n                <tr> \n                    <td> 4</td>\n                    <td> 120-149 days past due </td>\n                </tr>\n                <tr> \n                    <td> 5</td>\n                    <td> Overdue or bad debts,<p> write-offs for more than 150 days</p> </td>\n                </tr>\n                <tr> \n                    <td> X</td>\n                    <td> No loan for the month</td>\n                </tr>\n            </table>\n        </li>\n        <li style='margin-top:20px'> \n            Observe that in our case only the 3, 4 and 5 codes are of our interest. Thus it would be convenient to create a binary flag that denounces whether \n            has ever caused an NPL.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.031172,"end_time":"2023-09-08T16:01:54.767879","exception":false,"start_time":"2023-09-08T16:01:54.736707","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The dependent variable's conception needs a custom GroupBy that PySpark is unable to perform. Hence, we are going to resort to pandas\n# in this section.\nimport pandas as pd\n\n# Defining the GroupBy's schema.\nschema_flag_npl = '`ID_CLIENT` STRING, `NPL` BOOLEAN'\n\n# This lambda expression signs whether a client has produced an NPL in the past.\nlambda_npl = lambda x: any(i in x for i in ('3', '4', '5'))\n\ndef has_npl(df:pd.DataFrame)->pd.DataFrame:\n    '''\n        Verifies if a client's  records contain any sort of Non-Performing Loan.\n        \n        Parameter\n        ---------\n        `df`: The loan records of a certain client.\n        \n        Returns\n        -------\n        A `pd.DataFrame` with the client's ID and a flag indicating NPL existence in their loan history. \n    '''\n    df['NPL'] = df.STATUS.map(lambda_npl)\n    return df[['ID_CLIENT', 'NPL']].drop_duplicates()\n\n# Finally, generating our target-variable.\ntarget = df_loans.groupBy('ID_CLIENT').applyInPandas(has_npl, schema_flag_npl)","metadata":{"papermill":{"duration":0.079718,"end_time":"2023-09-08T16:01:54.877696","exception":false,"start_time":"2023-09-08T16:01:54.797978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-03T16:03:08.310728Z","iopub.execute_input":"2023-10-03T16:03:08.310982Z","iopub.status.idle":"2023-10-03T16:03:08.702119Z","shell.execute_reply.started":"2023-10-03T16:03:08.310958Z","shell.execute_reply":"2023-10-03T16:03:08.701482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='font-size:30px'> Consolidating the Data</h2>\n<div> \n    <ul style='font-size:20px'> \n        <li> \n            With both datasets properly treated, we are able to JOIN them in a single table.\n        </li>\n    </ul>\n</div>","metadata":{"papermill":{"duration":0.005969,"end_time":"2023-09-08T16:00:22.323830","exception":false,"start_time":"2023-09-08T16:00:22.317861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# But before performing the consolidation, a quick observation: not all \ntarget.groupBy(['ID_CLIENT']).count().filter('`count`>1').show(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:08:07.975370Z","iopub.execute_input":"2023-10-03T16:08:07.975717Z","iopub.status.idle":"2023-10-03T16:08:24.112818Z","shell.execute_reply.started":"2023-10-03T16:08:07.975691Z","shell.execute_reply":"2023-10-03T16:08:24.112084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clients.dropDuplicates(['ID_CLIENT']).count(),  df_loans.dropDuplicates(['ID_CLIENT']).count()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:03:28.703525Z","iopub.execute_input":"2023-10-03T16:03:28.703850Z","iopub.status.idle":"2023-10-03T16:03:45.898727Z","shell.execute_reply.started":"2023-10-03T16:03:28.703822Z","shell.execute_reply":"2023-10-03T16:03:45.898010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ID\ndf_clients.count()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:03:45.899482Z","iopub.execute_input":"2023-10-03T16:03:45.899710Z","iopub.status.idle":"2023-10-03T16:03:51.256256Z","shell.execute_reply.started":"2023-10-03T16:03:45.899687Z","shell.execute_reply":"2023-10-03T16:03:51.255256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clients.count(), df_loans.count(), df_loans.dropDuplicates(['ID']).count()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:03:51.257070Z","iopub.execute_input":"2023-10-03T16:03:51.257331Z","iopub.status.idle":"2023-10-03T16:04:11.370887Z","shell.execute_reply.started":"2023-10-03T16:03:51.257307Z","shell.execute_reply":"2023-10-03T16:04:11.370097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clients.join(df_loans.dropDuplicates(['ID']), on=['ID']).count()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T16:04:11.371842Z","iopub.execute_input":"2023-10-03T16:04:11.372094Z","iopub.status.idle":"2023-10-03T16:04:24.977407Z","shell.execute_reply.started":"2023-10-03T16:04:11.372073Z","shell.execute_reply":"2023-10-03T16:04:24.976684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='color:red'> target com ID_CLIENT's com mais de uma aparição</p>","metadata":{"papermill":{"duration":0.022705,"end_time":"2023-09-08T16:02:02.469402","exception":false,"start_time":"2023-09-08T16:02:02.446697","status":"completed"},"tags":[]}}]}